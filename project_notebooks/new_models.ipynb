{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.models.nn.rnn import LSTM\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, NBeats, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import NaNLabelEncoder\n",
    "from pytorch_forecasting.data.examples import generate_ar_data\n",
    "from pytorch_forecasting.metrics import SMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralprophet import NeuralProphet\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>27.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-05-01 00:05:00</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-05-01 00:10:00</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ds     y\n",
       "0  2017-05-01 00:00:00  27.8\n",
       "1  2017-05-01 00:05:00  27.0\n",
       "2  2017-05-01 00:10:00  26.8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_location = \"../\"\n",
    "df = pd.read_csv(data_location + \"example_data/yosemite_temps.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralprophet import configure\n",
    "from neuralprophet import time_net\n",
    "from neuralprophet import time_dataset\n",
    "from neuralprophet import df_utils\n",
    "from neuralprophet import utils\n",
    "from neuralprophet import utils_torch\n",
    "from neuralprophet.plot_forecast import plot, plot_components\n",
    "from neuralprophet.plot_model_parameters import plot_parameters\n",
    "from neuralprophet import metrics\n",
    "from neuralprophet.utils import set_logger_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_NP:\n",
    "    \"\"\"LSTM forecaster.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_forecasts=1,\n",
    "        num_hidden_layers=0,\n",
    "        d_hidden=None,\n",
    "        learning_rate=None,\n",
    "        epochs=None,\n",
    "        batch_size=None,\n",
    "        loss_func=\"Huber\",\n",
    "        optimizer=\"AdamW\",\n",
    "        train_speed=None,\n",
    "        normalize=\"auto\",\n",
    "        impute_missing=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "            ## Model Config\n",
    "            n_forecasts (int): Number of steps ahead of prediction time step to forecast.\n",
    "            num_hidden_layers (int): number of hidden layer to include in AR-Net. defaults to 0.\n",
    "            d_hidden (int): dimension of hidden layers of the AR-Net. Ignored if num_hidden_layers == 0.\n",
    "\n",
    "            ## Train Config\n",
    "            learning_rate (float): Maximum learning rate setting for 1cycle policy scheduler.\n",
    "                default: None: Automatically sets the learning_rate based on a learning rate range test.\n",
    "                For manual values, try values ~0.001-10.\n",
    "            epochs (int): Number of epochs (complete iterations over dataset) to train model.\n",
    "                default: None: Automatically sets the number of epochs based on dataset size.\n",
    "                    For best results also leave batch_size to None.\n",
    "                For manual values, try ~5-500.\n",
    "            batch_size (int): Number of samples per mini-batch.\n",
    "                default: None: Automatically sets the batch_size based on dataset size.\n",
    "                    For best results also leave epochs to None.\n",
    "                For manual values, try ~1-512.\n",
    "            loss_func (str, torch.nn.modules.loss._Loss, 'typing.Callable'):\n",
    "                Type of loss to use: str ['Huber', 'MSE'],\n",
    "                or torch loss or callable for custom loss, eg. asymmetric Huber loss\n",
    "\n",
    "            ## Data config\n",
    "            normalize (str): Type of normalization to apply to the time series.\n",
    "                options: ['auto', 'soft', 'off', 'minmax, 'standardize']\n",
    "                default: 'auto' uses 'minmax' if variable is binary, else 'soft'\n",
    "                'soft' scales minimum to 0.1 and the 90th quantile to 0.9\n",
    "            impute_missing (bool): whether to automatically impute missing dates/values\n",
    "                imputation follows a linear method up to 10 missing values, more are filled with trend.\n",
    "        \"\"\"\n",
    "        kwargs = locals()\n",
    "\n",
    "        # General\n",
    "        self.name = \"LSTM\"\n",
    "        self.n_forecasts = n_forecasts\n",
    "\n",
    "        # Data Preprocessing\n",
    "        self.normalize = normalize\n",
    "        self.impute_missing = impute_missing\n",
    "        self.impute_limit_linear = 5\n",
    "        self.impute_rolling = 20\n",
    "\n",
    "        # Training\n",
    "        self.config_train = configure.from_kwargs(configure.Train, kwargs)\n",
    "\n",
    "\n",
    "\n",
    "        self.metrics = metrics.MetricsCollection(\n",
    "            metrics=[\n",
    "                metrics.LossMetric(self.config_train.loss_func),\n",
    "                metrics.MAE(),\n",
    "                metrics.MSE(),\n",
    "            ],\n",
    "            value_metrics=[\n",
    "                # metrics.ValueMetric(\"Loss\"),\n",
    "                metrics.ValueMetric(\"RegLoss\"),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Model\n",
    "        self.config_model = configure.from_kwargs(configure.Model, kwargs)\n",
    "\n",
    "        # set during fit()\n",
    "        self.data_freq = None\n",
    "\n",
    "        # Set during _train()\n",
    "        self.fitted = False\n",
    "        self.data_params = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.model = None\n",
    "\n",
    "        # set during prediction\n",
    "        self.future_periods = None\n",
    "        # later set by user (optional)\n",
    "        self.highlight_forecast_step_n = None\n",
    "        self.true_ar_weights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'ar_sparsity'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-900638bc1556>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_NP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-74a543c64011>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_forecasts, num_hidden_layers, d_hidden, learning_rate, epochs, batch_size, loss_func, optimizer, train_speed, normalize, impute_missing)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/neural_prophet/neuralprophet/configure.py\u001b[0m in \u001b[0;36mfrom_kwargs\u001b[0;34m(cls, kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfrom_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'ar_sparsity'"
     ]
    }
   ],
   "source": [
    "m = LSTM_NP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'learning_rate': <Parameter \"learning_rate: (<class 'float'>, None)\">,\n",
       "              'epochs': <Parameter \"epochs: (<class 'int'>, None)\">,\n",
       "              'batch_size': <Parameter \"batch_size: (<class 'int'>, None)\">,\n",
       "              'loss_func': <Parameter \"loss_func: (<class 'str'>, <class 'torch.nn.modules.loss._Loss'>, 'typing.Callable')\">,\n",
       "              'optimizer': <Parameter \"optimizer: (<class 'str'>, <class 'torch.optim.optimizer.Optimizer'>)\">,\n",
       "              'train_speed': <Parameter \"train_speed: (<class 'int'>, <class 'float'>, None)\">,\n",
       "              'ar_sparsity': <Parameter \"ar_sparsity: (<class 'float'>, None)\">,\n",
       "              'reg_lambda_trend': <Parameter \"reg_lambda_trend: float = None\">,\n",
       "              'trend_reg_threshold': <Parameter \"trend_reg_threshold: (<class 'bool'>, <class 'float'>) = None\">,\n",
       "              'reg_lambda_season': <Parameter \"reg_lambda_season: float = None\">})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.signature(configure.Train).parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize (str): Type of normalization to apply to the time series.\n",
    "    options: ['auto', 'soft', 'off', 'minmax, 'standardize']\n",
    "    default: 'auto' uses 'minmax' if variable is binary, else 'soft'\n",
    "    'soft' scales minimum to 0.1 and the 90th quantile to 0.9\n",
    "impute_missing (bool): whether to automatically impute missing dates/values\n",
    "    imputation follows a linear method up to 10 missing values, more are filled with trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "self.normalize = normalize\n",
    "self.impute_missing = impute_missing\n",
    "self.impute_limit_linear = 5\n",
    "self.impute_rolling = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _handle_missing_data(self, df, freq, predicting=False):\n",
    "    \"\"\"Checks, auto-imputes and normalizes new data\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): raw data with columns 'ds' and 'y'\n",
    "        freq (str): data frequency\n",
    "        predicting (bool): when no lags, allow NA values in 'y' of forecast series or 'y' to miss completely\n",
    "\n",
    "    Returns:\n",
    "        pre-processed df\n",
    "    \"\"\"\n",
    "\n",
    "    # add missing dates for autoregression modelling\n",
    "    df, missing_dates = df_utils.add_missing_dates_nan(df, freq=freq)\n",
    "    if missing_dates > 0:\n",
    "        if self.impute_missing:\n",
    "            log.info(\"{} missing dates added.\".format(missing_dates))\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"{} missing dates found. Please preprocess data manually or set impute_missing to True.\".format(\n",
    "                    missing_dates\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # impute missing values\n",
    "    data_columns = []\n",
    "    data_columns.append(\"y\")\n",
    "    \n",
    "    for column in data_columns:\n",
    "        sum_na = sum(df[column].isnull())\n",
    "        if sum_na > 0:\n",
    "            if self.impute_missing:\n",
    "                df.loc[:, column], remaining_na = df_utils.fill_linear_then_rolling_avg(\n",
    "                        df[column],\n",
    "                        limit_linear=self.impute_limit_linear,\n",
    "                        rolling=self.impute_rolling,\n",
    "                    )\n",
    "                log.info(\"{} NaN values in column {} were auto-imputed.\".format(sum_na - remaining_na, column))\n",
    "                if remaining_na > 0:\n",
    "                    raise ValueError(\n",
    "                        \"More than {} consecutive missing values encountered in column {}. \"\n",
    "                        \"{} NA remain. Please preprocess data manually.\".format(\n",
    "                            2 * self.impute_limit_linear + self.impute_rolling, column, remaining_na\n",
    "                        )\n",
    "                    )\n",
    "            else:  # fail because set to not impute missing\n",
    "                raise ValueError(\n",
    "                    \"Missing values found. \" \"Please preprocess data manually or set impute_missing to True.\"\n",
    "                )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_utils.normalize(df, self.data_params)\n",
    "df = df_utils.check_dataframe(df, check_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 'D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_tsd = df.copy(deep = True)\n",
    "df_for_tsd['ds'] = pd.to_datetime(df_for_tsd['ds'])\n",
    "df_for_tsd = pd.DataFrame(pd.date_range(start=df_for_tsd.ds.min(), end=df_for_tsd.ds.max(), freq = freq),\n",
    "             columns = ['ds']).merge(df_for_tsd, how = 'left')\n",
    "df_for_tsd = df_for_tsd.sort_values('ds')\n",
    "df_for_tsd = df_for_tsd.reset_index()\n",
    "df_for_tsd.columns = ['time_idx', 'date', 'value']\n",
    "df_for_tsd['group'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_idx</th>\n",
       "      <th>date</th>\n",
       "      <th>value</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-01</td>\n",
       "      <td>27.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-05-02</td>\n",
       "      <td>29.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-05-03</td>\n",
       "      <td>31.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2017-05-04</td>\n",
       "      <td>33.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2017-05-05</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>61</td>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>42.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62</td>\n",
       "      <td>2017-07-02</td>\n",
       "      <td>40.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>63</td>\n",
       "      <td>2017-07-03</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>2017-07-04</td>\n",
       "      <td>40.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>2017-07-05</td>\n",
       "      <td>41.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    time_idx       date  value  group\n",
       "0          0 2017-05-01   27.8      0\n",
       "1          1 2017-05-02   29.4      0\n",
       "2          2 2017-05-03   31.3      0\n",
       "3          3 2017-05-04   33.1      0\n",
       "4          4 2017-05-05   33.6      0\n",
       "..       ...        ...    ...    ...\n",
       "61        61 2017-07-01   42.2      0\n",
       "62        62 2017-07-02   40.6      0\n",
       "63        63 2017-07-03   41.0      0\n",
       "64        64 2017-07-04   40.9      0\n",
       "65        65 2017-07-05   41.4      0\n",
       "\n",
       "[66 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_tsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "m = NeuralProphet(n_lags = 10, epochs = 15)\n",
    "df = pd.DataFrame()\n",
    "df['ds'] = pd.date_range(start = '2020-01-01', periods = 100)\n",
    "df['y'] = np.random.randint(0, 100, size = len(df))\n",
    "a = m.fit(df, freq = 'D')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
