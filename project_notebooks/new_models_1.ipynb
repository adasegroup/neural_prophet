{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e8d28da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc28eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import OrderedDict\n",
    "from attrdict import AttrDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19fe3010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.models.nn.rnn import LSTM\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, NBeats, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import NaNLabelEncoder\n",
    "from pytorch_forecasting.data.examples import generate_ar_data\n",
    "from pytorch_forecasting.metrics import SMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "771ded2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralprophet.forecaster_additional_models import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b43156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d04539d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d707617a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d177f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralprophet import time_net\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c52d1e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neuralprophet import LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f51ff81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f836c85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>27.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-05-01 00:05:00</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-05-01 00:10:00</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ds     y\n",
       "0  2017-05-01 00:00:00  27.8\n",
       "1  2017-05-01 00:05:00  27.0\n",
       "2  2017-05-01 00:10:00  26.8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_location = \"../\"\n",
    "df = pd.read_csv(data_location + \"example_data/yosemite_temps.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09d83309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18721, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00543421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd5cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9067e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55379d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralprophet import configure\n",
    "from neuralprophet import time_net\n",
    "from neuralprophet import time_dataset\n",
    "from neuralprophet import df_utils\n",
    "from neuralprophet import utils\n",
    "from neuralprophet import utils_torch\n",
    "from neuralprophet.plot_forecast import plot, plot_components\n",
    "from neuralprophet.plot_model_parameters import plot_parameters\n",
    "from neuralprophet import metrics\n",
    "from neuralprophet.utils import set_logger_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64757ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1aba9c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "log = logging.getLogger(\"NP.LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6bffa2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightLSTM(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_size,\n",
    "                 num_layers, \n",
    "                 bias, \n",
    "                 bidirectional,\n",
    "                 n_forecasts):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, \n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, \n",
    "                            bias=bias, \n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first = False)\n",
    "        self.linear = nn.Linear(hidden_size, n_forecasts)\n",
    "        # Metrics live\n",
    "        self.metrics_live = {}\n",
    "        \n",
    "    def set_optimizer(self, optimizer):\n",
    "        self.optimizer = optimizer  ##### todo add this to init\n",
    "\n",
    "    def set_scheduler(self, scheduler):\n",
    "        self.scheduler = scheduler  ##### todo add this to init\n",
    "\n",
    "    def set_loss_func(self, loss_func):\n",
    "        self.loss_func = loss_func  ##### todo add this to init\n",
    "        \n",
    "    def set_forecaster(self, self_forecaster):\n",
    "        self.forecaster = self_forecaster\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[\"lags\"]\n",
    "        x.resize_((x.size()[0], 1, x.size()[1]))\n",
    "\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        y_pred = self.linear(lstm_out[:,-1])\n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_func(y_hat, y)\n",
    "\n",
    "        self.forecaster.metrics.update(predicted = y_hat.detach(),\n",
    "                                       target = y.detach(),\n",
    "                                       values = {\"Loss\": loss})\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_func(y_hat, y)\n",
    "                \n",
    "        self.forecaster.val_metrics.update(predicted=y_hat.detach(), target=y.detach())\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_func(y_hat, y)\n",
    "        \n",
    "        self.forecaster.test_metrics.update(predicted=y_hat.detach(), target=y.detach())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        epoch_metrics = self.forecaster.metrics.compute(save=True)\n",
    "        self.metrics_live[\"{}\".format(list(epoch_metrics)[0])] = epoch_metrics[list(epoch_metrics)[0]]\n",
    "\n",
    "        self.forecaster.metrics.reset()\n",
    "        \n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        val_epoch_metrics = self.forecaster.val_metrics.compute(save=True)\n",
    "        self.metrics_live[\"val_{}\".format(list(val_epoch_metrics)[0])] = val_epoch_metrics[\n",
    "            list(val_epoch_metrics)[0]]\n",
    "        self.forecaster.val_metrics.reset()\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return [self.optimizer], [self.scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0f9170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "886d070f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "22e1e550",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'running_stage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-222-24edee734c4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, test_dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_running_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRunningStage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTESTING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;31m# If you supply a datamodule you can't supply train_dataloader or val_dataloaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_set_running_stage\u001b[0;34m(self, stage, model_ref)\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \"\"\"\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mmodel_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_stage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_stage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'running_stage'"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aadba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "d48d47ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_NP:\n",
    "    \"\"\"LSTM forecaster.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_lags = 10,\n",
    "        n_forecasts=1,\n",
    "        num_hidden_layers=1,\n",
    "        d_hidden=10,\n",
    "        learning_rate=None,\n",
    "        epochs=None,\n",
    "        batch_size=None,\n",
    "        loss_func=\"Huber\",\n",
    "        optimizer=\"AdamW\",\n",
    "        train_speed=None,\n",
    "        normalize=\"auto\",\n",
    "        impute_missing=True,\n",
    "        lstm_bias = True,\n",
    "        lstm_biderectional = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "            ## Model Config\n",
    "            n_forecasts (int): Number of steps ahead of prediction time step to forecast.\n",
    "            num_hidden_layers (int): number of hidden layer to include in AR-Net. defaults to 0.\n",
    "            d_hidden (int): dimension of hidden layers of the AR-Net. Ignored if num_hidden_layers == 0.\n",
    "\n",
    "            ## Train Config\n",
    "            learning_rate (float): Maximum learning rate setting for 1cycle policy scheduler.\n",
    "                default: None: Automatically sets the learning_rate based on a learning rate range test.\n",
    "                For manual values, try values ~0.001-10.\n",
    "            epochs (int): Number of epochs (complete iterations over dataset) to train model.\n",
    "                default: None: Automatically sets the number of epochs based on dataset size.\n",
    "                    For best results also leave batch_size to None.\n",
    "                For manual values, try ~5-500.\n",
    "            batch_size (int): Number of samples per mini-batch.\n",
    "                default: None: Automatically sets the batch_size based on dataset size.\n",
    "                    For best results also leave epochs to None.\n",
    "                For manual values, try ~1-512.\n",
    "            loss_func (str, torch.nn.modules.loss._Loss, 'typing.Callable'):\n",
    "                Type of loss to use: str ['Huber', 'MSE'],\n",
    "                or torch loss or callable for custom loss, eg. asymmetric Huber loss\n",
    "\n",
    "            ## Data config\n",
    "            normalize (str): Type of normalization to apply to the time series.\n",
    "                options: ['auto', 'soft', 'off', 'minmax, 'standardize']\n",
    "                default: 'auto' uses 'minmax' if variable is binary, else 'soft'\n",
    "                'soft' scales minimum to 0.1 and the 90th quantile to 0.9\n",
    "            impute_missing (bool): whether to automatically impute missing dates/values\n",
    "                imputation follows a linear method up to 10 missing values, more are filled with trend.\n",
    "                \n",
    "            ## LSTM specific\n",
    "            bias (bool): If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "            bidirectional (bool): If True, becomes a bidirectional LSTM. Default: False\n",
    "            \n",
    "        \"\"\"\n",
    "        kwargs = locals()\n",
    "\n",
    "        # General\n",
    "        self.name = \"LSTM\"\n",
    "        self.n_forecasts = n_forecasts\n",
    "        self.n_lags = n_lags\n",
    "\n",
    "        # Data Preprocessing\n",
    "        self.normalize = normalize\n",
    "        self.impute_missing = impute_missing\n",
    "        self.impute_limit_linear = 5\n",
    "        self.impute_rolling = 20\n",
    "\n",
    "        # Training\n",
    "        self.config_train = configure.from_kwargs(configure.Train, kwargs)\n",
    "\n",
    "\n",
    "\n",
    "        self.metrics = metrics.MetricsCollection(\n",
    "            metrics=[\n",
    "                metrics.LossMetric(self.config_train.loss_func),\n",
    "                metrics.MAE(),\n",
    "                metrics.MSE(),\n",
    "            ],\n",
    "            value_metrics=[\n",
    "                # metrics.ValueMetric(\"Loss\"),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Model\n",
    "        self.config_model = configure.from_kwargs(configure.Model, kwargs)\n",
    "        \n",
    "        # LSTM specific\n",
    "        self.lstm_bias = lstm_bias\n",
    "        self.lstm_biderectional = lstm_biderectional\n",
    "\n",
    "        # set during fit()\n",
    "        self.data_freq = None\n",
    "\n",
    "        # Set during _train()\n",
    "        self.fitted = False\n",
    "        self.data_params = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.model = None\n",
    "\n",
    "        # set during prediction\n",
    "        self.future_periods = None\n",
    "        # later set by user (optional)\n",
    "        self.highlight_forecast_step_n = None\n",
    "        self.true_ar_weights = None\n",
    "        \n",
    "        \n",
    "    def _init_model(self):\n",
    "        \"\"\"Build Pytorch model with configured hyperparamters.\n",
    "\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        self.model = LightLSTM(\n",
    "            input_size = self.n_lags,\n",
    "            hidden_size = self.config_model.d_hidden,\n",
    "            num_layers = self.config_model.num_hidden_layers,\n",
    "            bias = self.lstm_bias,\n",
    "            bidirectional = self.lstm_biderectional,\n",
    "            n_forecasts = self.n_forecasts\n",
    "        )\n",
    "\n",
    "        self.model.set_loss_func(self.config_train.loss_func)\n",
    "        self.model.set_forecaster(self)\n",
    "        \n",
    "        log.debug(self.model)\n",
    "        return self.model\n",
    "    \n",
    "    def _create_dataset(self, df, predict_mode):\n",
    "        \"\"\"Construct dataset from dataframe.\n",
    "\n",
    "        (Configured Hyperparameters can be overridden by explicitly supplying them.\n",
    "        Useful to predict a single model component.)\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): containing original and normalized columns 'ds', 'y', 't', 'y_scaled'\n",
    "            predict_mode (bool): False includes target values.\n",
    "                True does not include targets but includes entire dataset as input\n",
    "        Returns:\n",
    "            TimeDataset\n",
    "        \"\"\"\n",
    "        return time_dataset.TimeDataset(\n",
    "            df,\n",
    "            n_lags=self.n_lags,\n",
    "            n_forecasts=self.n_forecasts,\n",
    "            predict_mode=predict_mode,\n",
    "        )\n",
    "    \n",
    "    def _handle_missing_data(self, df, freq, predicting=False):\n",
    "        \"\"\"Checks, auto-imputes and normalizes new data\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): raw data with columns 'ds' and 'y'\n",
    "            freq (str): data frequency\n",
    "            predicting (bool): when no lags, allow NA values in 'y' of forecast series or 'y' to miss completely\n",
    "\n",
    "        Returns:\n",
    "            pre-processed df\n",
    "        \"\"\"\n",
    "\n",
    "        # add missing dates for autoregression modelling\n",
    "        df, missing_dates = df_utils.add_missing_dates_nan(df, freq=freq)\n",
    "        if missing_dates > 0:\n",
    "            if self.impute_missing:\n",
    "                log.info(\"{} missing dates added.\".format(missing_dates))\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"{} missing dates found. Please preprocess data manually or set impute_missing to True.\".format(\n",
    "                        missing_dates\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # impute missing values\n",
    "        data_columns = []\n",
    "        data_columns.append(\"y\")\n",
    "        \n",
    "        for column in data_columns:\n",
    "            sum_na = sum(df[column].isnull())\n",
    "            if sum_na > 0:\n",
    "                if self.impute_missing:\n",
    "                    df.loc[:, column], remaining_na = df_utils.fill_linear_then_rolling_avg(\n",
    "                        df[column],\n",
    "                        limit_linear=self.impute_limit_linear,\n",
    "                        rolling=self.impute_rolling,\n",
    "                    )\n",
    "                    log.info(\"{} NaN values in column {} were auto-imputed.\".format(sum_na - remaining_na, column))\n",
    "                    if remaining_na > 0:\n",
    "                        raise ValueError(\n",
    "                            \"More than {} consecutive missing values encountered in column {}. \"\n",
    "                            \"{} NA remain. Please preprocess data manually.\".format(\n",
    "                                2 * self.impute_limit_linear + self.impute_rolling, column, remaining_na\n",
    "                            )\n",
    "                        )\n",
    "                else:  # fail because set to not impute missing\n",
    "                    raise ValueError(\n",
    "                        \"Missing values found. \" \"Please preprocess data manually or set impute_missing to True.\"\n",
    "                    )\n",
    "        return df\n",
    "    \n",
    "    def _init_train_loader(self, df):\n",
    "        \"\"\"Executes data preparation steps and initiates training procedure.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): containing column 'ds', 'y' with training data\n",
    "\n",
    "        Returns:\n",
    "            torch DataLoader\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            self.data_params = df_utils.init_data_params(\n",
    "                df,\n",
    "                normalize=self.normalize,\n",
    "                covariates_config=None,\n",
    "                regressor_config=None,\n",
    "                events_config=None,\n",
    "            )\n",
    "            \n",
    "        df = df_utils.normalize(df, self.data_params)\n",
    "        self.config_train.set_auto_batch_epoch(n_data=len(df))\n",
    "        self.config_train.apply_train_speed(batch=True, epoch=True)\n",
    "        dataset = self._create_dataset(df, predict_mode=False)  # needs to be called after set_auto_seasonalities\n",
    "        loader = DataLoader(dataset, batch_size=self.config_train.batch_size, shuffle=True)\n",
    "        self.loader_size = len(loader)\n",
    "    \n",
    "        \n",
    "        \n",
    "        # выкинуть это отсюда в трейн!!!\n",
    "        if not self.fitted:\n",
    "            self.model = self._init_model()  # needs to be called after set_auto_seasonalities        \n",
    "        \n",
    "        \n",
    "        assert self.config_train.learning_rate is not None, 'Please, provide a learning rate'\n",
    "            \n",
    "            \n",
    "            \n",
    "        self.config_train.apply_train_speed(lr=True)\n",
    "        self.optimizer = self.config_train.get_optimizer(self.model.parameters())\n",
    "        ######\n",
    "        self.model.set_optimizer(self.optimizer)\n",
    "        self.scheduler = self.config_train.get_scheduler(self.optimizer, steps_per_epoch=len(loader))\n",
    "        self.model.set_scheduler(self.scheduler)\n",
    "        ######\n",
    "        return loader\n",
    "    \n",
    "    def _init_val_loader(self, df):\n",
    "        \"\"\"Executes data preparation steps and initiates evaluation procedure.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): containing column 'ds', 'y' with validation data\n",
    "\n",
    "        Returns:\n",
    "            torch DataLoader\n",
    "        \"\"\"\n",
    "        df = df_utils.normalize(df, self.data_params)\n",
    "        dataset = self._create_dataset(df, predict_mode=False)\n",
    "        loader = DataLoader(dataset, batch_size=min(1024, len(dataset)), shuffle=False, drop_last=False)\n",
    "        return loader\n",
    "    \n",
    "\n",
    "    def _train(self, df, df_val=None, progress_bar=True, plot_live_loss=False):\n",
    "        \"\"\"Execute model training procedure for a configured number of epochs.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): containing column 'ds', 'y' with training data\n",
    "            df_val (pd.DataFrame): containing column 'ds', 'y' with validation data\n",
    "            progress_bar (bool): display updating progress bar\n",
    "            plot_live_loss (bool): plot live training loss,\n",
    "                requires [live] install or livelossplot package installed.\n",
    "        Returns:\n",
    "            df with metrics\n",
    "        \"\"\"\n",
    "        if plot_live_loss:\n",
    "            try:\n",
    "                from livelossplot import PlotLosses\n",
    "            except:\n",
    "                plot_live_loss = False\n",
    "                log.warning(\n",
    "                    \"To plot live loss, please install neuralprophet[live].\"\n",
    "                    \"Using pip: 'pip install neuralprophet[live]'\"\n",
    "                    \"Or install the missing package manually: 'pip install livelossplot'\",\n",
    "                    exc_info=True,\n",
    "                )\n",
    "\n",
    "        loader = self._init_train_loader(df)\n",
    "        val = df_val is not None\n",
    "        ## Metrics\n",
    "        if self.highlight_forecast_step_n is not None:\n",
    "            self.metrics.add_specific_target(target_pos=self.highlight_forecast_step_n - 1)\n",
    "        if not self.normalize == \"off\":\n",
    "            self.metrics.set_shift_scale((self.data_params[\"y\"].shift, self.data_params[\"y\"].scale))\n",
    "        if val:\n",
    "            val_loader = self._init_val_loader(df_val)\n",
    "            val_metrics = metrics.MetricsCollection([m.new() for m in self.metrics.batch_metrics])\n",
    "            \n",
    "            self.val_metrics = val_metrics\n",
    "            \n",
    "        ## Run\n",
    "        start = time.time()\n",
    "        if progress_bar:\n",
    "            training_loop = tqdm(\n",
    "                range(self.config_train.epochs), total=self.config_train.epochs, leave=log.getEffectiveLevel() <= 20\n",
    "            )\n",
    "        else:\n",
    "            training_loop = range(self.config_train.epochs)\n",
    "        if plot_live_loss:\n",
    "            live_out = [\"MatplotlibPlot\"]\n",
    "            if not progress_bar:\n",
    "                live_out.append(\"ExtremaPrinter\")\n",
    "            live_loss = PlotLosses(outputs=live_out)\n",
    "\n",
    "        self.metrics.reset()\n",
    "        if val:\n",
    "            self.val_metrics.reset()\n",
    "\n",
    "        self.trainer = Trainer(max_epochs=self.config_train.epochs,\n",
    "                          # logger = log\n",
    "                          )\n",
    "        \n",
    "        if val:\n",
    "            self.trainer.fit(self.model, train_dataloader = loader, val_dataloaders = val_loader)\n",
    "        else:\n",
    "            self.trainer.fit(self.model, train_dataloader = loader)\n",
    "                    \n",
    "\n",
    "        metrics_df = self.metrics.get_stored_as_df()\n",
    "\n",
    "        if val:\n",
    "            metrics_df_val = self.val_metrics.get_stored_as_df()\n",
    "            for col in metrics_df_val.columns:\n",
    "                metrics_df[\"{}_val\".format(col)] = metrics_df_val[col]\n",
    "        return metrics_df\n",
    "    \n",
    "    def _evaluate(self, loader):\n",
    "        \"\"\"Evaluates model performance.\n",
    "\n",
    "        Args:\n",
    "            loader (torch DataLoader):  instantiated Validation Dataloader (with TimeDataset)\n",
    "        Returns:\n",
    "            df with evaluation metrics\n",
    "        \"\"\"\n",
    "        test_metrics = metrics.MetricsCollection([m.new() for m in self.metrics.batch_metrics])\n",
    "        if self.highlight_forecast_step_n is not None:\n",
    "            test_metrics.add_specific_target(target_pos=self.highlight_forecast_step_n - 1)\n",
    "        ## Run\n",
    "        \n",
    "        self.test_metrics = test_metrics\n",
    "        self.trainer.test(self.model, test_dataloaders=loader, ckpt_path=None, verbose = False)\n",
    "        \n",
    "        test_metrics_dict = self.test_metrics.compute(save=True)\n",
    "        \n",
    "        log.info(\"Validation metrics: {}\".format(utils.print_epoch_metrics(test_metrics_dict)))\n",
    "        val_metrics_df = self.test_metrics.get_stored_as_df()\n",
    "        return val_metrics_df\n",
    "\n",
    "    def split_df(self, df, freq, valid_p=0.2):\n",
    "        \"\"\"Splits timeseries df into train and validation sets.\n",
    "\n",
    "        Prevents overbleed of targets. Overbleed of inputs can be configured.\n",
    "        Also performs basic data checks and fills in missing data.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): data\n",
    "            freq (str):Data step sizes. Frequency of data recording,\n",
    "                Any valid frequency for pd.date_range, such as '5min', 'D' or 'MS'\n",
    "            valid_p (float): fraction of data to use for holdout validation set\n",
    "                Targets will still never be shared.\n",
    "\n",
    "        Returns:\n",
    "            df_train (pd.DataFrame):  training data\n",
    "            df_val (pd.DataFrame): validation data\n",
    "        \"\"\"\n",
    "        df = df.copy(deep=True)\n",
    "        df = df_utils.check_dataframe(df, check_y=False)\n",
    "        df = self._handle_missing_data(df, freq=freq, predicting=False)\n",
    "        df_train, df_val = df_utils.split_df(\n",
    "            df,\n",
    "            n_lags=self.n_lags,\n",
    "            n_forecasts=self.n_forecasts,\n",
    "            valid_p=valid_p,\n",
    "            inputs_overbleed=True,\n",
    "        )\n",
    "        return df_train, df_val\n",
    "\n",
    "    def crossvalidation_split_df(self, df, freq, k=5, fold_pct=0.1, fold_overlap_pct=0.5):\n",
    "        \"\"\"Splits timeseries data in k folds for crossvalidation.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): data\n",
    "            freq (str):Data step sizes. Frequency of data recording,\n",
    "                Any valid frequency for pd.date_range, such as '5min', 'D' or 'MS'\n",
    "            k: number of CV folds\n",
    "            fold_pct: percentage of overall samples to be in each fold\n",
    "            fold_overlap_pct: percentage of overlap between the validation folds.\n",
    "\n",
    "        Returns:\n",
    "            list of k tuples [(df_train, df_val), ...] where:\n",
    "                df_train (pd.DataFrame):  training data\n",
    "                df_val (pd.DataFrame): validation data\n",
    "        \"\"\"\n",
    "        df = df.copy(deep=True)\n",
    "        df = df_utils.check_dataframe(df, check_y=False)\n",
    "        df = self._handle_missing_data(df, freq=freq, predicting=False)\n",
    "        folds = df_utils.crossvalidation_split_df(\n",
    "            df,\n",
    "            n_lags=self.n_lags,\n",
    "            n_forecasts=self.n_forecasts,\n",
    "            k=k,\n",
    "            fold_pct=fold_pct,\n",
    "            fold_overlap_pct=fold_overlap_pct,\n",
    "        )\n",
    "        return folds\n",
    "\n",
    "    def fit(\n",
    "        self, df, freq, epochs=None, validate_each_epoch=False, valid_p=0.2, progress_bar=True, plot_live_loss=False\n",
    "    ):\n",
    "        \"\"\"Train, and potentially evaluate model.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): containing column 'ds', 'y' with all data\n",
    "            freq (str):Data step sizes. Frequency of data recording,\n",
    "                Any valid frequency for pd.date_range, such as '5min', 'D' or 'MS'\n",
    "            epochs (int): number of epochs to train.\n",
    "                default: if not specified, uses self.epochs\n",
    "            validate_each_epoch (bool): whether to evaluate performance after each training epoch\n",
    "            valid_p (float): fraction of data to hold out from training for model evaluation\n",
    "            progress_bar (bool): display updating progress bar (tqdm)\n",
    "            plot_live_loss (bool): plot live training loss,\n",
    "                requires [live] install or livelossplot package installed.\n",
    "        Returns:\n",
    "            metrics with training and potentially evaluation metrics\n",
    "        \"\"\"\n",
    "        self.data_freq = freq\n",
    "        if epochs is not None:\n",
    "            default_epochs = self.config_train.epochs\n",
    "            self.config_train.epochs = epochs\n",
    "        if self.fitted is True:\n",
    "            log.warning(\"Model has already been fitted. Re-fitting will produce different results.\")\n",
    "        df = df_utils.check_dataframe(\n",
    "            df, check_y=True)\n",
    "        df = self._handle_missing_data(df, freq=self.data_freq)\n",
    "        if validate_each_epoch:\n",
    "            df_train, df_val = df_utils.split_df(df, n_lags=self.n_lags, n_forecasts=self.n_forecasts, valid_p=valid_p)\n",
    "            metrics_df = self._train(df_train, df_val, progress_bar=progress_bar, plot_live_loss=plot_live_loss)\n",
    "        else:\n",
    "            metrics_df = self._train(df, progress_bar=progress_bar, plot_live_loss=plot_live_loss)\n",
    "        if epochs is not None:\n",
    "            self.config_train.epochs = default_epochs\n",
    "        self.fitted = True\n",
    "\n",
    "        return metrics_df\n",
    "    \n",
    "    def test(self, df):\n",
    "        \"\"\"Evaluate model on holdout data.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): containing column 'ds', 'y' with holdout data\n",
    "        Returns:\n",
    "            df with evaluation metrics\n",
    "        \"\"\"\n",
    "        if self.fitted is False:\n",
    "            log.warning(\"Model has not been fitted. Test results will be random.\")\n",
    "        df = df_utils.check_dataframe(df, check_y=True)\n",
    "        df = self._handle_missing_data(df, freq=self.data_freq)\n",
    "        loader = self._init_val_loader(df)\n",
    "        val_metrics_df = self._evaluate(loader)\n",
    "        return val_metrics_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    def make_future_dataframe(self, df, periods=None, n_historic_predictions=0):\n",
    "        df = df.copy(deep=True)\n",
    "        \n",
    "        n_lags = 0 if self.n_lags is None else self.n_lags\n",
    "        if periods is None:\n",
    "            periods = 1 if n_lags == 0 else self.n_forecasts\n",
    "        else:\n",
    "            assert periods >= 0\n",
    "\n",
    "        if isinstance(n_historic_predictions, bool):\n",
    "            if n_historic_predictions:\n",
    "                n_historic_predictions = len(df) - n_lags\n",
    "            else:\n",
    "                n_historic_predictions = 0\n",
    "        elif not isinstance(n_historic_predictions, int):\n",
    "            log.error(\"non-integer value for n_historic_predictions set to zero.\")\n",
    "            n_historic_predictions = 0\n",
    "\n",
    "        if periods == 0 and n_historic_predictions == 0:\n",
    "            raise ValueError(\"Set either history or future to contain more than zero values.\")\n",
    "            \n",
    "        last_date = pd.to_datetime(df[\"ds\"].copy(deep=True)).sort_values().max()\n",
    "\n",
    "        if len(df) < n_lags:\n",
    "            raise ValueError(\"Insufficient data for a prediction\")\n",
    "        elif len(df) < n_lags + n_historic_predictions:\n",
    "            log.warning(\n",
    "                \"Insufficient data for {} historic forecasts, reduced to {}.\".format(\n",
    "                    n_historic_predictions, len(df) - n_lags\n",
    "                )\n",
    "            )\n",
    "            n_historic_predictions = len(df) - n_lags\n",
    "        if (n_historic_predictions + n_lags) == 0:\n",
    "            df = pd.DataFrame(columns=df.columns)\n",
    "        else:\n",
    "            df = df[-(n_lags + n_historic_predictions) :]\n",
    "\n",
    "        if len(df) > 0:\n",
    "            if len(df.columns) == 1 and \"ds\" in df:\n",
    "                assert n_lags == 0\n",
    "                df = df_utils.check_dataframe(df, check_y=False)\n",
    "            else:\n",
    "                df = df_utils.check_dataframe(\n",
    "                    df, check_y=n_lags > 0\n",
    "                )\n",
    "                df = self._handle_missing_data(df, freq=self.data_freq, predicting=True)\n",
    "            df = df_utils.normalize(df, self.data_params)\n",
    "\n",
    "        # future data\n",
    "        # check for external events known in future\n",
    "        \n",
    "        if n_lags > 0:\n",
    "            if periods > 0 and periods != self.n_forecasts:\n",
    "                periods = self.n_forecasts\n",
    "                log.warning(\n",
    "                    \"Number of forecast steps is defined by n_forecasts. \" \"Adjusted to {}.\".format(self.n_forecasts)\n",
    "                )\n",
    "\n",
    "        if periods > 0:\n",
    "            future_df = df_utils.make_future_df(\n",
    "                df_columns=df.columns,\n",
    "                last_date=last_date,\n",
    "                periods=periods,\n",
    "                freq=self.data_freq,\n",
    "            )\n",
    "            future_df = df_utils.normalize(future_df, self.data_params)\n",
    "            if len(df) > 0:\n",
    "                df = df.append(future_df)\n",
    "            else:\n",
    "                df = future_df\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"Runs the model to make predictions.\n",
    "\n",
    "        and compute stats (MSE, MAE)\n",
    "        Args:\n",
    "            df (pandas DataFrame): Dataframe with columns 'ds' datestamps, 'y' time series values and\n",
    "                other external variables\n",
    "\n",
    "        Returns:\n",
    "            df_forecast (pandas DataFrame): columns 'ds', 'y', 'trend' and ['yhat<i>']\n",
    "        \"\"\"\n",
    "        # TODO: Implement data sanity checks?\n",
    "        if self.fitted is False:\n",
    "            log.warning(\"Model has not been fitted. Predictions will be random.\")\n",
    "        dataset = self._create_dataset(df, predict_mode=True)\n",
    "        loader = DataLoader(dataset, batch_size=min(1024, len(df)), shuffle=False, drop_last=False)\n",
    "\n",
    "        predicted_vectors = list()\n",
    "        component_vectors = None\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            for inputs, _ in loader:\n",
    "                predicted = self.model.forward(inputs)\n",
    "                predicted_vectors.append(predicted.detach().numpy())\n",
    "                \n",
    "        predicted = np.concatenate(predicted_vectors)\n",
    "\n",
    "        scale_y, shift_y = self.data_params[\"y\"].scale, self.data_params[\"y\"].shift\n",
    "        predicted = predicted * scale_y + shift_y\n",
    "\n",
    "        cols = [\"ds\", \"y\"]  # cols to keep from df\n",
    "        df_forecast = pd.concat((df[cols],), axis=1)\n",
    "\n",
    "        # create a line for each forecast_lag\n",
    "        # 'yhat<i>' is the forecast for 'y' at 'ds' from i steps ago.\n",
    "        for i in range(self.n_forecasts):\n",
    "            forecast_lag = i + 1\n",
    "            forecast = predicted[:, forecast_lag - 1]\n",
    "            pad_before = self.n_lags + forecast_lag - 1\n",
    "            pad_after = self.n_forecasts - forecast_lag\n",
    "            yhat = np.concatenate(([None] * pad_before, forecast, [None] * pad_after))\n",
    "            df_forecast[\"yhat{}\".format(i + 1)] = yhat\n",
    "            df_forecast[\"residual{}\".format(i + 1)] = yhat - df_forecast[\"y\"]\n",
    "\n",
    "        \n",
    "        return df_forecast\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1839c1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>27.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-05-01 00:05:00</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-05-01 00:10:00</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ds     y\n",
       "0  2017-05-01 00:00:00  27.8\n",
       "1  2017-05-01 00:05:00  27.0\n",
       "2  2017-05-01 00:10:00  26.8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_location = \"../\"\n",
    "df = pd.read_csv(data_location + \"example_data/yosemite_temps.csv\").iloc[:1000]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ddde2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52d9fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = LSTM(n_lags = 10,\n",
    "            n_forecasts=3,\n",
    "            num_hidden_layers=1,\n",
    "            d_hidden=64,\n",
    "            learning_rate=0.1,\n",
    "            epochs=10,\n",
    "            batch_size=None,\n",
    "            loss_func=\"Huber\",\n",
    "            optimizer=\"AdamW\",\n",
    "            train_speed=None,\n",
    "            normalize=\"auto\",\n",
    "            impute_missing=True,\n",
    "            lstm_bias = True,\n",
    "            lstm_biderectional = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "787d0e0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 32\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "WARNING - (py.warnings._showwarnmsg) - /Users/polina/fds/neural_prophet/venv/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: you defined a validation_step but have no val_dataloader. Skipping validation loop\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "\n",
      "  | Name      | Type         | Params\n",
      "-------------------------------------------\n",
      "0 | lstm      | LSTM         | 19.5 K\n",
      "1 | linear    | Linear       | 195   \n",
      "2 | loss_func | SmoothL1Loss | 0     \n",
      "-------------------------------------------\n",
      "19.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.7 K    Total params\n",
      "0.079     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - /Users/polina/fds/neural_prophet/venv/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c6f144a31f46d6b1a1236b0726f410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics_df = m.fit(df, freq = '5min', validate_each_epoch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51075265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b772dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SmoothL1Loss</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.059940</td>\n",
       "      <td>9.500672</td>\n",
       "      <td>153.647242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.024996</td>\n",
       "      <td>6.748224</td>\n",
       "      <td>64.072436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005584</td>\n",
       "      <td>3.043180</td>\n",
       "      <td>14.312893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002526</td>\n",
       "      <td>1.935654</td>\n",
       "      <td>6.475463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001938</td>\n",
       "      <td>1.686954</td>\n",
       "      <td>4.966622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001854</td>\n",
       "      <td>1.557276</td>\n",
       "      <td>4.753134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001737</td>\n",
       "      <td>1.483760</td>\n",
       "      <td>4.452983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001581</td>\n",
       "      <td>1.420255</td>\n",
       "      <td>4.052020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.396395</td>\n",
       "      <td>3.845985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001354</td>\n",
       "      <td>1.307024</td>\n",
       "      <td>3.470326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SmoothL1Loss       MAE         MSE\n",
       "0      0.059940  9.500672  153.647242\n",
       "1      0.024996  6.748224   64.072436\n",
       "2      0.005584  3.043180   14.312893\n",
       "3      0.002526  1.935654    6.475463\n",
       "4      0.001938  1.686954    4.966622\n",
       "5      0.001854  1.557276    4.753134\n",
       "6      0.001737  1.483760    4.452983\n",
       "7      0.001581  1.420255    4.052020\n",
       "8      0.001500  1.396395    3.845985\n",
       "9      0.001354  1.307024    3.470326"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1882f6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75fa6b02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - /Users/polina/fds/neural_prophet/venv/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee85a826721b4e47a0b6082582f1402d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SmoothL1Loss</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001187</td>\n",
       "      <td>1.285098</td>\n",
       "      <td>3.042945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SmoothL1Loss       MAE       MSE\n",
       "0      0.001187  1.285098  3.042945"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69dc618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23ef74c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d473b9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of forecast steps is defined by n_forecasts. Adjusted to 3.\n"
     ]
    }
   ],
   "source": [
    "x = m.make_future_dataframe(df, periods=100, n_historic_predictions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77a1efb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>yhat1</th>\n",
       "      <th>residual1</th>\n",
       "      <th>yhat2</th>\n",
       "      <th>residual2</th>\n",
       "      <th>yhat3</th>\n",
       "      <th>residual3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-04 09:40:00</td>\n",
       "      <td>8.7</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-05-04 09:45:00</td>\n",
       "      <td>8.5</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-05-04 09:50:00</td>\n",
       "      <td>8.4</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-05-04 09:55:00</td>\n",
       "      <td>8.6</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-05-04 10:00:00</td>\n",
       "      <td>8.5</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-05-04 10:05:00</td>\n",
       "      <td>8.5</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-05-04 10:10:00</td>\n",
       "      <td>8.7</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-05-04 10:15:00</td>\n",
       "      <td>8.6</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-05-04 10:20:00</td>\n",
       "      <td>8.5</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-05-04 10:25:00</td>\n",
       "      <td>8.3</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017-05-04 10:30:00</td>\n",
       "      <td>8.4</td>\n",
       "      <td>7.605643</td>\n",
       "      <td>-0.794357</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017-05-04 10:35:00</td>\n",
       "      <td>8.3</td>\n",
       "      <td>8.846068</td>\n",
       "      <td>0.546068</td>\n",
       "      <td>8.608027</td>\n",
       "      <td>0.308027</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017-05-04 10:40:00</td>\n",
       "      <td>8.4</td>\n",
       "      <td>8.966639</td>\n",
       "      <td>0.566639</td>\n",
       "      <td>9.043421</td>\n",
       "      <td>0.643421</td>\n",
       "      <td>10.061571</td>\n",
       "      <td>1.661571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017-05-04 10:45:00</td>\n",
       "      <td>8.3</td>\n",
       "      <td>8.885863</td>\n",
       "      <td>0.585863</td>\n",
       "      <td>8.870827</td>\n",
       "      <td>0.570827</td>\n",
       "      <td>9.670608</td>\n",
       "      <td>1.370608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017-05-04 10:50:00</td>\n",
       "      <td>8.4</td>\n",
       "      <td>8.760267</td>\n",
       "      <td>0.360267</td>\n",
       "      <td>8.726901</td>\n",
       "      <td>0.326901</td>\n",
       "      <td>9.068205</td>\n",
       "      <td>0.668205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017-05-04 10:55:00</td>\n",
       "      <td>8.3</td>\n",
       "      <td>8.67764</td>\n",
       "      <td>0.37764</td>\n",
       "      <td>8.617004</td>\n",
       "      <td>0.317004</td>\n",
       "      <td>8.725646</td>\n",
       "      <td>0.425646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017-05-04 11:00:00</td>\n",
       "      <td>8.2</td>\n",
       "      <td>8.594065</td>\n",
       "      <td>0.394065</td>\n",
       "      <td>8.575699</td>\n",
       "      <td>0.375699</td>\n",
       "      <td>8.551384</td>\n",
       "      <td>0.351384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017-05-04 11:05:00</td>\n",
       "      <td>8.2</td>\n",
       "      <td>8.536319</td>\n",
       "      <td>0.336319</td>\n",
       "      <td>8.534701</td>\n",
       "      <td>0.334701</td>\n",
       "      <td>8.490426</td>\n",
       "      <td>0.290426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017-05-04 11:10:00</td>\n",
       "      <td>8.2</td>\n",
       "      <td>8.497712</td>\n",
       "      <td>0.297712</td>\n",
       "      <td>8.500061</td>\n",
       "      <td>0.300061</td>\n",
       "      <td>8.460813</td>\n",
       "      <td>0.260813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2017-05-04 11:15:00</td>\n",
       "      <td>8.1</td>\n",
       "      <td>8.479259</td>\n",
       "      <td>0.379259</td>\n",
       "      <td>8.470301</td>\n",
       "      <td>0.370301</td>\n",
       "      <td>8.437881</td>\n",
       "      <td>0.337881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2017-05-04 11:20:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.441754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.451024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.419638</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2017-05-04 11:25:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.40943</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.407062</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2017-05-04 11:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.372431</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ds    y     yhat1 residual1     yhat2 residual2  \\\n",
       "0  2017-05-04 09:40:00  8.7      None       NaN      None       NaN   \n",
       "1  2017-05-04 09:45:00  8.5      None       NaN      None       NaN   \n",
       "2  2017-05-04 09:50:00  8.4      None       NaN      None       NaN   \n",
       "3  2017-05-04 09:55:00  8.6      None       NaN      None       NaN   \n",
       "4  2017-05-04 10:00:00  8.5      None       NaN      None       NaN   \n",
       "5  2017-05-04 10:05:00  8.5      None       NaN      None       NaN   \n",
       "6  2017-05-04 10:10:00  8.7      None       NaN      None       NaN   \n",
       "7  2017-05-04 10:15:00  8.6      None       NaN      None       NaN   \n",
       "8  2017-05-04 10:20:00  8.5      None       NaN      None       NaN   \n",
       "9  2017-05-04 10:25:00  8.3      None       NaN      None       NaN   \n",
       "10 2017-05-04 10:30:00  8.4  7.605643 -0.794357      None       NaN   \n",
       "11 2017-05-04 10:35:00  8.3  8.846068  0.546068  8.608027  0.308027   \n",
       "12 2017-05-04 10:40:00  8.4  8.966639  0.566639  9.043421  0.643421   \n",
       "13 2017-05-04 10:45:00  8.3  8.885863  0.585863  8.870827  0.570827   \n",
       "14 2017-05-04 10:50:00  8.4  8.760267  0.360267  8.726901  0.326901   \n",
       "15 2017-05-04 10:55:00  8.3   8.67764   0.37764  8.617004  0.317004   \n",
       "16 2017-05-04 11:00:00  8.2  8.594065  0.394065  8.575699  0.375699   \n",
       "17 2017-05-04 11:05:00  8.2  8.536319  0.336319  8.534701  0.334701   \n",
       "18 2017-05-04 11:10:00  8.2  8.497712  0.297712  8.500061  0.300061   \n",
       "19 2017-05-04 11:15:00  8.1  8.479259  0.379259  8.470301  0.370301   \n",
       "20 2017-05-04 11:20:00  NaN  8.441754       NaN  8.451024       NaN   \n",
       "21 2017-05-04 11:25:00  NaN      None       NaN   8.40943       NaN   \n",
       "22 2017-05-04 11:30:00  NaN      None       NaN      None       NaN   \n",
       "\n",
       "        yhat3 residual3  \n",
       "0        None       NaN  \n",
       "1        None       NaN  \n",
       "2        None       NaN  \n",
       "3        None       NaN  \n",
       "4        None       NaN  \n",
       "5        None       NaN  \n",
       "6        None       NaN  \n",
       "7        None       NaN  \n",
       "8        None       NaN  \n",
       "9        None       NaN  \n",
       "10       None       NaN  \n",
       "11       None       NaN  \n",
       "12  10.061571  1.661571  \n",
       "13   9.670608  1.370608  \n",
       "14   9.068205  0.668205  \n",
       "15   8.725646  0.425646  \n",
       "16   8.551384  0.351384  \n",
       "17   8.490426  0.290426  \n",
       "18   8.460813  0.260813  \n",
       "19   8.437881  0.337881  \n",
       "20   8.419638       NaN  \n",
       "21   8.407062       NaN  \n",
       "22   8.372431       NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3286984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845f4729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327b6930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76381b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46edcbba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3578b5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "297c8d33",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-259-e0709dc6266c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(df.ds, df.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ab5629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "62f7aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_utils.check_dataframe(\n",
    "            df, check_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "39608c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/400 [48:28<?, ?it/s]\n",
      "  0%|          | 0/400 [42:29<?, ?it/s]\n",
      "  0%|          | 0/400 [40:39<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "df = m._handle_missing_data(df, freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "dd4e8df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = df_utils.split_df(df, n_lags=10, n_forecasts=1, valid_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "f7c8f1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fd33d1732b0>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m._init_train_loader(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "697d8266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>t</th>\n",
       "      <th>y_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-01</td>\n",
       "      <td>27.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.613466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-05-02</td>\n",
       "      <td>29.4</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.653367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-05-03</td>\n",
       "      <td>31.3</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>0.700748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-05-04</td>\n",
       "      <td>33.1</td>\n",
       "      <td>0.046154</td>\n",
       "      <td>0.745636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-05-05</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>0.758105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>42.2</td>\n",
       "      <td>0.938462</td>\n",
       "      <td>0.972569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2017-07-02</td>\n",
       "      <td>40.6</td>\n",
       "      <td>0.953846</td>\n",
       "      <td>0.932668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2017-07-03</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.942643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2017-07-04</td>\n",
       "      <td>40.9</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.940150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2017-07-05</td>\n",
       "      <td>41.4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ds     y         t  y_scaled\n",
       "0  2017-05-01  27.8  0.000000  0.613466\n",
       "1  2017-05-02  29.4  0.015385  0.653367\n",
       "2  2017-05-03  31.3  0.030769  0.700748\n",
       "3  2017-05-04  33.1  0.046154  0.745636\n",
       "4  2017-05-05  33.6  0.061538  0.758105\n",
       "..        ...   ...       ...       ...\n",
       "61 2017-07-01  42.2  0.938462  0.972569\n",
       "62 2017-07-02  40.6  0.953846  0.932668\n",
       "63 2017-07-03  41.0  0.969231  0.942643\n",
       "64 2017-07-04  40.9  0.984615  0.940150\n",
       "65 2017-07-05  41.4  1.000000  0.952618\n",
       "\n",
       "[66 rows x 4 columns]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "8421c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time_dataset.TimeDataset(\n",
    "            df,\n",
    "            n_lags=n_lags,\n",
    "            n_forecasts=n_forecasts,\n",
    "            predict_mode=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c933702d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "087fa362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<neuralprophet.time_dataset.TimeDataset at 0x7fd33d1cabb0>"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e8f526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f9c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b2d7bd99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 16\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e607b389fc4a9ca063b48378463224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<neuralprophet.time_dataset.TimeDataset object at 0x7fd3381d0a00>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-5390be02660b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-111-e25c71143256>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, df, freq, epochs, validate_each_epoch, valid_p, progress_bar, plot_live_loss)\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mmetrics_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_live_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplot_live_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0mmetrics_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_live_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplot_live_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-111-e25c71143256>\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, df, df_val, progress_bar, plot_live_loss)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 )\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_train_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_val\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;31m## Metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-111-e25c71143256>\u001b[0m in \u001b[0;36m_init_train_loader\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             self.config_train.learning_rate = utils_torch.lr_range_test(\n\u001b[0m\u001b[1;32m    240\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                 \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/neuralprophet/utils_torch.py\u001b[0m in \u001b[0;36mlr_range_test\u001b[0;34m(model, dataset, loss_func, optimizer, batch_size, num_iter, skip_start, skip_end, start_lr, end_lr, plot)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHiddenPrints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mlr_finder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLRFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlrtest_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         lr_finder.range_test(\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mlrtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlrtest_loader_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/torch_lr_finder/lr_finder.py\u001b[0m in \u001b[0;36mrange_test\u001b[0;34m(self, train_loader, val_loader, start_lr, end_lr, num_iter, step_mode, smooth_f, diverge_th, accumulation_steps, non_blocking_transfer)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;31m# Train on batch and retrieve loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             loss = self._train_batch(\n\u001b[0m\u001b[1;32m    318\u001b[0m                 \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0maccumulation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/torch_lr_finder/lr_finder.py\u001b[0m in \u001b[0;36m_train_batch\u001b[0;34m(self, train_iter, accumulation_steps, non_blocking_transfer)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-107-085ea130bd53>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# in lightning, forward defines the prediction/inference actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# lstm_out = (batch_size, seq_len, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m             \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m             \u001b[0mmax_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m             \u001b[0msorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "m.fit(df, freq = 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa17e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09a2749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "27817053",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = LSTM(input_size = 10,\n",
    "     hidden_size = 64,\n",
    "     num_layers = 100,\n",
    "     bias = False,\n",
    "     bidirectional = False\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "53e08193",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-fafacc626c8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "a.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88db928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acff9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a1689d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8068b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesDataset(Dataset):   \n",
    "    '''\n",
    "    Custom Dataset subclass. \n",
    "    Serves as input to DataLoader to transform X \n",
    "      into sequence data using rolling window. \n",
    "    DataLoader using this dataset will output batches \n",
    "      of `(batch_size, seq_len, n_features)` shape.\n",
    "    Suitable as an input to RNNs. \n",
    "    '''\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, seq_len: int = 1):\n",
    "        self.X = torch.tensor(X).float()\n",
    "        self.y = torch.tensor(y).float()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.__len__() - (self.seq_len-1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.X[index:index+self.seq_len], self.y[index+self.seq_len-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2af5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSDataModule(pl.LightningDataModule):\n",
    "    '''\n",
    "    PyTorch Lighting DataModule subclass:\n",
    "    https://pytorch-lightning.readthedocs.io/en/latest/datamodules.html\n",
    "\n",
    "    Serves the purpose of aggregating all data loading \n",
    "      and processing work in one place.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, seq_len = 1, batch_size = 128, num_workers=0):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_val = None\n",
    "        self.y_val = None\n",
    "        self.X_test = None\n",
    "        self.X_test = None\n",
    "        self.columns = None\n",
    "        self.preprocessing = None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        '''\n",
    "        Data is resampled to hourly intervals.\n",
    "        Both 'np.nan' and '?' are converted to 'np.nan'\n",
    "        'Date' and 'Time' columns are merged into 'dt' index\n",
    "        '''\n",
    "\n",
    "        if stage == 'fit' and self.X_train is not None:\n",
    "            return \n",
    "        if stage == 'test' and self.X_test is not None:\n",
    "            return\n",
    "        if stage is None and self.X_train is not None and self.X_test is not None:  \n",
    "            return\n",
    "        \n",
    "        \n",
    "        df = pd.read_csv(\n",
    "            path, \n",
    "            sep=';', \n",
    "            parse_dates={'dt' : ['Date', 'Time']}, \n",
    "            infer_datetime_format=True, \n",
    "            low_memory=False, \n",
    "            na_values=['nan','?'], \n",
    "            index_col='dt'\n",
    "        )\n",
    "\n",
    "        df_resample = df.resample('h').mean()\n",
    "\n",
    "        X = df_resample.dropna().copy()\n",
    "        y = X['Global_active_power'].shift(-1).ffill()\n",
    "        self.columns = X.columns\n",
    "\n",
    "\n",
    "        X_cv, X_test, y_cv, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, shuffle=False\n",
    "        )\n",
    "    \n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_cv, y_cv, test_size=0.25, shuffle=False\n",
    "        )\n",
    "\n",
    "        preprocessing = StandardScaler()\n",
    "        preprocessing.fit(X_train)\n",
    "\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.X_train = preprocessing.transform(X_train)\n",
    "            self.y_train = y_train.values.reshape((-1, 1))\n",
    "            self.X_val = preprocessing.transform(X_val)\n",
    "            self.y_val = y_val.values.reshape((-1, 1))\n",
    "\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.X_test = preprocessing.transform(X_test)\n",
    "            self.y_test = y_test.values.reshape((-1, 1))\n",
    "        \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = TimeseriesDataset(self.X_train, \n",
    "                                          self.y_train, \n",
    "                                          seq_len=self.seq_len)\n",
    "        train_loader = DataLoader(train_dataset, \n",
    "                                  batch_size = self.batch_size, \n",
    "                                  shuffle = False, \n",
    "                                  num_workers = self.num_workers)\n",
    "        \n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = TimeseriesDataset(self.X_val, \n",
    "                                        self.y_val, \n",
    "                                        seq_len=self.seq_len)\n",
    "        val_loader = DataLoader(val_dataset, \n",
    "                                batch_size = self.batch_size, \n",
    "                                shuffle = False, \n",
    "                                num_workers = self.num_workers)\n",
    "\n",
    "        return val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_dataset = TimeseriesDataset(self.X_test, \n",
    "                                         self.y_test, \n",
    "                                         seq_len=self.seq_len)\n",
    "        test_loader = DataLoader(test_dataset, \n",
    "                                 batch_size = self.batch_size, \n",
    "                                 shuffle = False, \n",
    "                                 num_workers = self.num_workers)\n",
    "\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f040d41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "595ff8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lags = 10\n",
    "n_forecasts = 1\n",
    "valid_p = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1407fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639913c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3effb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "216e823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = '5min'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "588b5ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_tsd = df.copy(deep = True)\n",
    "df_for_tsd['ds'] = pd.to_datetime(df_for_tsd['ds'])\n",
    "df_for_tsd = pd.DataFrame(pd.date_range(start=df_for_tsd.ds.min(), end=df_for_tsd.ds.max(), freq = freq),\n",
    "             columns = ['ds']).merge(df_for_tsd, how = 'left')\n",
    "df_for_tsd = df_for_tsd.sort_values('ds')\n",
    "df_for_tsd = df_for_tsd.reset_index()\n",
    "df_for_tsd.columns = ['time_idx', 'date', 'value']\n",
    "df_for_tsd['series'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2f0714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "77c55e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_idx</th>\n",
       "      <th>date</th>\n",
       "      <th>value</th>\n",
       "      <th>series</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>27.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-05-01 00:05:00</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-05-01 00:10:00</td>\n",
       "      <td>26.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2017-05-01 00:15:00</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2017-05-01 00:20:00</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18716</th>\n",
       "      <td>18716</td>\n",
       "      <td>2017-07-04 23:40:00</td>\n",
       "      <td>42.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18717</th>\n",
       "      <td>18717</td>\n",
       "      <td>2017-07-04 23:45:00</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18718</th>\n",
       "      <td>18718</td>\n",
       "      <td>2017-07-04 23:50:00</td>\n",
       "      <td>42.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18719</th>\n",
       "      <td>18719</td>\n",
       "      <td>2017-07-04 23:55:00</td>\n",
       "      <td>42.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18720</th>\n",
       "      <td>18720</td>\n",
       "      <td>2017-07-05 00:00:00</td>\n",
       "      <td>41.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18721 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       time_idx                date  value  series\n",
       "0             0 2017-05-01 00:00:00   27.8       0\n",
       "1             1 2017-05-01 00:05:00   27.0       0\n",
       "2             2 2017-05-01 00:10:00   26.8       0\n",
       "3             3 2017-05-01 00:15:00   26.5       0\n",
       "4             4 2017-05-01 00:20:00   25.6       0\n",
       "...         ...                 ...    ...     ...\n",
       "18716     18716 2017-07-04 23:40:00   42.8       0\n",
       "18717     18717 2017-07-04 23:45:00   43.0       0\n",
       "18718     18718 2017-07-04 23:50:00   42.1       0\n",
       "18719     18719 2017-07-04 23:55:00   42.1       0\n",
       "18720     18720 2017-07-05 00:00:00   41.4       0\n",
       "\n",
       "[18721 rows x 4 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_tsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57694d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9214f2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "dcc8815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lags = 10\n",
    "n_forecasts = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "ab6d6e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time_idx                     50\n",
       "date        2017-05-01 04:10:00\n",
       "value                       8.5\n",
       "series                        0\n",
       "Name: 50, dtype: object"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_tsd.iloc[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "6e7a71ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "# import dataset, network to train and metric to optimize\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, QuantileLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "33e2b909",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "12 (0.08%) of value values were found to be NA or infinite (even after encoding). NA values are not allowed `allow_missings` refers to missing rows, not to missing values. Possible strategies to fix the issue are (a) dropping the variable value, (b) using `NaNLabelEncoder(add_nan=True)` for categorical variables, (c) filling missing values and/or (d) optionally adding a variable indicating filled values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-257-ac623e10f3cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtraining_cutoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2017-06-20 04:10:00\"\u001b[0m  \u001b[0;31m# day for cutoff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m training = TimeSeriesDataSet(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdf_for_tsd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtraining_cutoff\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtime_idx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'time_idx'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# column name of time of observation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/pytorch_forecasting/data/timeseries.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, time_idx, target, group_ids, weight, max_encoder_length, min_encoder_length, min_prediction_idx, min_prediction_length, max_prediction_length, static_categoricals, static_reals, time_varying_known_categoricals, time_varying_known_reals, time_varying_unknown_categoricals, time_varying_unknown_reals, variable_groups, dropout_categoricals, constant_fill_strategy, allow_missings, lags, add_relative_time_idx, add_target_scales, add_encoder_length, target_normalizer, categorical_encoders, scalers, randomize_length, predict_mode)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# convert to torch tensor for high performance data loading later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_lagged_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/pytorch_forecasting/data/timeseries.py\u001b[0m in \u001b[0;36m_data_to_tensors\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                 target = [\n\u001b[0;32m--> 963\u001b[0;31m                     check_for_nonfinite(\n\u001b[0m\u001b[1;32m    964\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"__target__{self.target}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/pytorch_forecasting/data/timeseries.py\u001b[0m in \u001b[0;36mcheck_for_nonfinite\u001b[0;34m(tensor, names)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mna\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0;34mf\"{na} ({na/tensor.size(0):.2%}) of {name} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;34m\"values were found to be NA or infinite (even after encoding). NA values are not allowed \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 12 (0.08%) of value values were found to be NA or infinite (even after encoding). NA values are not allowed `allow_missings` refers to missing rows, not to missing values. Possible strategies to fix the issue are (a) dropping the variable value, (b) using `NaNLabelEncoder(add_nan=True)` for categorical variables, (c) filling missing values and/or (d) optionally adding a variable indicating filled values"
     ]
    }
   ],
   "source": [
    "# define the dataset, i.e. add metadata to pandas dataframe for the model to understand it\n",
    "max_encoder_length = 36\n",
    "max_prediction_length = 6\n",
    "training_cutoff = \"2017-06-20 04:10:00\"  # day for cutoff\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    df_for_tsd[lambda x: x.date <= training_cutoff],\n",
    "    time_idx= 'time_idx',  # column name of time of observation\n",
    "    target= 'value',  # column name of target to predict\n",
    "    group_ids= ['series'],  # column name(s) for timeseries IDs\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "a6b2a719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pytorch_forecasting.data.timeseries.TimeSeriesDataSet at 0x7fd359af3df0>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "10db88b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation dataset using the same normalization techniques as for the training dataset\n",
    "validation = TimeSeriesDataSet.from_dataset(training, df_for_tsd,\n",
    "                                            min_prediction_idx=training.index.time.max() + 1,\n",
    "                                            stop_randomization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "65f9dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datasets to dataloaders for training\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "2c7b4ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "503d8a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'encoder_cat': tensor([], size=(25, 36, 0), dtype=torch.int64), 'encoder_cont': tensor([], size=(25, 36, 0)), 'encoder_target': tensor([[27.8000, 29.4000, 31.3000, 33.1000, 33.6000, 22.7000,  4.4000, 11.2000,\n",
      "         19.9000, 26.5000, 27.6000, 19.1000, 17.5000, 20.8000, 16.3000,  3.2000,\n",
      "          7.3000, 22.0000, 26.3000, 28.0000, 31.6000, 33.3000, 36.1000, 35.0000,\n",
      "         31.9000, 31.6000, 29.8000, 34.9000, 37.5000, 36.1000, 27.5000, 12.8000,\n",
      "         26.2000, 37.7000, 37.3000, 36.9000],\n",
      "        [29.4000, 31.3000, 33.1000, 33.6000, 22.7000,  4.4000, 11.2000, 19.9000,\n",
      "         26.5000, 27.6000, 19.1000, 17.5000, 20.8000, 16.3000,  3.2000,  7.3000,\n",
      "         22.0000, 26.3000, 28.0000, 31.6000, 33.3000, 36.1000, 35.0000, 31.9000,\n",
      "         31.6000, 29.8000, 34.9000, 37.5000, 36.1000, 27.5000, 12.8000, 26.2000,\n",
      "         37.7000, 37.3000, 36.9000, 37.2000],\n",
      "        [31.3000, 33.1000, 33.6000, 22.7000,  4.4000, 11.2000, 19.9000, 26.5000,\n",
      "         27.6000, 19.1000, 17.5000, 20.8000, 16.3000,  3.2000,  7.3000, 22.0000,\n",
      "         26.3000, 28.0000, 31.6000, 33.3000, 36.1000, 35.0000, 31.9000, 31.6000,\n",
      "         29.8000, 34.9000, 37.5000, 36.1000, 27.5000, 12.8000, 26.2000, 37.7000,\n",
      "         37.3000, 36.9000, 37.2000, 38.2000],\n",
      "        [33.1000, 33.6000, 22.7000,  4.4000, 11.2000, 19.9000, 26.5000, 27.6000,\n",
      "         19.1000, 17.5000, 20.8000, 16.3000,  3.2000,  7.3000, 22.0000, 26.3000,\n",
      "         28.0000, 31.6000, 33.3000, 36.1000, 35.0000, 31.9000, 31.6000, 29.8000,\n",
      "         34.9000, 37.5000, 36.1000, 27.5000, 12.8000, 26.2000, 37.7000, 37.3000,\n",
      "         36.9000, 37.2000, 38.2000, 30.0000],\n",
      "        [33.6000, 22.7000,  4.4000, 11.2000, 19.9000, 26.5000, 27.6000, 19.1000,\n",
      "         17.5000, 20.8000, 16.3000,  3.2000,  7.3000, 22.0000, 26.3000, 28.0000,\n",
      "         31.6000, 33.3000, 36.1000, 35.0000, 31.9000, 31.6000, 29.8000, 34.9000,\n",
      "         37.5000, 36.1000, 27.5000, 12.8000, 26.2000, 37.7000, 37.3000, 36.9000,\n",
      "         37.2000, 38.2000, 30.0000, 13.4000],\n",
      "        [22.7000,  4.4000, 11.2000, 19.9000, 26.5000, 27.6000, 19.1000, 17.5000,\n",
      "         20.8000, 16.3000,  3.2000,  7.3000, 22.0000, 26.3000, 28.0000, 31.6000,\n",
      "         33.3000, 36.1000, 35.0000, 31.9000, 31.6000, 29.8000, 34.9000, 37.5000,\n",
      "         36.1000, 27.5000, 12.8000, 26.2000, 37.7000, 37.3000, 36.9000, 37.2000,\n",
      "         38.2000, 30.0000, 13.4000, 12.1000],\n",
      "        [ 4.4000, 11.2000, 19.9000, 26.5000, 27.6000, 19.1000, 17.5000, 20.8000,\n",
      "         16.3000,  3.2000,  7.3000, 22.0000, 26.3000, 28.0000, 31.6000, 33.3000,\n",
      "         36.1000, 35.0000, 31.9000, 31.6000, 29.8000, 34.9000, 37.5000, 36.1000,\n",
      "         27.5000, 12.8000, 26.2000, 37.7000, 37.3000, 36.9000, 37.2000, 38.2000,\n",
      "         30.0000, 13.4000, 12.1000, 22.8000],\n",
      "        [11.2000, 19.9000, 26.5000, 27.6000, 19.1000, 17.5000, 20.8000, 16.3000,\n",
      "          3.2000,  7.3000, 22.0000, 26.3000, 28.0000, 31.6000, 33.3000, 36.1000,\n",
      "         35.0000, 31.9000, 31.6000, 29.8000, 34.9000, 37.5000, 36.1000, 27.5000,\n",
      "         12.8000, 26.2000, 37.7000, 37.3000, 36.9000, 37.2000, 38.2000, 30.0000,\n",
      "         13.4000, 12.1000, 22.8000,  8.9000],\n",
      "        [19.9000, 26.5000, 27.6000, 19.1000, 17.5000, 20.8000, 16.3000,  3.2000,\n",
      "          7.3000, 22.0000, 26.3000, 28.0000, 31.6000, 33.3000, 36.1000, 35.0000,\n",
      "         31.9000, 31.6000, 29.8000, 34.9000, 37.5000, 36.1000, 27.5000, 12.8000,\n",
      "         26.2000, 37.7000, 37.3000, 36.9000, 37.2000, 38.2000, 30.0000, 13.4000,\n",
      "         12.1000, 22.8000,  8.9000, 12.4000],\n",
      "        [26.5000, 27.6000, 19.1000, 17.5000, 20.8000, 16.3000,  3.2000,  7.3000,\n",
      "         22.0000, 26.3000, 28.0000, 31.6000, 33.3000, 36.1000, 35.0000, 31.9000,\n",
      "         31.6000, 29.8000, 34.9000, 37.5000, 36.1000, 27.5000, 12.8000, 26.2000,\n",
      "         37.7000, 37.3000, 36.9000, 37.2000, 38.2000, 30.0000, 13.4000, 12.1000,\n",
      "         22.8000,  8.9000, 12.4000, 28.5000],\n",
      "        [27.6000, 19.1000, 17.5000, 20.8000, 16.3000,  3.2000,  7.3000, 22.0000,\n",
      "         26.3000, 28.0000, 31.6000, 33.3000, 36.1000, 35.0000, 31.9000, 31.6000,\n",
      "         29.8000, 34.9000, 37.5000, 36.1000, 27.5000, 12.8000, 26.2000, 37.7000,\n",
      "         37.3000, 36.9000, 37.2000, 38.2000, 30.0000, 13.4000, 12.1000, 22.8000,\n",
      "          8.9000, 12.4000, 28.5000, 33.9000],\n",
      "        [19.1000, 17.5000, 20.8000, 16.3000,  3.2000,  7.3000, 22.0000, 26.3000,\n",
      "         28.0000, 31.6000, 33.3000, 36.1000, 35.0000, 31.9000, 31.6000, 29.8000,\n",
      "         34.9000, 37.5000, 36.1000, 27.5000, 12.8000, 26.2000, 37.7000, 37.3000,\n",
      "         36.9000, 37.2000, 38.2000, 30.0000, 13.4000, 12.1000, 22.8000,  8.9000,\n",
      "         12.4000, 28.5000, 33.9000, 36.6000],\n",
      "        [17.5000, 20.8000, 16.3000,  3.2000,  7.3000, 22.0000, 26.3000, 28.0000,\n",
      "         31.6000, 33.3000, 36.1000, 35.0000, 31.9000, 31.6000, 29.8000, 34.9000,\n",
      "         37.5000, 36.1000, 27.5000, 12.8000, 26.2000, 37.7000, 37.3000, 36.9000,\n",
      "         37.2000, 38.2000, 30.0000, 13.4000, 12.1000, 22.8000,  8.9000, 12.4000,\n",
      "         28.5000, 33.9000, 36.6000, 39.8000],\n",
      "        [20.8000, 16.3000,  3.2000,  7.3000, 22.0000, 26.3000, 28.0000, 31.6000,\n",
      "         33.3000, 36.1000, 35.0000, 31.9000, 31.6000, 29.8000, 34.9000, 37.5000,\n",
      "         36.1000, 27.5000, 12.8000, 26.2000, 37.7000, 37.3000, 36.9000, 37.2000,\n",
      "         38.2000, 30.0000, 13.4000, 12.1000, 22.8000,  8.9000, 12.4000, 28.5000,\n",
      "         33.9000, 36.6000, 39.8000, 40.8000],\n",
      "        [16.3000,  3.2000,  7.3000, 22.0000, 26.3000, 28.0000, 31.6000, 33.3000,\n",
      "         36.1000, 35.0000, 31.9000, 31.6000, 29.8000, 34.9000, 37.5000, 36.1000,\n",
      "         27.5000, 12.8000, 26.2000, 37.7000, 37.3000, 36.9000, 37.2000, 38.2000,\n",
      "         30.0000, 13.4000, 12.1000, 22.8000,  8.9000, 12.4000, 28.5000, 33.9000,\n",
      "         36.6000, 39.8000, 40.8000, 44.9000],\n",
      "        [ 3.2000,  7.3000, 22.0000, 26.3000, 28.0000, 31.6000, 33.3000, 36.1000,\n",
      "         35.0000, 31.9000, 31.6000, 29.8000, 34.9000, 37.5000, 36.1000, 27.5000,\n",
      "         12.8000, 26.2000, 37.7000, 37.3000, 36.9000, 37.2000, 38.2000, 30.0000,\n",
      "         13.4000, 12.1000, 22.8000,  8.9000, 12.4000, 28.5000, 33.9000, 36.6000,\n",
      "         39.8000, 40.8000, 44.9000, 43.1000],\n",
      "        [ 7.3000, 22.0000, 26.3000, 28.0000, 31.6000, 33.3000, 36.1000, 35.0000,\n",
      "         31.9000, 31.6000, 29.8000, 34.9000, 37.5000, 36.1000, 27.5000, 12.8000,\n",
      "         26.2000, 37.7000, 37.3000, 36.9000, 37.2000, 38.2000, 30.0000, 13.4000,\n",
      "         12.1000, 22.8000,  8.9000, 12.4000, 28.5000, 33.9000, 36.6000, 39.8000,\n",
      "         40.8000, 44.9000, 43.1000, 43.6000],\n",
      "        [22.0000, 26.3000, 28.0000, 31.6000, 33.3000, 36.1000, 35.0000, 31.9000,\n",
      "         31.6000, 29.8000, 34.9000, 37.5000, 36.1000, 27.5000, 12.8000, 26.2000,\n",
      "         37.7000, 37.3000, 36.9000, 37.2000, 38.2000, 30.0000, 13.4000, 12.1000,\n",
      "         22.8000,  8.9000, 12.4000, 28.5000, 33.9000, 36.6000, 39.8000, 40.8000,\n",
      "         44.9000, 43.1000, 43.6000, 42.5000],\n",
      "        [26.3000, 28.0000, 31.6000, 33.3000, 36.1000, 35.0000, 31.9000, 31.6000,\n",
      "         29.8000, 34.9000, 37.5000, 36.1000, 27.5000, 12.8000, 26.2000, 37.7000,\n",
      "         37.3000, 36.9000, 37.2000, 38.2000, 30.0000, 13.4000, 12.1000, 22.8000,\n",
      "          8.9000, 12.4000, 28.5000, 33.9000, 36.6000, 39.8000, 40.8000, 44.9000,\n",
      "         43.1000, 43.6000, 42.5000, 44.1000],\n",
      "        [28.0000, 31.6000, 33.3000, 36.1000, 35.0000, 31.9000, 31.6000, 29.8000,\n",
      "         34.9000, 37.5000, 36.1000, 27.5000, 12.8000, 26.2000, 37.7000, 37.3000,\n",
      "         36.9000, 37.2000, 38.2000, 30.0000, 13.4000, 12.1000, 22.8000,  8.9000,\n",
      "         12.4000, 28.5000, 33.9000, 36.6000, 39.8000, 40.8000, 44.9000, 43.1000,\n",
      "         43.6000, 42.5000, 44.1000, 43.3000],\n",
      "        [31.6000, 33.3000, 36.1000, 35.0000, 31.9000, 31.6000, 29.8000, 34.9000,\n",
      "         37.5000, 36.1000, 27.5000, 12.8000, 26.2000, 37.7000, 37.3000, 36.9000,\n",
      "         37.2000, 38.2000, 30.0000, 13.4000, 12.1000, 22.8000,  8.9000, 12.4000,\n",
      "         28.5000, 33.9000, 36.6000, 39.8000, 40.8000, 44.9000, 43.1000, 43.6000,\n",
      "         42.5000, 44.1000, 43.3000, 24.4000],\n",
      "        [33.3000, 36.1000, 35.0000, 31.9000, 31.6000, 29.8000, 34.9000, 37.5000,\n",
      "         36.1000, 27.5000, 12.8000, 26.2000, 37.7000, 37.3000, 36.9000, 37.2000,\n",
      "         38.2000, 30.0000, 13.4000, 12.1000, 22.8000,  8.9000, 12.4000, 28.5000,\n",
      "         33.9000, 36.6000, 39.8000, 40.8000, 44.9000, 43.1000, 43.6000, 42.5000,\n",
      "         44.1000, 43.3000, 24.4000, 40.7000],\n",
      "        [36.1000, 35.0000, 31.9000, 31.6000, 29.8000, 34.9000, 37.5000, 36.1000,\n",
      "         27.5000, 12.8000, 26.2000, 37.7000, 37.3000, 36.9000, 37.2000, 38.2000,\n",
      "         30.0000, 13.4000, 12.1000, 22.8000,  8.9000, 12.4000, 28.5000, 33.9000,\n",
      "         36.6000, 39.8000, 40.8000, 44.9000, 43.1000, 43.6000, 42.5000, 44.1000,\n",
      "         43.3000, 24.4000, 40.7000, 39.4000],\n",
      "        [35.0000, 31.9000, 31.6000, 29.8000, 34.9000, 37.5000, 36.1000, 27.5000,\n",
      "         12.8000, 26.2000, 37.7000, 37.3000, 36.9000, 37.2000, 38.2000, 30.0000,\n",
      "         13.4000, 12.1000, 22.8000,  8.9000, 12.4000, 28.5000, 33.9000, 36.6000,\n",
      "         39.8000, 40.8000, 44.9000, 43.1000, 43.6000, 42.5000, 44.1000, 43.3000,\n",
      "         24.4000, 40.7000, 39.4000, 39.6000],\n",
      "        [31.9000, 31.6000, 29.8000, 34.9000, 37.5000, 36.1000, 27.5000, 12.8000,\n",
      "         26.2000, 37.7000, 37.3000, 36.9000, 37.2000, 38.2000, 30.0000, 13.4000,\n",
      "         12.1000, 22.8000,  8.9000, 12.4000, 28.5000, 33.9000, 36.6000, 39.8000,\n",
      "         40.8000, 44.9000, 43.1000, 43.6000, 42.5000, 44.1000, 43.3000, 24.4000,\n",
      "         40.7000, 39.4000, 39.6000, 39.9000]]), 'encoder_lengths': tensor([36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,\n",
      "        36, 36, 36, 36, 36, 36, 36]), 'decoder_cat': tensor([], size=(25, 6, 0), dtype=torch.int64), 'decoder_cont': tensor([], size=(25, 6, 0)), 'decoder_target': tensor([[37.2000, 38.2000, 30.0000, 13.4000, 12.1000, 22.8000],\n",
      "        [38.2000, 30.0000, 13.4000, 12.1000, 22.8000,  8.9000],\n",
      "        [30.0000, 13.4000, 12.1000, 22.8000,  8.9000, 12.4000],\n",
      "        [13.4000, 12.1000, 22.8000,  8.9000, 12.4000, 28.5000],\n",
      "        [12.1000, 22.8000,  8.9000, 12.4000, 28.5000, 33.9000],\n",
      "        [22.8000,  8.9000, 12.4000, 28.5000, 33.9000, 36.6000],\n",
      "        [ 8.9000, 12.4000, 28.5000, 33.9000, 36.6000, 39.8000],\n",
      "        [12.4000, 28.5000, 33.9000, 36.6000, 39.8000, 40.8000],\n",
      "        [28.5000, 33.9000, 36.6000, 39.8000, 40.8000, 44.9000],\n",
      "        [33.9000, 36.6000, 39.8000, 40.8000, 44.9000, 43.1000],\n",
      "        [36.6000, 39.8000, 40.8000, 44.9000, 43.1000, 43.6000],\n",
      "        [39.8000, 40.8000, 44.9000, 43.1000, 43.6000, 42.5000],\n",
      "        [40.8000, 44.9000, 43.1000, 43.6000, 42.5000, 44.1000],\n",
      "        [44.9000, 43.1000, 43.6000, 42.5000, 44.1000, 43.3000],\n",
      "        [43.1000, 43.6000, 42.5000, 44.1000, 43.3000, 24.4000],\n",
      "        [43.6000, 42.5000, 44.1000, 43.3000, 24.4000, 40.7000],\n",
      "        [42.5000, 44.1000, 43.3000, 24.4000, 40.7000, 39.4000],\n",
      "        [44.1000, 43.3000, 24.4000, 40.7000, 39.4000, 39.6000],\n",
      "        [43.3000, 24.4000, 40.7000, 39.4000, 39.6000, 39.9000],\n",
      "        [24.4000, 40.7000, 39.4000, 39.6000, 39.9000, 40.0000],\n",
      "        [40.7000, 39.4000, 39.6000, 39.9000, 40.0000, 42.2000],\n",
      "        [39.4000, 39.6000, 39.9000, 40.0000, 42.2000, 40.6000],\n",
      "        [39.6000, 39.9000, 40.0000, 42.2000, 40.6000, 41.0000],\n",
      "        [39.9000, 40.0000, 42.2000, 40.6000, 41.0000, 40.9000],\n",
      "        [40.0000, 42.2000, 40.6000, 41.0000, 40.9000, 41.4000]]), 'decoder_lengths': tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6]), 'decoder_time_idx': tensor([[36, 37, 38, 39, 40, 41],\n",
      "        [37, 38, 39, 40, 41, 42],\n",
      "        [38, 39, 40, 41, 42, 43],\n",
      "        [39, 40, 41, 42, 43, 44],\n",
      "        [40, 41, 42, 43, 44, 45],\n",
      "        [41, 42, 43, 44, 45, 46],\n",
      "        [42, 43, 44, 45, 46, 47],\n",
      "        [43, 44, 45, 46, 47, 48],\n",
      "        [44, 45, 46, 47, 48, 49],\n",
      "        [45, 46, 47, 48, 49, 50],\n",
      "        [46, 47, 48, 49, 50, 51],\n",
      "        [47, 48, 49, 50, 51, 52],\n",
      "        [48, 49, 50, 51, 52, 53],\n",
      "        [49, 50, 51, 52, 53, 54],\n",
      "        [50, 51, 52, 53, 54, 55],\n",
      "        [51, 52, 53, 54, 55, 56],\n",
      "        [52, 53, 54, 55, 56, 57],\n",
      "        [53, 54, 55, 56, 57, 58],\n",
      "        [54, 55, 56, 57, 58, 59],\n",
      "        [55, 56, 57, 58, 59, 60],\n",
      "        [56, 57, 58, 59, 60, 61],\n",
      "        [57, 58, 59, 60, 61, 62],\n",
      "        [58, 59, 60, 61, 62, 63],\n",
      "        [59, 60, 61, 62, 63, 64],\n",
      "        [60, 61, 62, 63, 64, 65]]), 'groups': tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]]), 'target_scale': tensor([[26.2278,  9.6184],\n",
      "        [26.4889,  9.7884],\n",
      "        [26.7333,  9.9713],\n",
      "        [26.6972,  9.9567],\n",
      "        [26.1500, 10.1345],\n",
      "        [25.5528, 10.3148],\n",
      "        [25.5556, 10.3140],\n",
      "        [25.6806, 10.0748],\n",
      "        [25.7139, 10.0274],\n",
      "        [25.9528,  9.9873],\n",
      "        [26.1583, 10.0746],\n",
      "        [26.4083, 10.2220],\n",
      "        [26.9833, 10.3802],\n",
      "        [27.6306, 10.4977],\n",
      "        [28.3000, 10.8134],\n",
      "        [29.0444, 10.8859],\n",
      "        [30.1667, 10.2067],\n",
      "        [31.1444,  9.6229],\n",
      "        [31.7583,  9.7272],\n",
      "        [32.2306,  9.8663],\n",
      "        [32.1306,  9.9285],\n",
      "        [32.3833, 10.0299],\n",
      "        [32.5528, 10.0971],\n",
      "        [32.6500, 10.1490],\n",
      "        [32.7861, 10.2140]])}, (tensor([[37.2000, 38.2000, 30.0000, 13.4000, 12.1000, 22.8000],\n",
      "        [38.2000, 30.0000, 13.4000, 12.1000, 22.8000,  8.9000],\n",
      "        [30.0000, 13.4000, 12.1000, 22.8000,  8.9000, 12.4000],\n",
      "        [13.4000, 12.1000, 22.8000,  8.9000, 12.4000, 28.5000],\n",
      "        [12.1000, 22.8000,  8.9000, 12.4000, 28.5000, 33.9000],\n",
      "        [22.8000,  8.9000, 12.4000, 28.5000, 33.9000, 36.6000],\n",
      "        [ 8.9000, 12.4000, 28.5000, 33.9000, 36.6000, 39.8000],\n",
      "        [12.4000, 28.5000, 33.9000, 36.6000, 39.8000, 40.8000],\n",
      "        [28.5000, 33.9000, 36.6000, 39.8000, 40.8000, 44.9000],\n",
      "        [33.9000, 36.6000, 39.8000, 40.8000, 44.9000, 43.1000],\n",
      "        [36.6000, 39.8000, 40.8000, 44.9000, 43.1000, 43.6000],\n",
      "        [39.8000, 40.8000, 44.9000, 43.1000, 43.6000, 42.5000],\n",
      "        [40.8000, 44.9000, 43.1000, 43.6000, 42.5000, 44.1000],\n",
      "        [44.9000, 43.1000, 43.6000, 42.5000, 44.1000, 43.3000],\n",
      "        [43.1000, 43.6000, 42.5000, 44.1000, 43.3000, 24.4000],\n",
      "        [43.6000, 42.5000, 44.1000, 43.3000, 24.4000, 40.7000],\n",
      "        [42.5000, 44.1000, 43.3000, 24.4000, 40.7000, 39.4000],\n",
      "        [44.1000, 43.3000, 24.4000, 40.7000, 39.4000, 39.6000],\n",
      "        [43.3000, 24.4000, 40.7000, 39.4000, 39.6000, 39.9000],\n",
      "        [24.4000, 40.7000, 39.4000, 39.6000, 39.9000, 40.0000],\n",
      "        [40.7000, 39.4000, 39.6000, 39.9000, 40.0000, 42.2000],\n",
      "        [39.4000, 39.6000, 39.9000, 40.0000, 42.2000, 40.6000],\n",
      "        [39.6000, 39.9000, 40.0000, 42.2000, 40.6000, 41.0000],\n",
      "        [39.9000, 40.0000, 42.2000, 40.6000, 41.0000, 40.9000],\n",
      "        [40.0000, 42.2000, 40.6000, 41.0000, 40.9000, 41.4000]]), None))\n"
     ]
    }
   ],
   "source": [
    "for i in iter(val_dataloader):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c95b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e946291e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511d55cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "399d0f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "# create PyTorch Lighning Trainer with early stopping\n",
    "lr_logger = LearningRateMonitor()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    gpus=0,  # run on CPU, if on multiple GPUs, use accelerator=\"ddp\"\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,  # 30 batches per epoch\n",
    "    callbacks=[lr_logger],\n",
    "    logger=TensorBoardLogger(\"lightning_logs\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "1a5dd3c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'LSTM' has no attribute 'from_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-224-299a0020cff4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# define network to train - the architecture is mostly inferred from the dataset, so that only a few hyperparameters have to be set by the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m tft = LSTM.from_dataset(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'LSTM' has no attribute 'from_dataset'"
     ]
    }
   ],
   "source": [
    "# define network to train - the architecture is mostly inferred from the dataset, so that only a few hyperparameters have to be set by the user\n",
    "tft = LSTM.from_dataset(\n",
    "    # dataset\n",
    "    training,\n",
    "\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "3ee9c93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 0     \n",
      "3  | prescalers                         | ModuleDict                      | 0     \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 0     \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 0     \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 0     \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K \n",
      "11 | lstm_encoder                       | LSTM                            | 8.4 K \n",
      "12 | lstm_decoder                       | LSTM                            | 8.4 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 4.2 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K \n",
      "20 | output_layer                       | Linear                          | 231   \n",
      "----------------------------------------------------------------------------------------\n",
      "54.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "54.6 K    Total params\n",
      "0.218     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104b795c98b84943a8040843b3c99f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-223-e7b6a836a434>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trainer.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# dispath `start_training` or `start_testing` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_or_test_or_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# double dispatch to initiate the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_bar_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;31m# set stage for logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_sanity_check\u001b[0;34m(self, ref_model)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_sanity_val_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_sanity_check_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_evaluation\u001b[0;34m(self, max_batches, on_epoch)\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[0;31m# lightning module methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"evaluation_step_and_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/evaluation_loop.py\u001b[0m in \u001b[0;36mevaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mmodel_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_fx_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# capture any logged information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# log loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprog_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, x, y, batch_idx)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \"\"\"\n\u001b[1;32m    522\u001b[0m         \u001b[0;31m# extract data and run model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         \u001b[0;31m# calculate interpretations etc for latter logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_interval\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, x, y, batch_idx, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmonotinicity_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmax_encoder_length\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         }\n\u001b[0;32m--> 430\u001b[0;31m         embeddings_varying_encoder, encoder_sparse_weights = self.encoder_variable_selection(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0membeddings_varying_encoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mstatic_context_variable_selection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmax_encoder_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fds/neural_prophet/venv/lib/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for one input, do not perform variable selection but just encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_variable_grns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0mvariable_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprescalers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    tft, train_dataloader=train_dataloader, val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0024c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\n",
    "predictions = tft.predict(val_dataloader)\n",
    "print(f\"Mean absolute error of model: {(actuals - predictions).abs().mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f323a133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc59f3a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba020d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f570ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5372a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabularize_univariate_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda3d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "bfefe8f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not determine the shape of object type 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-8a04f45cc0b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m TimeseriesDataset(df_train, \n\u001b[0m\u001b[1;32m      2\u001b[0m                  \u001b[0mdf_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                  seq_len=n_lags)\n",
      "\u001b[0;32m<ipython-input-172-d54344f919f5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, X, y, seq_len)\u001b[0m\n\u001b[1;32m      9\u001b[0m     '''\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not determine the shape of object type 'DataFrame'"
     ]
    }
   ],
   "source": [
    "TimeseriesDataset(df_train, \n",
    "                  df_val, \n",
    "                  seq_len=n_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c792218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2b96e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e0c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92fb7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "742bffb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>27.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-05-01 00:05:00</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-05-01 00:10:00</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-05-01 00:15:00</td>\n",
       "      <td>26.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-05-01 00:20:00</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18716</th>\n",
       "      <td>2017-07-04 23:40:00</td>\n",
       "      <td>42.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18717</th>\n",
       "      <td>2017-07-04 23:45:00</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18718</th>\n",
       "      <td>2017-07-04 23:50:00</td>\n",
       "      <td>42.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18719</th>\n",
       "      <td>2017-07-04 23:55:00</td>\n",
       "      <td>42.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18720</th>\n",
       "      <td>2017-07-05 00:00:00</td>\n",
       "      <td>41.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18721 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        ds     y\n",
       "0      2017-05-01 00:00:00  27.8\n",
       "1      2017-05-01 00:05:00  27.0\n",
       "2      2017-05-01 00:10:00  26.8\n",
       "3      2017-05-01 00:15:00  26.5\n",
       "4      2017-05-01 00:20:00  25.6\n",
       "...                    ...   ...\n",
       "18716  2017-07-04 23:40:00  42.8\n",
       "18717  2017-07-04 23:45:00  43.0\n",
       "18718  2017-07-04 23:50:00  42.1\n",
       "18719  2017-07-04 23:55:00  42.1\n",
       "18720  2017-07-05 00:00:00  41.4\n",
       "\n",
       "[18721 rows x 2 columns]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e7ddc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c0bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "a350d499",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesDataset(Dataset):   \n",
    "    '''\n",
    "    Custom Dataset subclass. \n",
    "    Serves as input to DataLoader to transform X \n",
    "      into sequence data using rolling window. \n",
    "    DataLoader using this dataset will output batches \n",
    "      of `(batch_size, seq_len, n_features)` shape.\n",
    "    Suitable as an input to RNNs. \n",
    "    '''\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, seq_len: int = 1):\n",
    "        self.X = torch.tensor(X).float()\n",
    "        self.y = torch.tensor(y).float()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.__len__() - (self.seq_len-1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.X[index:index+self.seq_len], self.y[index+self.seq_len-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef50ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerConsumptionDataModule(pl.LightningDataModule):\n",
    "    '''\n",
    "    PyTorch Lighting DataModule subclass:\n",
    "    https://pytorch-lightning.readthedocs.io/en/latest/datamodules.html\n",
    "\n",
    "    Serves the purpose of aggregating all data loading \n",
    "      and processing work in one place.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, seq_len = 1, batch_size = 128, num_workers=0):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_val = None\n",
    "        self.y_val = None\n",
    "        self.X_test = None\n",
    "        self.X_test = None\n",
    "        self.columns = None\n",
    "        self.preprocessing = None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \n",
    "        if stage == 'fit' and self.X_train is not None:\n",
    "            return \n",
    "        if stage == 'test' and self.X_test is not None:\n",
    "            return\n",
    "        if stage is None and self.X_train is not None and self.X_test is not None:  \n",
    "            return\n",
    "        \n",
    "\n",
    "        X = df_resample.dropna().copy()\n",
    "        y = X['Global_active_power'].shift(-1).ffill()\n",
    "        self.columns = X.columns\n",
    "\n",
    "\n",
    "        X_cv, X_test, y_cv, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, shuffle=False\n",
    "        )\n",
    "    \n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_cv, y_cv, test_size=0.25, shuffle=False\n",
    "        )\n",
    "\n",
    "        preprocessing = StandardScaler()\n",
    "        preprocessing.fit(X_train)\n",
    "\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.X_train = preprocessing.transform(X_train)\n",
    "            self.y_train = y_train.values.reshape((-1, 1))\n",
    "            self.X_val = preprocessing.transform(X_val)\n",
    "            self.y_val = y_val.values.reshape((-1, 1))\n",
    "\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.X_test = preprocessing.transform(X_test)\n",
    "            self.y_test = y_test.values.reshape((-1, 1))\n",
    "        \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = TimeseriesDataset(self.X_train, \n",
    "                                          self.y_train, \n",
    "                                          seq_len=self.seq_len)\n",
    "        train_loader = DataLoader(train_dataset, \n",
    "                                  batch_size = self.batch_size, \n",
    "                                  shuffle = False, \n",
    "                                  num_workers = self.num_workers)\n",
    "        \n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = TimeseriesDataset(self.X_val, \n",
    "                                        self.y_val, \n",
    "                                        seq_len=self.seq_len)\n",
    "        val_loader = DataLoader(val_dataset, \n",
    "                                batch_size = self.batch_size, \n",
    "                                shuffle = False, \n",
    "                                num_workers = self.num_workers)\n",
    "\n",
    "        return val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_dataset = TimeseriesDataset(self.X_test, \n",
    "                                         self.y_test, \n",
    "                                         seq_len=self.seq_len)\n",
    "        test_loader = DataLoader(test_dataset, \n",
    "                                 batch_size = self.batch_size, \n",
    "                                 shuffle = False, \n",
    "                                 num_workers = self.num_workers)\n",
    "\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526cba4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "786ffe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralprophet.time_dataset import tabularize_univariate_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "d018992e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>t</th>\n",
       "      <th>y_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>27.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-05-01 00:05:00</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-05-01 00:10:00</td>\n",
       "      <td>26.8</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-05-01 00:15:00</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-05-01 00:20:00</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18716</th>\n",
       "      <td>2017-07-04 23:40:00</td>\n",
       "      <td>42.8</td>\n",
       "      <td>0.999786</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18717</th>\n",
       "      <td>2017-07-04 23:45:00</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.999840</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18718</th>\n",
       "      <td>2017-07-04 23:50:00</td>\n",
       "      <td>42.1</td>\n",
       "      <td>0.999893</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18719</th>\n",
       "      <td>2017-07-04 23:55:00</td>\n",
       "      <td>42.1</td>\n",
       "      <td>0.999947</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18720</th>\n",
       "      <td>2017-07-05 00:00:00</td>\n",
       "      <td>41.4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18721 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ds     y         t  y_scaled\n",
       "0     2017-05-01 00:00:00  27.8  0.000000       NaN\n",
       "1     2017-05-01 00:05:00  27.0  0.000053       NaN\n",
       "2     2017-05-01 00:10:00  26.8  0.000107       NaN\n",
       "3     2017-05-01 00:15:00  26.5  0.000160       NaN\n",
       "4     2017-05-01 00:20:00  25.6  0.000214       NaN\n",
       "...                   ...   ...       ...       ...\n",
       "18716 2017-07-04 23:40:00  42.8  0.999786       NaN\n",
       "18717 2017-07-04 23:45:00  43.0  0.999840       NaN\n",
       "18718 2017-07-04 23:50:00  42.1  0.999893       NaN\n",
       "18719 2017-07-04 23:55:00  42.1  0.999947       NaN\n",
       "18720 2017-07-05 00:00:00  41.4  1.000000       NaN\n",
       "\n",
       "[18721 rows x 4 columns]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c9d0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "7d8f4872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>27.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-05-01 00:05:00</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-05-01 00:10:00</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ds     y\n",
       "0  2017-05-01 00:00:00  27.8\n",
       "1  2017-05-01 00:05:00  27.0\n",
       "2  2017-05-01 00:10:00  26.8"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_location = \"../\"\n",
    "df = pd.read_csv(data_location + \"example_data/yosemite_temps.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "2ed10774",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_utils.check_dataframe(\n",
    "            df, check_y=True)\n",
    "# df = self._handle_missing_data(df, freq=self.data_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc17d243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f7418d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31424313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "161d8d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params = df_utils.init_data_params(\n",
    "                df,\n",
    "                normalize=\"auto\",\n",
    "                covariates_config=None,\n",
    "                regressor_config=None,\n",
    "                events_config=None,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "bd2ac1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_utils.normalize(df, data_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "847e6abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>t</th>\n",
       "      <th>y_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>27.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-05-01 00:05:00</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-05-01 00:10:00</td>\n",
       "      <td>26.8</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-05-01 00:15:00</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-05-01 00:20:00</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18716</th>\n",
       "      <td>2017-07-04 23:40:00</td>\n",
       "      <td>42.8</td>\n",
       "      <td>0.999786</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18717</th>\n",
       "      <td>2017-07-04 23:45:00</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.999840</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18718</th>\n",
       "      <td>2017-07-04 23:50:00</td>\n",
       "      <td>42.1</td>\n",
       "      <td>0.999893</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18719</th>\n",
       "      <td>2017-07-04 23:55:00</td>\n",
       "      <td>42.1</td>\n",
       "      <td>0.999947</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18720</th>\n",
       "      <td>2017-07-05 00:00:00</td>\n",
       "      <td>41.4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18721 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ds     y         t  y_scaled\n",
       "0     2017-05-01 00:00:00  27.8  0.000000       NaN\n",
       "1     2017-05-01 00:05:00  27.0  0.000053       NaN\n",
       "2     2017-05-01 00:10:00  26.8  0.000107       NaN\n",
       "3     2017-05-01 00:15:00  26.5  0.000160       NaN\n",
       "4     2017-05-01 00:20:00  25.6  0.000214       NaN\n",
       "...                   ...   ...       ...       ...\n",
       "18716 2017-07-04 23:40:00  42.8  0.999786       NaN\n",
       "18717 2017-07-04 23:45:00  43.0  0.999840       NaN\n",
       "18718 2017-07-04 23:50:00  42.1  0.999893       NaN\n",
       "18719 2017-07-04 23:55:00  42.1  0.999947       NaN\n",
       "18720 2017-07-05 00:00:00  41.4  1.000000       NaN\n",
       "\n",
       "[18721 rows x 4 columns]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "971fede2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('time',\n",
       "              array([[0.00000000e+00],\n",
       "                     [5.34188034e-05],\n",
       "                     [1.06837607e-04],\n",
       "                     ...,\n",
       "                     [9.99893162e-01],\n",
       "                     [9.99946581e-01],\n",
       "                     [1.00000000e+00]]))])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabularize_univariate_datetime(df)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c84207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e811196d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
