{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "758ea55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralprophet import NBeats, LSTM, DeepAR, NeuralProphet, TFT\n",
    "from neuralprophet.utils.df_utils import split_df\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f60ad8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralprophet.tools.metrics_libra import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "646f9c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc = '../example_data/LIBRA/'\n",
    "datasets = os.listdir(data_loc)\n",
    "\n",
    "\n",
    "datasets_economics = [dataset for dataset in datasets if 'economics' in dataset]\n",
    "datasets_finance = [dataset for dataset in datasets if 'finance' in dataset]\n",
    "datasets_human = [dataset for dataset in datasets if 'human' in dataset]\n",
    "datasets_nature = [dataset for dataset in datasets if 'nature' in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a8ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase = 'nature'\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "if usecase == 'economics':\n",
    "    for dataset in datasets_economics:\n",
    "        datasets.update({dataset:pd.read_csv(data_loc + dataset)})\n",
    "elif usecase == 'finance':\n",
    "    for dataset in datasets_finance:\n",
    "        datasets.update({dataset:pd.read_csv(data_loc + dataset)})\n",
    "elif usecase == 'human':\n",
    "    for dataset in datasets_human:\n",
    "        datasets.update({dataset:pd.read_csv(data_loc + dataset)})\n",
    "elif usecase == 'nature':\n",
    "    for dataset in datasets_nature:\n",
    "        datasets.update({dataset:pd.read_csv(data_loc + dataset)})\n",
    "    \n",
    "    \n",
    "frequencies = pd.read_csv(data_loc + 'freq.csv')\n",
    "mapping_frequencies_economics = {\n",
    "    1 : 'D',\n",
    "    4: 'Q',\n",
    "    12: 'M',\n",
    "    52: 'W',\n",
    "    24: 'H',\n",
    "    7: 'W',\n",
    "    91: 'D',\n",
    "    364: 'D', \n",
    "    360: 'D',\n",
    "    168: 'D',\n",
    "    672: 'D',\n",
    "    96: 'H', \n",
    "    288: 'D',\n",
    "    28: 'D',\n",
    "    6: 'D',\n",
    "    30: 'D',\n",
    "    720: 'D'\n",
    "    \n",
    "}\n",
    "\n",
    "def mapping(x):\n",
    "    try:\n",
    "        return mapping_frequencies_economics[x]\n",
    "    except:\n",
    "        return 'D'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ae7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56617abd",
   "metadata": {},
   "source": [
    "Following the original LIBRA work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a58ed46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc42711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b99df16",
   "metadata": {},
   "source": [
    "# One step ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35b1a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_p = 0.2\n",
    "n_forecasts = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfa0860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 16\n",
      "\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[AGPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name      | Type         | Params\n",
      "-------------------------------------------\n",
      "0 | lstm      | LSTM         | 960   \n",
      "1 | linear    | Linear       | 11    \n",
      "2 | loss_func | SmoothL1Loss | 0     \n",
      "-------------------------------------------\n",
      "971       Trainable params\n",
      "0         Non-trainable params\n",
      "971       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2f715c43b041ef93ccd10152823f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - /Users/polina/fds/neural_prophet/neuralprophet/tools/metrics_libra.py:13: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return y_pred_mae / naive_mae\n",
      "\n",
      "INFO - (NP.utils.set_auto_seasonalities) - Disabling weekly seasonality. Run NeuralProphet with weekly_seasonality=True to override this.\n",
      "INFO - (NP.utils.set_auto_seasonalities) - Disabling daily seasonality. Run NeuralProphet with daily_seasonality=True to override this.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 16\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[AGPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: you defined a validation_step but have no val_dataloader. Skipping validation loop\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | season_params | ParameterDict | 12    \n",
      "1 | ar_net        | ModuleList    | 12    \n",
      "2 | loss_func     | SmoothL1Loss  | 0     \n",
      "------------------------------------------------\n",
      "37        Trainable params\n",
      "0         Non-trainable params\n",
      "37        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "\n",
      "\n",
      "                                      \u001b[A\u001b[A"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45fed170f15348fc83f360f8275f803b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - /Users/polina/fds/neural_prophet/neuralprophet/tools/metrics_libra.py:13: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return y_pred_mae / naive_mae\n",
      "\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: you defined a validation_step but have no val_dataloader. Skipping validation loop\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 12.9 K\n",
      "4 | distribution_projector | Linear                 | 66    \n",
      "------------------------------------------------------------------\n",
      "13.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "13.0 K    Total params\n",
      "0.052     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f5c6e8d35247aa9da99e624025b1ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restored states from the checkpoint file at /Users/polina/fds/neural_prophet/example_notebooks/lr_find_temp_model.ckpt\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 12.9 K\n",
      "4 | distribution_projector | Linear                 | 66    \n",
      "------------------------------------------------------------------\n",
      "13.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "13.0 K    Total params\n",
      "0.052     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a1682d34e54e7d914c2cda952d1fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 14it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "metrics = {}\n",
    "\n",
    "counter = 0\n",
    "for dataset_name, df in datasets.items():\n",
    "    one_dataset_metrics = {}\n",
    "\n",
    "    idx_ts = int(dataset_name.split('.')[0].split('_')[-1])-1\n",
    "    freq_number = frequencies.iloc[\n",
    "        idx_ts][[col for col in frequencies.columns if usecase in col][0]]\n",
    "\n",
    "    freq = mapping(frequencies.iloc[\n",
    "        idx_ts][[col for col in frequencies.columns if usecase in col][0]])\n",
    "    \n",
    "    \n",
    "    n_epochs = 50\n",
    "    if len(df) == 372864:\n",
    "        freq = 'H'\n",
    "        n_epochs = 15\n",
    "    \n",
    "    n_lags = freq_number\n",
    "    \n",
    "    method = 'LSTM_1step'\n",
    "    m = LSTM(n_lags = n_lags,\n",
    "            n_forecasts=n_forecasts,\n",
    "            learning_rate=0.01,\n",
    "            epochs=n_epochs)\n",
    "    \n",
    "    tr, vl = split_df(df, n_lags = n_lags, n_forecasts = n_forecasts, valid_p = valid_p)\n",
    "    m.fit(tr, freq = freq)\n",
    "    future = m.make_future_dataframe(vl, periods = 0, n_historic_predictions=True)\n",
    "    forecast = m.predict(future)\n",
    "    fold = forecast.iloc[n_lags:][[f'yhat{i}' for i in range(1, n_forecasts+1)]]\n",
    "\n",
    "    y_predicted = [np.array(fold).diagonal(offset=-i) for i in range(len(fold) - n_forecasts + 1)]\n",
    "    y = np.array(vl[n_lags:][\"y\"])\n",
    "    y_rolled = [y[i : i + n_forecasts] for i in range(len(y) - n_forecasts + 1)]\n",
    "    \n",
    "    y_naive = np.array(vl[n_lags-1:-1][\"y\"])\n",
    "    y_naive_rolled = [y_naive[i : i + n_forecasts] for i in range(len(y_naive) - n_forecasts + 1)]\n",
    "    \n",
    "    smapes = np.mean([smape(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    mases = np.mean([mase(y_rolled[i], y_predicted[i], y_naive_rolled[i]) for i in range(len(y_rolled))])\n",
    "    mueses = np.mean([mues(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    moeses = np.mean([moes(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    muases = np.mean([muas(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    moases = np.mean([moas(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    \n",
    "    one_dataset_metrics.update({\n",
    "        f'smape_{method}': smapes,\n",
    "        f'mase_{method}': mases,\n",
    "        f'mues_{method}': mueses,\n",
    "        f'moes_{method}': moeses,\n",
    "        f'muas_{method}': muases,\n",
    "        f'moas_{method}': moases\n",
    "        \n",
    "    })\n",
    "    \n",
    "    \n",
    "    method = 'NP_1step'\n",
    "    m = NeuralProphet(n_lags = n_lags,\n",
    "            n_forecasts=n_forecasts,\n",
    "            learning_rate=0.01,\n",
    "            epochs=n_epochs)\n",
    "    \n",
    "    tr, vl = split_df(df, n_lags = n_lags, n_forecasts = n_forecasts, valid_p = valid_p)\n",
    "    m.fit(tr, freq = freq)\n",
    "    future = m.make_future_dataframe(vl, periods = 0, n_historic_predictions=True)\n",
    "    forecast = m.predict(future)\n",
    "    fold = forecast.iloc[n_lags:][[f'yhat{i}' for i in range(1, n_forecasts+1)]]\n",
    "\n",
    "    y_predicted = [np.array(fold).diagonal(offset=-i) for i in range(len(fold) - n_forecasts + 1)]\n",
    "    y = np.array(vl[n_lags:][\"y\"])\n",
    "    y_rolled = [y[i : i + n_forecasts] for i in range(len(y) - n_forecasts + 1)]\n",
    "    \n",
    "    y_naive = np.array(vl[n_lags-1:-1][\"y\"])\n",
    "    y_naive_rolled = [y_naive[i : i + n_forecasts] for i in range(len(y_naive) - n_forecasts + 1)]\n",
    "    \n",
    "    smapes = np.mean([smape(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    mases = np.mean([mase(y_rolled[i], y_predicted[i], y_naive_rolled[i]) for i in range(len(y_rolled))])\n",
    "    mueses = np.mean([mues(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    moeses = np.mean([moes(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    muases = np.mean([muas(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    moases = np.mean([moas(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    \n",
    "    one_dataset_metrics.update({\n",
    "        f'smape_{method}': smapes,\n",
    "        f'mase_{method}': mases,\n",
    "        f'mues_{method}': mueses,\n",
    "        f'moes_{method}': moeses,\n",
    "        f'muas_{method}': muases,\n",
    "        f'moas_{method}': moases\n",
    "        \n",
    "    })\n",
    "    \n",
    "    \n",
    "    method = 'DeepAR_1step'\n",
    "    m = DeepAR(n_lags = int(n_lags),\n",
    "            n_forecasts=n_forecasts,\n",
    "            learning_rate=0.01,\n",
    "            epochs=n_epochs)\n",
    "    \n",
    "    tr, vl = split_df(df, n_lags = n_lags, n_forecasts = n_forecasts, valid_p = valid_p)\n",
    "    m.fit(tr, freq = freq)\n",
    "    future = m.make_future_dataframe(vl, periods = 0, n_historic_predictions=True)\n",
    "    forecast = m.predict(future)\n",
    "    fold = forecast.iloc[n_lags:][[f'yhat{i}' for i in range(1, n_forecasts+1)]]\n",
    "\n",
    "    y_predicted = [np.array(fold).diagonal(offset=-i) for i in range(len(fold) - n_forecasts + 1)]\n",
    "    y = np.array(vl[n_lags:][\"y\"])\n",
    "    y_rolled = [y[i : i + n_forecasts] for i in range(len(y) - n_forecasts + 1)]\n",
    "    \n",
    "    y_naive = np.array(vl[n_lags-1:-1][\"y\"])\n",
    "    y_naive_rolled = [y_naive[i : i + n_forecasts] for i in range(len(y_naive) - n_forecasts + 1)]\n",
    "    \n",
    "    smapes = np.mean([smape(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    mases = np.mean([mase(y_rolled[i], y_predicted[i], y_naive_rolled[i]) for i in range(len(y_rolled))])\n",
    "    mueses = np.mean([mues(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    moeses = np.mean([moes(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    muases = np.mean([muas(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    moases = np.mean([moas(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    \n",
    "    one_dataset_metrics.update({\n",
    "        f'smape_{method}': smapes,\n",
    "        f'mase_{method}': mases,\n",
    "        f'mues_{method}': mueses,\n",
    "        f'moes_{method}': moeses,\n",
    "        f'muas_{method}': muases,\n",
    "        f'moas_{method}': moases\n",
    "        \n",
    "    })\n",
    "    \n",
    "    method = 'NBeats_1step'\n",
    "    m = NBeats(n_lags = int(n_lags),\n",
    "            n_forecasts=n_forecasts,\n",
    "            learning_rate=0.01,\n",
    "            epochs=n_epochs)\n",
    "    \n",
    "    tr, vl = split_df(df, n_lags = n_lags, n_forecasts = n_forecasts, valid_p = valid_p)\n",
    "    m.fit(tr, freq = freq)\n",
    "    future = m.make_future_dataframe(vl, periods = 0, n_historic_predictions=True)\n",
    "    forecast = m.predict(future)\n",
    "    fold = forecast.iloc[n_lags:][[f'yhat{i}' for i in range(1, n_forecasts+1)]]\n",
    "\n",
    "    y_predicted = [np.array(fold).diagonal(offset=-i) for i in range(len(fold) - n_forecasts + 1)]\n",
    "    y = np.array(vl[n_lags:][\"y\"])\n",
    "    y_rolled = [y[i : i + n_forecasts] for i in range(len(y) - n_forecasts + 1)]\n",
    "    \n",
    "    y_naive = np.array(vl[n_lags-1:-1][\"y\"])\n",
    "    y_naive_rolled = [y_naive[i : i + n_forecasts] for i in range(len(y_naive) - n_forecasts + 1)]\n",
    "    \n",
    "    smapes = np.mean([smape(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    mases = np.mean([mase(y_rolled[i], y_predicted[i], y_naive_rolled[i]) for i in range(len(y_rolled))])\n",
    "    mueses = np.mean([mues(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    moeses = np.mean([moes(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    muases = np.mean([muas(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    moases = np.mean([moas(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    \n",
    "    one_dataset_metrics.update({\n",
    "        f'smape_{method}': smapes,\n",
    "        f'mase_{method}': mases,\n",
    "        f'mues_{method}': mueses,\n",
    "        f'moes_{method}': moeses,\n",
    "        f'muas_{method}': muases,\n",
    "        f'moas_{method}': moases\n",
    "        \n",
    "    })\n",
    "    \n",
    "    method = 'TFT_1step'\n",
    "    m = TFT(n_lags = int(n_lags),\n",
    "            n_forecasts=n_forecasts,\n",
    "            learning_rate=0.01,\n",
    "            epochs=n_epochs)\n",
    "    \n",
    "    tr, vl = split_df(df, n_lags = n_lags, n_forecasts = n_forecasts, valid_p = valid_p)\n",
    "    m.fit(tr, freq = freq)\n",
    "    future = m.make_future_dataframe(vl, periods = 0, n_historic_predictions=True)\n",
    "    forecast = m.predict(future)\n",
    "    fold = forecast.iloc[n_lags:][[f'yhat{i}' for i in range(1, n_forecasts+1)]]\n",
    "\n",
    "    y_predicted = [np.array(fold).diagonal(offset=-i) for i in range(len(fold) - n_forecasts + 1)]\n",
    "    y = np.array(vl[n_lags:][\"y\"])\n",
    "    y_rolled = [y[i : i + n_forecasts] for i in range(len(y) - n_forecasts + 1)]\n",
    "    \n",
    "    y_naive = np.array(vl[n_lags-1:-1][\"y\"])\n",
    "    y_naive_rolled = [y_naive[i : i + n_forecasts] for i in range(len(y_naive) - n_forecasts + 1)]\n",
    "    \n",
    "    smapes = np.mean([smape(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    mases = np.mean([mase(y_rolled[i], y_predicted[i], y_naive_rolled[i]) for i in range(len(y_rolled))])\n",
    "    mueses = np.mean([mues(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    moeses = np.mean([moes(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    muases = np.mean([muas(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    moases = np.mean([moas(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    \n",
    "    one_dataset_metrics.update({\n",
    "        f'smape_{method}': smapes,\n",
    "        f'mase_{method}': mases,\n",
    "        f'mues_{method}': mueses,\n",
    "        f'moes_{method}': moeses,\n",
    "        f'muas_{method}': muases,\n",
    "        f'moas_{method}': moases\n",
    "        \n",
    "    })\n",
    "    \n",
    "    metrics.update({dataset_name:one_dataset_metrics})\n",
    "    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dcb7796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nature_83.csv</th>\n",
       "      <th>nature_97.csv</th>\n",
       "      <th>nature_68.csv</th>\n",
       "      <th>nature_40.csv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>smape_TFT_1step</th>\n",
       "      <td>28.105743</td>\n",
       "      <td>14.503072</td>\n",
       "      <td>1.670347</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mase_TFT_1step</th>\n",
       "      <td>inf</td>\n",
       "      <td>3.749676</td>\n",
       "      <td>2.978227</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mues_TFT_1step</th>\n",
       "      <td>0.639535</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moes_TFT_1step</th>\n",
       "      <td>0.360465</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>muas_TFT_1step</th>\n",
       "      <td>0.156907</td>\n",
       "      <td>0.086928</td>\n",
       "      <td>0.006151</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moas_TFT_1step</th>\n",
       "      <td>0.115789</td>\n",
       "      <td>0.056377</td>\n",
       "      <td>0.010603</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smape_NP_1step</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.813974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mase_NP_1step</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mues_NP_1step</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.368128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moes_NP_1step</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.631872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>muas_NP_1step</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.098137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moas_NP_1step</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.276155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 nature_83.csv  nature_97.csv  nature_68.csv  nature_40.csv\n",
       "smape_TFT_1step      28.105743      14.503072       1.670347            NaN\n",
       "mase_TFT_1step             inf       3.749676       2.978227            NaN\n",
       "mues_TFT_1step        0.639535       0.666667       0.363636            NaN\n",
       "moes_TFT_1step        0.360465       0.333333       0.636364            NaN\n",
       "muas_TFT_1step        0.156907       0.086928       0.006151            NaN\n",
       "moas_TFT_1step        0.115789       0.056377       0.010603            NaN\n",
       "smape_NP_1step             NaN            NaN            NaN      33.813974\n",
       "mase_NP_1step              NaN            NaN            NaN            inf\n",
       "mues_NP_1step              NaN            NaN            NaN       0.368128\n",
       "moes_NP_1step              NaN            NaN            NaN       0.631872\n",
       "muas_NP_1step              NaN            NaN            NaN       0.098137\n",
       "moas_NP_1step              NaN            NaN            NaN       0.276155"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cdea62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d6e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2969d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57526d1b",
   "metadata": {},
   "source": [
    "# Multi step ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a3792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_p = 0.2\n",
    "n_forecasts_p = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09330024",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics = {}\n",
    "\n",
    "counter = 0\n",
    "for dataset_name, df in datasets.items():\n",
    "\n",
    "    idx_ts = int(dataset_name.split('.')[0].split('_')[-1])-1\n",
    "    freq_number = frequencies.iloc[\n",
    "        idx_ts][[col for col in frequencies.columns if usecase in col][0]]\n",
    "\n",
    "    freq = mapping(frequencies.iloc[\n",
    "        idx_ts][[col for col in frequencies.columns if usecase in col][0]])\n",
    "    \n",
    "    n_lags = freq_number\n",
    "    n_forecasts = int(n_forecasts_p*len(df))\n",
    "    \n",
    "    method = 'LSTM_1step'\n",
    "    m = LSTM(n_lags = n_lags,\n",
    "            n_forecasts=n_forecasts,\n",
    "            learning_rate=0.01,\n",
    "            epochs=50)\n",
    "    \n",
    "    tr, vl = split_df(df, n_lags = n_lags, n_forecasts = n_forecasts, valid_p = valid_p)\n",
    "    m.fit(tr, freq = freq)\n",
    "    future = m.make_future_dataframe(vl, periods = 0, n_historic_predictions=True)\n",
    "    forecast = m.predict(future)\n",
    "    fold = forecast.iloc[n_lags:][[f'yhat{i}' for i in range(1, n_forecasts+1)]]\n",
    "\n",
    "    y_predicted = [np.array(fold).diagonal(offset=-i) for i in range(len(fold) - n_forecasts + 1)]\n",
    "    y = np.array(vl[n_lags:][\"y\"])\n",
    "    y_rolled = [y[i : i + n_forecasts] for i in range(len(y) - n_forecasts + 1)]\n",
    "    \n",
    "    y_naive = np.array(vl[n_lags-1:-1][\"y\"])\n",
    "    y_naive_rolled = [y_naive[i : i + n_forecasts] for i in range(len(y_naive) - n_forecasts + 1)]\n",
    "    \n",
    "    smapes = np.mean([smape(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    mases = np.mean([mase(y_rolled[i], y_predicted[i], y_naive_rolled[i]) for i in range(len(y_rolled))])\n",
    "    mueses = np.mean([mues(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    moeses = np.mean([moes(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    muases = np.mean([muas(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    moases = np.mean([moas(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    \n",
    "    metrics.update({dataset_name:{\n",
    "        f'smape_{method}': smapes,\n",
    "        f'mase_{method}': mases,\n",
    "        f'mues_{method}': mueses,\n",
    "        f'moes_{method}': moeses,\n",
    "        f'muas_{method}': muases,\n",
    "        f'moas_{method}': moases\n",
    "        \n",
    "    }})\n",
    "    \n",
    "    \n",
    "    method = 'NP_1step'\n",
    "    m = NeuralProphet(n_lags = n_lags,\n",
    "            n_forecasts=n_forecasts,\n",
    "            learning_rate=0.01,\n",
    "            epochs=50)\n",
    "    \n",
    "    tr, vl = split_df(df, n_lags = n_lags, n_forecasts = n_forecasts, valid_p = valid_p)\n",
    "    m.fit(tr, freq = freq)\n",
    "    future = m.make_future_dataframe(vl, periods = 0, n_historic_predictions=True)\n",
    "    forecast = m.predict(future)\n",
    "    fold = forecast.iloc[n_lags:][[f'yhat{i}' for i in range(1, n_forecasts+1)]]\n",
    "\n",
    "    y_predicted = [np.array(fold).diagonal(offset=-i) for i in range(len(fold) - n_forecasts + 1)]\n",
    "    y = np.array(vl[n_lags:][\"y\"])\n",
    "    y_rolled = [y[i : i + n_forecasts] for i in range(len(y) - n_forecasts + 1)]\n",
    "    \n",
    "    y_naive = np.array(vl[n_lags-1:-1][\"y\"])\n",
    "    y_naive_rolled = [y_naive[i : i + n_forecasts] for i in range(len(y_naive) - n_forecasts + 1)]\n",
    "    \n",
    "    smapes = np.mean([smape(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    mases = np.mean([mase(y_rolled[i], y_predicted[i], y_naive_rolled[i]) for i in range(len(y_rolled))])\n",
    "    mueses = np.mean([mues(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    moeses = np.mean([moes(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    muases = np.mean([muas(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    moases = np.mean([moas(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    \n",
    "    metrics.update({dataset_name:{\n",
    "        f'smape_{method}': smapes,\n",
    "        f'mase_{method}': mases,\n",
    "        f'mues_{method}': mueses,\n",
    "        f'moes_{method}': moeses,\n",
    "        f'muas_{method}': muases,\n",
    "        f'moas_{method}': moases\n",
    "        \n",
    "    }})\n",
    "    \n",
    "    \n",
    "    method = 'DeepAR_1step'\n",
    "    m = DeepAR(n_lags = int(n_lags),\n",
    "            n_forecasts=n_forecasts,\n",
    "            learning_rate=0.01,\n",
    "            epochs=50)\n",
    "    \n",
    "    tr, vl = split_df(df, n_lags = n_lags, n_forecasts = n_forecasts, valid_p = valid_p)\n",
    "    m.fit(tr, freq = freq)\n",
    "    future = m.make_future_dataframe(vl, periods = 0, n_historic_predictions=True)\n",
    "    forecast = m.predict(future)\n",
    "    fold = forecast.iloc[n_lags:][[f'yhat{i}' for i in range(1, n_forecasts+1)]]\n",
    "\n",
    "    y_predicted = [np.array(fold).diagonal(offset=-i) for i in range(len(fold) - n_forecasts + 1)]\n",
    "    y = np.array(vl[n_lags:][\"y\"])\n",
    "    y_rolled = [y[i : i + n_forecasts] for i in range(len(y) - n_forecasts + 1)]\n",
    "    \n",
    "    y_naive = np.array(vl[n_lags-1:-1][\"y\"])\n",
    "    y_naive_rolled = [y_naive[i : i + n_forecasts] for i in range(len(y_naive) - n_forecasts + 1)]\n",
    "    \n",
    "    smapes = np.mean([smape(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    mases = np.mean([mase(y_rolled[i], y_predicted[i], y_naive_rolled[i]) for i in range(len(y_rolled))])\n",
    "    mueses = np.mean([mues(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    moeses = np.mean([moes(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    muases = np.mean([muas(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    moases = np.mean([moas(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    \n",
    "    metrics.update({dataset_name:{\n",
    "        f'smape_{method}': smapes,\n",
    "        f'mase_{method}': mases,\n",
    "        f'mues_{method}': mueses,\n",
    "        f'moes_{method}': moeses,\n",
    "        f'muas_{method}': muases,\n",
    "        f'moas_{method}': moases\n",
    "        \n",
    "    }})\n",
    "    \n",
    "    method = 'NBeats_1step'\n",
    "    m = NBeats(n_lags = int(n_lags),\n",
    "            n_forecasts=n_forecasts,\n",
    "            learning_rate=0.01,\n",
    "            epochs=50)\n",
    "    \n",
    "    tr, vl = split_df(df, n_lags = n_lags, n_forecasts = n_forecasts, valid_p = valid_p)\n",
    "    m.fit(tr, freq = freq)\n",
    "    future = m.make_future_dataframe(vl, periods = 0, n_historic_predictions=True)\n",
    "    forecast = m.predict(future)\n",
    "    fold = forecast.iloc[n_lags:][[f'yhat{i}' for i in range(1, n_forecasts+1)]]\n",
    "\n",
    "    y_predicted = [np.array(fold).diagonal(offset=-i) for i in range(len(fold) - n_forecasts + 1)]\n",
    "    y = np.array(vl[n_lags:][\"y\"])\n",
    "    y_rolled = [y[i : i + n_forecasts] for i in range(len(y) - n_forecasts + 1)]\n",
    "    \n",
    "    y_naive = np.array(vl[n_lags-1:-1][\"y\"])\n",
    "    y_naive_rolled = [y_naive[i : i + n_forecasts] for i in range(len(y_naive) - n_forecasts + 1)]\n",
    "    \n",
    "    smapes = np.mean([smape(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    mases = np.mean([mase(y_rolled[i], y_predicted[i], y_naive_rolled[i]) for i in range(len(y_rolled))])\n",
    "    mueses = np.mean([mues(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    moeses = np.mean([moes(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    muases = np.mean([muas(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    moases = np.mean([moas(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    \n",
    "    metrics.update({dataset_name:{\n",
    "        f'smape_{method}': smapes,\n",
    "        f'mase_{method}': mases,\n",
    "        f'mues_{method}': mueses,\n",
    "        f'moes_{method}': moeses,\n",
    "        f'muas_{method}': muases,\n",
    "        f'moas_{method}': moases\n",
    "        \n",
    "    }})\n",
    "    \n",
    "    method = 'TFT_1step'\n",
    "    m = TFT(n_lags = int(n_lags),\n",
    "            n_forecasts=n_forecasts,\n",
    "            learning_rate=0.01,\n",
    "            epochs=50)\n",
    "    \n",
    "    tr, vl = split_df(df, n_lags = n_lags, n_forecasts = n_forecasts, valid_p = valid_p)\n",
    "    m.fit(tr, freq = freq)\n",
    "    future = m.make_future_dataframe(vl, periods = 0, n_historic_predictions=True)\n",
    "    forecast = m.predict(future)\n",
    "    fold = forecast.iloc[n_lags:][[f'yhat{i}' for i in range(1, n_forecasts+1)]]\n",
    "\n",
    "    y_predicted = [np.array(fold).diagonal(offset=-i) for i in range(len(fold) - n_forecasts + 1)]\n",
    "    y = np.array(vl[n_lags:][\"y\"])\n",
    "    y_rolled = [y[i : i + n_forecasts] for i in range(len(y) - n_forecasts + 1)]\n",
    "    \n",
    "    y_naive = np.array(vl[n_lags-1:-1][\"y\"])\n",
    "    y_naive_rolled = [y_naive[i : i + n_forecasts] for i in range(len(y_naive) - n_forecasts + 1)]\n",
    "    \n",
    "    smapes = np.mean([smape(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    mases = np.mean([mase(y_rolled[i], y_predicted[i], y_naive_rolled[i]) for i in range(len(y_rolled))])\n",
    "    mueses = np.mean([mues(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    moeses = np.mean([moes(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    muases = np.mean([muas(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    moases = np.mean([moas(y_rolled[i], y_predicted[i]) for i in range(len(y_rolled))])\n",
    "    \n",
    "    metrics.update({dataset_name:{\n",
    "        f'smape_{method}': smapes,\n",
    "        f'mase_{method}': mases,\n",
    "        f'mues_{method}': mueses,\n",
    "        f'moes_{method}': moeses,\n",
    "        f'muas_{method}': muases,\n",
    "        f'moas_{method}': moases\n",
    "        \n",
    "    }})\n",
    "    \n",
    "    \n",
    "    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a5876",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063dd042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58f98b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d1fc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928242ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
