{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b2e2806",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ourownstory/neural_prophet/blob/master/example_notebooks/autoregression_yosemite_temps.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80959639",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization with Ray Tune\n",
    "\n",
    "We introduce the module for hyperparameter otpimization with Ray Tune.\n",
    "\n",
    "It supports automatic tuning, with predefined by us hyperparameter sets, as well as manual tuning with user-provided configuration of the parameters.\n",
    "\n",
    "Firstly, we will show how it works with NP model in automated mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0756bd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install NeuralProphet from our repository\n",
    "# !pip install git+https://github.com/adasegroup/neural_prophet.git # may take a while\n",
    "# !pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3fa0841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neuralprophet import NeuralProphet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fcfe68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  \"update your install command.\", FutureWarning)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from neuralprophet.hyperparameter_tuner import tune_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7af071d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>27.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-05-01 00:05:00</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-05-01 00:10:00</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ds     y\n",
       "0  2017-05-01 00:00:00  27.8\n",
       "1  2017-05-01 00:05:00  27.0\n",
       "2  2017-05-01 00:10:00  26.8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    data_location = \"https://raw.githubusercontent.com/adasegroup/neural_prophet/master/\"\n",
    "else:\n",
    "    data_location = \"../\"\n",
    "\n",
    "df = pd.read_csv(data_location + \"example_data/yosemite_temps.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a2da3f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq = '5min'\n",
    "best_params, results_df = tune_hyperparameters('NP',\n",
    "                                               df,\n",
    "                                               freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c15953b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'growth': 'linear',\n",
       " 'n_changepoints': 10,\n",
       " 'changepoints_range': 0.5,\n",
       " 'trend_reg': 10.0,\n",
       " 'yearly_seasonality': False,\n",
       " 'weekly_seasonality': True,\n",
       " 'daily_seasonality': True,\n",
       " 'seasonality_mode': 'additive',\n",
       " 'seasonality_reg': 0.0,\n",
       " 'n_lags': 30,\n",
       " 'd_hidden': 8,\n",
       " 'num_hidden_layers': 2,\n",
       " 'ar_sparsity': 0.8,\n",
       " 'learning_rate': 0.024150905458487568,\n",
       " 'loss_func': 'Huber',\n",
       " 'normalize': 'auto'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf5b127",
   "metadata": {},
   "source": [
    "This dictionary can further be used in initialization of NeuralProphet model.\n",
    "\n",
    "This function has also additional parameters:\n",
    "- **num_epochs**: Max possible number of epochs to train each model.\n",
    "- **num_samples**: Number of samples from hyperparameter spaces to check.\n",
    "- **resources_per_trial**: Resources per trial setting for ray.tune.run, {'cpu': 1, 'gpu': 2} for example\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3158e074",
   "metadata": {},
   "source": [
    "This function additionally will output the dataframe with detailed result of each trial if return_results is set to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42a9dd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config.growth</th>\n",
       "      <th>config.n_changepoints</th>\n",
       "      <th>config.changepoints_range</th>\n",
       "      <th>config.trend_reg</th>\n",
       "      <th>config.yearly_seasonality</th>\n",
       "      <th>config.weekly_seasonality</th>\n",
       "      <th>config.daily_seasonality</th>\n",
       "      <th>config.seasonality_mode</th>\n",
       "      <th>config.seasonality_reg</th>\n",
       "      <th>config.n_lags</th>\n",
       "      <th>config.d_hidden</th>\n",
       "      <th>config.num_hidden_layers</th>\n",
       "      <th>config.ar_sparsity</th>\n",
       "      <th>config.learning_rate</th>\n",
       "      <th>config.loss_func</th>\n",
       "      <th>config.normalize</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8f12d_00000</th>\n",
       "      <td>off</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>multiplicative</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>MSE</td>\n",
       "      <td>off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8f12d_00001</th>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>additive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.019590</td>\n",
       "      <td>MSE</td>\n",
       "      <td>soft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8f12d_00002</th>\n",
       "      <td>linear</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>multiplicative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>MSE</td>\n",
       "      <td>standardize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8f12d_00003</th>\n",
       "      <td>linear</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>multiplicative</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>Huber</td>\n",
       "      <td>off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8f12d_00004</th>\n",
       "      <td>linear</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>additive</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>MSE</td>\n",
       "      <td>soft</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            config.growth  config.n_changepoints  config.changepoints_range  \\\n",
       "trial_id                                                                      \n",
       "8f12d_00000           off                     10                        0.9   \n",
       "8f12d_00001        linear                    100                        0.8   \n",
       "8f12d_00002        linear                     10                        0.5   \n",
       "8f12d_00003        linear                     10                        0.5   \n",
       "8f12d_00004        linear                     10                        0.5   \n",
       "\n",
       "             config.trend_reg  config.yearly_seasonality  \\\n",
       "trial_id                                                   \n",
       "8f12d_00000              10.0                      False   \n",
       "8f12d_00001               0.5                       True   \n",
       "8f12d_00002               0.0                       True   \n",
       "8f12d_00003              10.0                       True   \n",
       "8f12d_00004               0.5                       True   \n",
       "\n",
       "             config.weekly_seasonality  config.daily_seasonality  \\\n",
       "trial_id                                                           \n",
       "8f12d_00000                      False                      True   \n",
       "8f12d_00001                      False                      True   \n",
       "8f12d_00002                      False                     False   \n",
       "8f12d_00003                       True                      True   \n",
       "8f12d_00004                       True                      True   \n",
       "\n",
       "            config.seasonality_mode  config.seasonality_reg  config.n_lags  \\\n",
       "trial_id                                                                     \n",
       "8f12d_00000          multiplicative                     1.0            100   \n",
       "8f12d_00001                additive                     0.0             10   \n",
       "8f12d_00002          multiplicative                     0.0             30   \n",
       "8f12d_00003          multiplicative                     0.5             30   \n",
       "8f12d_00004                additive                     0.5             10   \n",
       "\n",
       "             config.d_hidden  config.num_hidden_layers  config.ar_sparsity  \\\n",
       "trial_id                                                                     \n",
       "8f12d_00000               64                         8                 0.8   \n",
       "8f12d_00001              128                        16                 0.8   \n",
       "8f12d_00002              128                         2                 0.1   \n",
       "8f12d_00003                8                         8                 0.8   \n",
       "8f12d_00004               64                         2                 0.8   \n",
       "\n",
       "             config.learning_rate config.loss_func config.normalize  \n",
       "trial_id                                                             \n",
       "8f12d_00000              0.000733              MSE              off  \n",
       "8f12d_00001              0.019590              MSE             soft  \n",
       "8f12d_00002              0.000306              MSE      standardize  \n",
       "8f12d_00003              0.000195            Huber              off  \n",
       "8f12d_00004              0.008229              MSE             soft  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[['config.growth', 'config.n_changepoints', 'config.changepoints_range',\n",
    "       'config.trend_reg', 'config.yearly_seasonality',\n",
    "       'config.weekly_seasonality', 'config.daily_seasonality',\n",
    "       'config.seasonality_mode', 'config.seasonality_reg', 'config.n_lags',\n",
    "       'config.d_hidden', 'config.num_hidden_layers', 'config.ar_sparsity',\n",
    "       'config.learning_rate', 'config.loss_func', 'config.normalize']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c754f6a",
   "metadata": {},
   "source": [
    "## Manual mode\n",
    "\n",
    "In case of manual mode, a user must provide a config dictionary with hyperparameter spaces in compatability with Ray Tune api.\n",
    "\n",
    "We provide a minimal example below, for more information on Search Spaces withit this link https://docs.ray.io/en/master/tune/api_docs/search_space.html?highlight=tune.choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1549cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "config = {'n_lags': tune.grid_search([10, 20, 30]),\n",
    "          'learning_rate': tune.loguniform(1e-4, 1e-1),\n",
    "          'num_hidden_layers': tune.choice([2, 8, 16])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b24847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=39356)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=39356)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=39356)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=39356)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=39356)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=39356)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=39356)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=39356)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=39356)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=39356)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=39356)\u001b[0m 1 | ar_net        | ModuleList    | 2.1 K \n",
      "\u001b[2m\u001b[36m(pid=39356)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=39356)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=39356)\u001b[0m 2.1 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=39356)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=39356)\u001b[0m 2.1 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=39356)\u001b[0m 0.009     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=39356)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=39356)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=39356)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=39356)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=39356)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=39356)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=39355)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=39361)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=39361)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=39361)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "\u001b[2m\u001b[36m(pid=39360)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=39360)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=39360)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "\u001b[2m\u001b[36m(pid=39355)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=39355)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "\u001b[2m\u001b[36m(pid=39357)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=39355)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=39355)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=39355)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=39355)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=39355)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=39355)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=39355)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=39355)\u001b[0m 1 | ar_net        | ModuleList    | 2.1 K \n",
      "\u001b[2m\u001b[36m(pid=39355)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=39355)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=39355)\u001b[0m 2.1 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=39355)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=39355)\u001b[0m 2.1 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=39355)\u001b[0m 0.009     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=39355)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=39355)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=39355)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=39355)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=39355)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=39355)\u001b[0m \n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=39361)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=39361)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=39361)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=39361)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=39361)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=39361)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=39361)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=39361)\u001b[0m 1 | ar_net        | ModuleList    | 2.0 K \n",
      "\u001b[2m\u001b[36m(pid=39361)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=39361)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=39361)\u001b[0m 2.0 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=39361)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=39361)\u001b[0m 2.0 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=39361)\u001b[0m 0.008     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=39361)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=39361)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=39361)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=39361)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=39361)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=39361)\u001b[0m \n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=39360)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=39360)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=39360)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=39360)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=39360)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=39360)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=39360)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=39360)\u001b[0m 1 | ar_net        | ModuleList    | 3.7 K \n",
      "\u001b[2m\u001b[36m(pid=39360)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=39360)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=39360)\u001b[0m 3.7 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=39360)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=39360)\u001b[0m 3.7 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=39360)\u001b[0m 0.015     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=39360)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=39360)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=39360)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=39360)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=39360)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=39360)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=39357)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=39357)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=39357)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=39357)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=39357)\u001b[0m TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=39357)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=39357)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=39357)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=39357)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=39357)\u001b[0m 1 | ar_net        | ModuleList    | 3.7 K \n",
      "\u001b[2m\u001b[36m(pid=39357)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=39357)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=39357)\u001b[0m 3.7 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=39357)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=39357)\u001b[0m 3.7 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=39357)\u001b[0m 0.015     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=39357)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=39357)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=39357)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=39357)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=39357)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=39357)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41765)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=41765)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=41765)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "\u001b[2m\u001b[36m(pid=41767)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=41767)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=41767)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41765)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=41765)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41765)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=41765)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41765)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=41765)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41765)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=41765)\u001b[0m 1 | ar_net        | ModuleList    | 15.9 K\n",
      "\u001b[2m\u001b[36m(pid=41765)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=41765)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41765)\u001b[0m 15.9 K    Trainable params\n",
      "\u001b[2m\u001b[36m(pid=41765)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=41765)\u001b[0m 15.9 K    Total params\n",
      "\u001b[2m\u001b[36m(pid=41765)\u001b[0m 0.064     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=41765)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41765)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41765)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41765)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41765)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41765)\u001b[0m \n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41767)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=41767)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41767)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=41767)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41767)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=41767)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41767)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=41767)\u001b[0m 1 | ar_net        | ModuleList    | 924   \n",
      "\u001b[2m\u001b[36m(pid=41767)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=41767)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41767)\u001b[0m 955       Trainable params\n",
      "\u001b[2m\u001b[36m(pid=41767)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=41767)\u001b[0m 955       Total params\n",
      "\u001b[2m\u001b[36m(pid=41767)\u001b[0m 0.004     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=41767)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41767)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41767)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41767)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41767)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41767)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41764)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=41764)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=41764)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41764)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=41764)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41764)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=41764)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41764)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=41764)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41764)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=41764)\u001b[0m 1 | ar_net        | ModuleList    | 264   \n",
      "\u001b[2m\u001b[36m(pid=41764)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=41764)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41764)\u001b[0m 295       Trainable params\n",
      "\u001b[2m\u001b[36m(pid=41764)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=41764)\u001b[0m 295       Total params\n",
      "\u001b[2m\u001b[36m(pid=41764)\u001b[0m 0.001     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=41764)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41764)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41764)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41764)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41764)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41764)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m 1 | ar_net        | ModuleList    | 15.9 K\n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m 15.9 K    Trainable params\n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m 15.9 K    Total params\n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m 0.064     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41823)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=41833)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=41833)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=41833)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41833)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=41833)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41833)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=41833)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41833)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=41833)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41833)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=41833)\u001b[0m 1 | ar_net        | ModuleList    | 2.1 K \n",
      "\u001b[2m\u001b[36m(pid=41833)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=41833)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41833)\u001b[0m 2.1 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=41833)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=41833)\u001b[0m 2.1 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=41833)\u001b[0m 0.009     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=41833)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41833)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41833)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41833)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41833)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41833)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m 1 | ar_net        | ModuleList    | 924   \n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m 955       Trainable params\n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m 955       Total params\n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m 0.004     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41842)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=41845)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=41845)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=41845)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41845)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=41845)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41845)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=41845)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41845)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=41845)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41845)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=41845)\u001b[0m 1 | ar_net        | ModuleList    | 2.0 K \n",
      "\u001b[2m\u001b[36m(pid=41845)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=41845)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41845)\u001b[0m 2.0 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=41845)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=41845)\u001b[0m 2.0 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=41845)\u001b[0m 0.008     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=41845)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41845)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41845)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41845)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41845)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41845)\u001b[0m \n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=41854)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=41854)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=41854)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41854)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=41854)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41854)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=41854)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41854)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=41854)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41854)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=41854)\u001b[0m 1 | ar_net        | ModuleList    | 2.1 K \n",
      "\u001b[2m\u001b[36m(pid=41854)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=41854)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41854)\u001b[0m 2.1 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=41854)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=41854)\u001b[0m 2.1 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=41854)\u001b[0m 0.009     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=41854)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41854)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41854)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41854)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41854)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41854)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=41886)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=41886)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=41886)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41886)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=41886)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41886)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=41886)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41886)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=41886)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41886)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=41886)\u001b[0m 1 | ar_net        | ModuleList    | 7.4 K \n",
      "\u001b[2m\u001b[36m(pid=41886)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=41886)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41886)\u001b[0m 7.4 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=41886)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=41886)\u001b[0m 7.4 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=41886)\u001b[0m 0.030     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=41886)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41886)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41886)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41886)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41886)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41886)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=41902)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=41902)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=41902)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41902)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=41902)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41902)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=41902)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41902)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=41902)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41902)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=41902)\u001b[0m 1 | ar_net        | ModuleList    | 15.9 K\n",
      "\u001b[2m\u001b[36m(pid=41902)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=41902)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41902)\u001b[0m 15.9 K    Trainable params\n",
      "\u001b[2m\u001b[36m(pid=41902)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=41902)\u001b[0m 15.9 K    Total params\n",
      "\u001b[2m\u001b[36m(pid=41902)\u001b[0m 0.064     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=41902)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41902)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41902)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41902)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41902)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41902)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=41951)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=41951)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=41951)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41951)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=41951)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41951)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=41951)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41951)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=41951)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41951)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=41951)\u001b[0m 1 | ar_net        | ModuleList    | 2.1 K \n",
      "\u001b[2m\u001b[36m(pid=41951)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=41951)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41951)\u001b[0m 2.1 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=41951)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=41951)\u001b[0m 2.1 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=41951)\u001b[0m 0.009     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=41951)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41951)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41951)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41951)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41951)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41951)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=41991)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=41991)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=41991)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41991)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=41991)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=41991)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=41991)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41991)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=41991)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41991)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=41991)\u001b[0m 1 | ar_net        | ModuleList    | 3.7 K \n",
      "\u001b[2m\u001b[36m(pid=41991)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=41991)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=41991)\u001b[0m 3.7 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=41991)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=41991)\u001b[0m 3.7 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=41991)\u001b[0m 0.015     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=41991)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41991)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41991)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41991)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=41991)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41991)\u001b[0m \n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=42006)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=42006)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=42006)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42006)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42006)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42006)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42006)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42006)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=42006)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42006)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=42006)\u001b[0m 1 | ar_net        | ModuleList    | 2.0 K \n",
      "\u001b[2m\u001b[36m(pid=42006)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=42006)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42006)\u001b[0m 2.0 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=42006)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=42006)\u001b[0m 2.0 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=42006)\u001b[0m 0.008     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=42006)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42006)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42006)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42006)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42006)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42006)\u001b[0m \n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=42015)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=42015)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=42015)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42015)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42015)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42015)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=42015)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42015)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=42015)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42015)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=42015)\u001b[0m 1 | ar_net        | ModuleList    | 2.1 K \n",
      "\u001b[2m\u001b[36m(pid=42015)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=42015)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42015)\u001b[0m 2.1 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=42015)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=42015)\u001b[0m 2.1 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=42015)\u001b[0m 0.009     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=42015)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42015)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42015)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42015)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42015)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42015)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=42042)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=42042)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=42042)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42042)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42042)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42042)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42042)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42042)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=42042)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42042)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=42042)\u001b[0m 1 | ar_net        | ModuleList    | 3.7 K \n",
      "\u001b[2m\u001b[36m(pid=42042)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=42042)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42042)\u001b[0m 3.7 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=42042)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=42042)\u001b[0m 3.7 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=42042)\u001b[0m 0.015     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=42042)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42042)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42042)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42042)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42042)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42042)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=42045)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=42045)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=42045)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42045)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42045)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42045)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=42045)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42045)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=42045)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42045)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=42045)\u001b[0m 1 | ar_net        | ModuleList    | 15.9 K\n",
      "\u001b[2m\u001b[36m(pid=42045)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=42045)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42045)\u001b[0m 15.9 K    Trainable params\n",
      "\u001b[2m\u001b[36m(pid=42045)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=42045)\u001b[0m 15.9 K    Total params\n",
      "\u001b[2m\u001b[36m(pid=42045)\u001b[0m 0.064     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=42045)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42045)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42045)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42045)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42045)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42045)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=42067)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=42067)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=42067)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42067)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42067)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42067)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42067)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42067)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=42067)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42067)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=42067)\u001b[0m 1 | ar_net        | ModuleList    | 1.1 K \n",
      "\u001b[2m\u001b[36m(pid=42067)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=42067)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42067)\u001b[0m 1.1 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=42067)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=42067)\u001b[0m 1.1 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=42067)\u001b[0m 0.004     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=42067)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42067)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42067)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42067)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42067)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42067)\u001b[0m \n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=42088)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=42088)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=42088)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42088)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42088)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42088)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42088)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42088)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=42088)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42088)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=42088)\u001b[0m 1 | ar_net        | ModuleList    | 924   \n",
      "\u001b[2m\u001b[36m(pid=42088)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=42088)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42088)\u001b[0m 955       Trainable params\n",
      "\u001b[2m\u001b[36m(pid=42088)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=42088)\u001b[0m 955       Total params\n",
      "\u001b[2m\u001b[36m(pid=42088)\u001b[0m 0.004     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=42088)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42088)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42088)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42088)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42088)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42088)\u001b[0m \n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=42109)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=42109)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=42109)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42109)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42109)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42109)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42109)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42109)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=42109)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42109)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=42109)\u001b[0m 1 | ar_net        | ModuleList    | 7.9 K \n",
      "\u001b[2m\u001b[36m(pid=42109)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=42109)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42109)\u001b[0m 8.0 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=42109)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=42109)\u001b[0m 8.0 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=42109)\u001b[0m 0.032     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=42109)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42109)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42109)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42109)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42109)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42109)\u001b[0m \n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=42191)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=42191)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=42191)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42191)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42191)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42191)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=42191)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42191)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=42191)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42191)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=42191)\u001b[0m 1 | ar_net        | ModuleList    | 2.1 K \n",
      "\u001b[2m\u001b[36m(pid=42191)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=42191)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42191)\u001b[0m 2.1 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=42191)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=42191)\u001b[0m 2.1 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=42191)\u001b[0m 0.009     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=42191)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42191)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42191)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42191)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42191)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42191)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42200)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=42200)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=42200)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42200)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42200)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42200)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=42200)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42200)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=42200)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42200)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=42200)\u001b[0m 1 | ar_net        | ModuleList    | 7.4 K \n",
      "\u001b[2m\u001b[36m(pid=42200)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=42200)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42200)\u001b[0m 7.4 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=42200)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=42200)\u001b[0m 7.4 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=42200)\u001b[0m 0.030     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=42200)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42200)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42200)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42200)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42200)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42200)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=42209)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=42209)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=42209)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42209)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42209)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42209)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42209)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=42209)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42209)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=42209)\u001b[0m 1 | ar_net        | ModuleList    | 2.0 K \n",
      "\u001b[2m\u001b[36m(pid=42209)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=42209)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42209)\u001b[0m 2.0 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=42209)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=42209)\u001b[0m 2.0 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=42209)\u001b[0m 0.008     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=42209)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42209)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42209)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42209)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42209)\u001b[0m \n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=42391)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=42391)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=42391)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42391)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42391)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42391)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42391)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42391)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=42391)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42391)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=42391)\u001b[0m 1 | ar_net        | ModuleList    | 264   \n",
      "\u001b[2m\u001b[36m(pid=42391)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=42391)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42391)\u001b[0m 295       Trainable params\n",
      "\u001b[2m\u001b[36m(pid=42391)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=42391)\u001b[0m 295       Total params\n",
      "\u001b[2m\u001b[36m(pid=42391)\u001b[0m 0.001     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=42391)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42391)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42391)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42391)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42391)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42391)\u001b[0m \n",
      "  0%|          | 0/100 [00:01<?, ?it/s]\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=42481)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=42481)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=42481)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42481)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42481)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42481)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42481)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42481)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=42481)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42481)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=42481)\u001b[0m 1 | ar_net        | ModuleList    | 924   \n",
      "\u001b[2m\u001b[36m(pid=42481)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=42481)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42481)\u001b[0m 955       Trainable params\n",
      "\u001b[2m\u001b[36m(pid=42481)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=42481)\u001b[0m 955       Total params\n",
      "\u001b[2m\u001b[36m(pid=42481)\u001b[0m 0.004     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=42481)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42481)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42481)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42481)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42481)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42481)\u001b[0m \n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=42508)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=42508)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=42508)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42508)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42508)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42508)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42508)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42508)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=42508)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42508)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=42508)\u001b[0m 1 | ar_net        | ModuleList    | 7.9 K \n",
      "\u001b[2m\u001b[36m(pid=42508)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=42508)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42508)\u001b[0m 8.0 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=42508)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=42508)\u001b[0m 8.0 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=42508)\u001b[0m 0.032     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=42508)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42508)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42508)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42508)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42508)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42508)\u001b[0m \n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=42608)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=42608)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=42608)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42608)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42608)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42608)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=42608)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42608)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=42608)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42608)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=42608)\u001b[0m 1 | ar_net        | ModuleList    | 2.1 K \n",
      "\u001b[2m\u001b[36m(pid=42608)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=42608)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42608)\u001b[0m 2.1 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=42608)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=42608)\u001b[0m 2.1 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=42608)\u001b[0m 0.009     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=42608)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42608)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42608)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42608)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42608)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42608)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=42723)\u001b[0m INFO - (NP.forecaster._handle_missing_data) - 12 NaN values in column y were auto-imputed.\n",
      "\u001b[2m\u001b[36m(pid=42723)\u001b[0m INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n",
      "\u001b[2m\u001b[36m(pid=42723)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42723)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=42723)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=42723)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(pid=42723)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42723)\u001b[0m   | Name          | Type          | Params\n",
      "\u001b[2m\u001b[36m(pid=42723)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42723)\u001b[0m 0 | season_params | ParameterDict | 18    \n",
      "\u001b[2m\u001b[36m(pid=42723)\u001b[0m 1 | ar_net        | ModuleList    | 7.4 K \n",
      "\u001b[2m\u001b[36m(pid=42723)\u001b[0m 2 | loss_func     | SmoothL1Loss  | 0     \n",
      "\u001b[2m\u001b[36m(pid=42723)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=42723)\u001b[0m 7.4 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=42723)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=42723)\u001b[0m 7.4 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=42723)\u001b[0m 0.030     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=42723)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42723)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42723)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=42723)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=42723)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=42723)\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "freq = '5min'\n",
    "best_params, results_df = tune_hyperparameters('NP', \n",
    "                                               df,\n",
    "                                               freq, \n",
    "                                               mode = 'manual',\n",
    "                                              config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4517177",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1335411",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa80c5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd7730e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb49452a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecddafb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4beda9a8",
   "metadata": {},
   "source": [
    "# Other models\n",
    "\n",
    "Hyperparameter optimization can be also run for each of 4 models, available in Neuralprophet library: LSTM, NBeats, TFT and DeepAR. In order to run optimization for particular model, the procedure is the same, you just need to insert the name of preferable model.\n",
    "\n",
    "An example for LSTM can be found below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74e08f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m 0 | lstm      | LSTM         | 6.1 M \n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m 1 | linear    | Linear       | 257   \n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m 6.1 M     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m 6.1 M     Total params\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m 24.241    Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m 0 | lstm      | LSTM         | 271 K \n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m 1 | linear    | Linear       | 65    \n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m 271 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m 271 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m 1.086     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m \n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m 0 | lstm      | LSTM         | 7.6 K \n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m 1 | linear    | Linear       | 9     \n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m 7.6 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m 7.6 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m 0.030     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m \n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m 0 | lstm      | LSTM         | 51.7 K\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m 1 | linear    | Linear       | 65    \n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m 51.8 K    Trainable params\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m 51.8 K    Total params\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m 0.207     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m \n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m 0 | lstm      | LSTM         | 17.7 K\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m 1 | linear    | Linear       | 17    \n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m 17.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m 17.7 K    Total params\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m 0.071     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m 0 | lstm      | LSTM         | 249 K \n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m 1 | linear    | Linear       | 129   \n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m 249 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m 249 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m 1.000     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m 0 | lstm      | LSTM         | 30.0 K\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m 1 | linear    | Linear       | 17    \n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m 30.0 K    Trainable params\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m 30.0 K    Total params\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m 0.120     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m 0 | lstm      | LSTM         | 2.1 M \n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m 1 | linear    | Linear       | 129   \n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m 2.1 M     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m 2.1 M     Total params\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m 8.332     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m \n",
      "                                       \n",
      "                                       \n",
      "                                       \n",
      "                                       \n",
      "                                       \n",
      "                                       \n",
      "                                       \n",
      "                                       \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m 0 | lstm      | LSTM         | 1.2 K \n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m 1 | linear    | Linear       | 9     \n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m 1.2 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m 1.2 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m 0.005     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m \n",
      "                                       \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m 0 | lstm      | LSTM         | 2.9 M \n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m 1 | linear    | Linear       | 257   \n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m 2.9 M     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m 2.9 M     Total params\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m 11.642    Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m 0 | lstm      | LSTM         | 275 K \n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m 1 | linear    | Linear       | 65    \n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m 275 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m 275 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m 1.102     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m \n",
      "                                       \n",
      "                                       \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m 0 | lstm      | LSTM         | 57.9 K\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m 1 | linear    | Linear       | 65    \n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m 57.9 K    Trainable params\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m 57.9 K    Total params\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m 0.232     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m \n",
      "                                       \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m 0 | lstm      | LSTM         | 146 K \n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m 1 | linear    | Linear       | 129   \n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m 146 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m 146 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m 0.586     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m \n",
      "                                       \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m 0 | lstm      | LSTM         | 6.1 M \n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m 1 | linear    | Linear       | 257   \n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m 6.1 M     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m 6.1 M     Total params\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m 24.290    Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m 0 | lstm      | LSTM         | 6.2 M \n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m 1 | linear    | Linear       | 257   \n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m 6.2 M     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m 6.2 M     Total params\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m 24.659    Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m \n",
      "                                       \n",
      "                                       \n",
      "2021-05-21 14:43:19,049\tWARNING util.py:162 -- The `start_trial` operation took 1.055 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m 0 | lstm      | LSTM         | 74.8 K\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m 1 | linear    | Linear       | 65    \n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m 74.8 K    Trainable params\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m 74.8 K    Total params\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m 0.299     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m \n",
      "                                       \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m 0 | lstm      | LSTM         | 559 K \n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m 1 | linear    | Linear       | 257   \n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m 559 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m 559 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m 2.237     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m \n",
      "                                       \n",
      "2021-05-21 14:48:23,596\tWARNING tune.py:507 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m 2021-05-21 14:48:23,672\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 349, in step\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m     block=True, timeout=RESULT_FETCH_TIMEOUT)\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/queue.py\", line 179, in get\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m     self.not_empty.wait(remaining)\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/threading.py\", line 300, in wait\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m     gotit = waiter.acquire(True, timeout)\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m 2021-05-21 14:48:23,660\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 349, in step\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m     block=True, timeout=RESULT_FETCH_TIMEOUT)\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/queue.py\", line 179, in get\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m     self.not_empty.wait(remaining)\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/threading.py\", line 300, in wait\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m     gotit = waiter.acquire(True, timeout)\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m 2021-05-21 14:48:23,648\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 349, in step\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m     block=True, timeout=RESULT_FETCH_TIMEOUT)\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/queue.py\", line 179, in get\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m     self.not_empty.wait(remaining)\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/threading.py\", line 300, in wait\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m     gotit = waiter.acquire(True, timeout)\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m 2021-05-21 14:48:23,676\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 349, in step\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m     block=True, timeout=RESULT_FETCH_TIMEOUT)\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/queue.py\", line 179, in get\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m     self.not_empty.wait(remaining)\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/threading.py\", line 300, in wait\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m     gotit = waiter.acquire(True, timeout)\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m 2021-05-21 14:48:23,692\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 349, in step\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m     block=True, timeout=RESULT_FETCH_TIMEOUT)\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/queue.py\", line 179, in get\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m     self.not_empty.wait(remaining)\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/threading.py\", line 300, in wait\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m     gotit = waiter.acquire(True, timeout)\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m 2021-05-21 14:48:23,712\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 349, in step\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m     block=True, timeout=RESULT_FETCH_TIMEOUT)\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/queue.py\", line 179, in get\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m     self.not_empty.wait(remaining)\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/threading.py\", line 300, in wait\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m     gotit = waiter.acquire(True, timeout)\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m 2021-05-21 14:48:23,694\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 349, in step\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m     block=True, timeout=RESULT_FETCH_TIMEOUT)\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/queue.py\", line 179, in get\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m     self.not_empty.wait(remaining)\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/threading.py\", line 300, in wait\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m     gotit = waiter.acquire(True, timeout)\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m 2021-05-21 14:48:23,715\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 349, in step\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m     block=True, timeout=RESULT_FETCH_TIMEOUT)\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/queue.py\", line 179, in get\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m     self.not_empty.wait(remaining)\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/threading.py\", line 300, in wait\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m     gotit = waiter.acquire(True, timeout)\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m SystemExit: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 14:48:23,922\tERROR tune.py:545 -- Trials did not complete: [train_LSTM_tune_ddb98_00000, train_LSTM_tune_ddb98_00004, train_LSTM_tune_ddb98_00009, train_LSTM_tune_ddb98_00011, train_LSTM_tune_ddb98_00013, train_LSTM_tune_ddb98_00014, train_LSTM_tune_ddb98_00015, train_LSTM_tune_ddb98_00016, train_LSTM_tune_ddb98_00017, train_LSTM_tune_ddb98_00018, train_LSTM_tune_ddb98_00019, train_LSTM_tune_ddb98_00020, train_LSTM_tune_ddb98_00021, train_LSTM_tune_ddb98_00022, train_LSTM_tune_ddb98_00023, train_LSTM_tune_ddb98_00024, train_LSTM_tune_ddb98_00025, train_LSTM_tune_ddb98_00026, train_LSTM_tune_ddb98_00027, train_LSTM_tune_ddb98_00028, train_LSTM_tune_ddb98_00029, train_LSTM_tune_ddb98_00030, train_LSTM_tune_ddb98_00031, train_LSTM_tune_ddb98_00032, train_LSTM_tune_ddb98_00033, train_LSTM_tune_ddb98_00034, train_LSTM_tune_ddb98_00035, train_LSTM_tune_ddb98_00036, train_LSTM_tune_ddb98_00037, train_LSTM_tune_ddb98_00038, train_LSTM_tune_ddb98_00039]\n",
      "2021-05-21 14:48:23,923\tWARNING tune.py:554 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    }
   ],
   "source": [
    "freq = '5min'\n",
    "best_params, results_df = tune_hyperparameters('LSTM',\n",
    "                                               df,\n",
    "                                               freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9108b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.013718466327407178,\n",
       " 'd_hidden': 128,\n",
       " 'n_lags': 100,\n",
       " 'num_hidden_layers': 2,\n",
       " 'lstm_bias': True,\n",
       " 'lstm_bidirectional': False}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ecc512c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>time_this_iter_s</th>\n",
       "      <th>done</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time_total_s</th>\n",
       "      <th>...</th>\n",
       "      <th>time_since_restore</th>\n",
       "      <th>timesteps_since_restore</th>\n",
       "      <th>iterations_since_restore</th>\n",
       "      <th>experiment_tag</th>\n",
       "      <th>config.learning_rate</th>\n",
       "      <th>config.d_hidden</th>\n",
       "      <th>config.n_lags</th>\n",
       "      <th>config.num_hidden_layers</th>\n",
       "      <th>config.lstm_bias</th>\n",
       "      <th>config.lstm_bidirectional</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ddb98_00000</th>\n",
       "      <td>0.047840</td>\n",
       "      <td>184.533827</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>a26094effe6246d5af6b147ae60a5933</td>\n",
       "      <td>2021-05-21_14-47-36</td>\n",
       "      <td>1.621598e+09</td>\n",
       "      <td>1486.938134</td>\n",
       "      <td>...</td>\n",
       "      <td>1486.938134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0_d_hidden=128,learning_rate=0.000337,lstm_bia...</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>128.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ddb98_00001</th>\n",
       "      <td>0.051339</td>\n",
       "      <td>48.592918</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4ebbb108e09e4b2986c3835634ed0295</td>\n",
       "      <td>2021-05-21_14-38-22</td>\n",
       "      <td>1.621597e+09</td>\n",
       "      <td>932.583967</td>\n",
       "      <td>...</td>\n",
       "      <td>932.583967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1_d_hidden=8,learning_rate=0.0097144,lstm_bias...</td>\n",
       "      <td>0.009714</td>\n",
       "      <td>8.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ddb98_00002</th>\n",
       "      <td>0.255517</td>\n",
       "      <td>28.091819</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4be3033d37004d4f98c47a2d0d564c11</td>\n",
       "      <td>2021-05-21_14-27-36</td>\n",
       "      <td>1.621596e+09</td>\n",
       "      <td>285.940451</td>\n",
       "      <td>...</td>\n",
       "      <td>285.940451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2_d_hidden=64,learning_rate=0.00010157,lstm_bi...</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>64.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ddb98_00003</th>\n",
       "      <td>0.000029</td>\n",
       "      <td>11.933487</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>44266641f7124ffda7d62dbdab2b0b49</td>\n",
       "      <td>2021-05-21_14-43-17</td>\n",
       "      <td>1.621597e+09</td>\n",
       "      <td>1227.605633</td>\n",
       "      <td>...</td>\n",
       "      <td>1227.605633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3_d_hidden=128,learning_rate=0.013718,lstm_bia...</td>\n",
       "      <td>0.013718</td>\n",
       "      <td>128.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ddb98_00004</th>\n",
       "      <td>0.047503</td>\n",
       "      <td>91.865763</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>f6f09c2bc638437498bd2d01fd44961d</td>\n",
       "      <td>2021-05-21_14-47-51</td>\n",
       "      <td>1.621598e+09</td>\n",
       "      <td>1501.235601</td>\n",
       "      <td>...</td>\n",
       "      <td>1501.235601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4_d_hidden=8,learning_rate=0.0013775,lstm_bias...</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>8.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 loss  time_this_iter_s   done  timesteps_total  \\\n",
       "trial_id                                                          \n",
       "ddb98_00000  0.047840        184.533827  False              NaN   \n",
       "ddb98_00001  0.051339         48.592918   True              NaN   \n",
       "ddb98_00002  0.255517         28.091819   True              NaN   \n",
       "ddb98_00003  0.000029         11.933487   True              NaN   \n",
       "ddb98_00004  0.047503         91.865763  False              NaN   \n",
       "\n",
       "             episodes_total  training_iteration  \\\n",
       "trial_id                                          \n",
       "ddb98_00000             NaN                 8.0   \n",
       "ddb98_00001             NaN                20.0   \n",
       "ddb98_00002             NaN                10.0   \n",
       "ddb98_00003             NaN               100.0   \n",
       "ddb98_00004             NaN                16.0   \n",
       "\n",
       "                                experiment_id                 date  \\\n",
       "trial_id                                                             \n",
       "ddb98_00000  a26094effe6246d5af6b147ae60a5933  2021-05-21_14-47-36   \n",
       "ddb98_00001  4ebbb108e09e4b2986c3835634ed0295  2021-05-21_14-38-22   \n",
       "ddb98_00002  4be3033d37004d4f98c47a2d0d564c11  2021-05-21_14-27-36   \n",
       "ddb98_00003  44266641f7124ffda7d62dbdab2b0b49  2021-05-21_14-43-17   \n",
       "ddb98_00004  f6f09c2bc638437498bd2d01fd44961d  2021-05-21_14-47-51   \n",
       "\n",
       "                timestamp  time_total_s  ...  time_since_restore  \\\n",
       "trial_id                                 ...                       \n",
       "ddb98_00000  1.621598e+09   1486.938134  ...         1486.938134   \n",
       "ddb98_00001  1.621597e+09    932.583967  ...          932.583967   \n",
       "ddb98_00002  1.621596e+09    285.940451  ...          285.940451   \n",
       "ddb98_00003  1.621597e+09   1227.605633  ...         1227.605633   \n",
       "ddb98_00004  1.621598e+09   1501.235601  ...         1501.235601   \n",
       "\n",
       "            timesteps_since_restore iterations_since_restore  \\\n",
       "trial_id                                                       \n",
       "ddb98_00000                     0.0                      8.0   \n",
       "ddb98_00001                     0.0                     20.0   \n",
       "ddb98_00002                     0.0                     10.0   \n",
       "ddb98_00003                     0.0                    100.0   \n",
       "ddb98_00004                     0.0                     16.0   \n",
       "\n",
       "                                                experiment_tag  \\\n",
       "trial_id                                                         \n",
       "ddb98_00000  0_d_hidden=128,learning_rate=0.000337,lstm_bia...   \n",
       "ddb98_00001  1_d_hidden=8,learning_rate=0.0097144,lstm_bias...   \n",
       "ddb98_00002  2_d_hidden=64,learning_rate=0.00010157,lstm_bi...   \n",
       "ddb98_00003  3_d_hidden=128,learning_rate=0.013718,lstm_bia...   \n",
       "ddb98_00004  4_d_hidden=8,learning_rate=0.0013775,lstm_bias...   \n",
       "\n",
       "             config.learning_rate  config.d_hidden config.n_lags  \\\n",
       "trial_id                                                           \n",
       "ddb98_00000              0.000337            128.0          30.0   \n",
       "ddb98_00001              0.009714              8.0         100.0   \n",
       "ddb98_00002              0.000102             64.0         100.0   \n",
       "ddb98_00003              0.013718            128.0         100.0   \n",
       "ddb98_00004              0.001378              8.0         100.0   \n",
       "\n",
       "             config.num_hidden_layers  config.lstm_bias  \\\n",
       "trial_id                                                  \n",
       "ddb98_00000                      16.0             False   \n",
       "ddb98_00001                       8.0             False   \n",
       "ddb98_00002                       8.0             False   \n",
       "ddb98_00003                       2.0              True   \n",
       "ddb98_00004                      16.0             False   \n",
       "\n",
       "             config.lstm_bidirectional  \n",
       "trial_id                                \n",
       "ddb98_00000                       True  \n",
       "ddb98_00001                       True  \n",
       "ddb98_00002                      False  \n",
       "ddb98_00003                      False  \n",
       "ddb98_00004                       True  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0111ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8768f19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c17b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7f82b3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 22:44:37,062\tINFO services.py:1269 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2021-05-21 22:44:38,947\tWARNING function_runner.py:545 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m   | Name            | Type       | Params\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m -----------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m 0 | loss            | MASE       | 0     \n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m 1 | logging_metrics | ModuleList | 0     \n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m 2 | net_blocks      | ModuleList | 1.7 M \n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m -----------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m 1.7 M     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m 1.7 M     Total params\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m 6.655     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m   | Name            | Type       | Params\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m -----------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m 0 | loss            | MASE       | 0     \n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m 1 | logging_metrics | ModuleList | 0     \n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m 2 | net_blocks      | ModuleList | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m -----------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m 1.8 M     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m 1.8 M     Total params\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m 7.112     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m   | Name            | Type       | Params\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m -----------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m 0 | loss            | MASE       | 0     \n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m 1 | logging_metrics | ModuleList | 0     \n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m 2 | net_blocks      | ModuleList | 1.8 M \n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m -----------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m 1.8 M     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m 1.8 M     Total params\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m 7.112     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([13, 20]) torch.Size([13, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([13, 20]) torch.Size([13, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([13, 20]) torch.Size([13, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([13, 20]) torch.Size([13, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m  \n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([13, 20]) torch.Size([13, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([13, 20]) torch.Size([13, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([13, 20]) torch.Size([13, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([13, 20]) torch.Size([13, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([13, 20]) torch.Size([13, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([13, 20]) torch.Size([13, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([13, 20]) torch.Size([13, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([13, 20]) torch.Size([13, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39362)\u001b[0m torch.Size([13, 20]) torch.Size([13, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39359)\u001b[0m torch.Size([13, 20]) torch.Size([13, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([64, 20]) torch.Size([64, 20])\n",
      "\u001b[2m\u001b[36m(pid=39358)\u001b[0m torch.Size([13, 20]) torch.Size([13, 20])\n"
     ]
    }
   ],
   "source": [
    "freq = '5min'\n",
    "best_params, results_df = tune_hyperparameters('NBeats',\n",
    "                                               df,\n",
    "                                               freq, \n",
    "                                               num_epochs = 5,\n",
    "                                              num_samples = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb5039a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc46193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412c6513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58834d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.0007916629501075156, 'context_length': 100}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "964ab1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>time_this_iter_s</th>\n",
       "      <th>done</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time_total_s</th>\n",
       "      <th>pid</th>\n",
       "      <th>hostname</th>\n",
       "      <th>node_ip</th>\n",
       "      <th>time_since_restore</th>\n",
       "      <th>timesteps_since_restore</th>\n",
       "      <th>iterations_since_restore</th>\n",
       "      <th>experiment_tag</th>\n",
       "      <th>config.learning_rate</th>\n",
       "      <th>config.context_length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>faaa5_00000</th>\n",
       "      <td>3.072153</td>\n",
       "      <td>18.586122</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>7b67c8b9c6eb44aca2845bc8e56a4ebb</td>\n",
       "      <td>2021-05-21_22-46-19</td>\n",
       "      <td>1621626379</td>\n",
       "      <td>97.502764</td>\n",
       "      <td>39359</td>\n",
       "      <td>MacBook-Pro-Polina.local</td>\n",
       "      <td>192.168.0.7</td>\n",
       "      <td>97.502764</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0_context_length=100,learning_rate=0.00015071</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faaa5_00001</th>\n",
       "      <td>3.030095</td>\n",
       "      <td>17.945912</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>1656dc1c435f4cc883e27b4c35c1d52a</td>\n",
       "      <td>2021-05-21_22-46-14</td>\n",
       "      <td>1621626374</td>\n",
       "      <td>92.765030</td>\n",
       "      <td>39362</td>\n",
       "      <td>MacBook-Pro-Polina.local</td>\n",
       "      <td>192.168.0.7</td>\n",
       "      <td>92.765030</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1_context_length=30,learning_rate=0.0029214</td>\n",
       "      <td>0.002921</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faaa5_00002</th>\n",
       "      <td>2.599046</td>\n",
       "      <td>18.865901</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>3fb214eaabee41b5b0fba90891813774</td>\n",
       "      <td>2021-05-21_22-46-19</td>\n",
       "      <td>1621626379</td>\n",
       "      <td>97.607086</td>\n",
       "      <td>39358</td>\n",
       "      <td>MacBook-Pro-Polina.local</td>\n",
       "      <td>192.168.0.7</td>\n",
       "      <td>97.607086</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2_context_length=100,learning_rate=0.00079166</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 loss  time_this_iter_s  done timesteps_total episodes_total  \\\n",
       "trial_id                                                                       \n",
       "faaa5_00000  3.072153         18.586122  True            None           None   \n",
       "faaa5_00001  3.030095         17.945912  True            None           None   \n",
       "faaa5_00002  2.599046         18.865901  True            None           None   \n",
       "\n",
       "             training_iteration                     experiment_id  \\\n",
       "trial_id                                                            \n",
       "faaa5_00000                   5  7b67c8b9c6eb44aca2845bc8e56a4ebb   \n",
       "faaa5_00001                   5  1656dc1c435f4cc883e27b4c35c1d52a   \n",
       "faaa5_00002                   5  3fb214eaabee41b5b0fba90891813774   \n",
       "\n",
       "                            date   timestamp  time_total_s    pid  \\\n",
       "trial_id                                                            \n",
       "faaa5_00000  2021-05-21_22-46-19  1621626379     97.502764  39359   \n",
       "faaa5_00001  2021-05-21_22-46-14  1621626374     92.765030  39362   \n",
       "faaa5_00002  2021-05-21_22-46-19  1621626379     97.607086  39358   \n",
       "\n",
       "                             hostname      node_ip  time_since_restore  \\\n",
       "trial_id                                                                 \n",
       "faaa5_00000  MacBook-Pro-Polina.local  192.168.0.7           97.502764   \n",
       "faaa5_00001  MacBook-Pro-Polina.local  192.168.0.7           92.765030   \n",
       "faaa5_00002  MacBook-Pro-Polina.local  192.168.0.7           97.607086   \n",
       "\n",
       "             timesteps_since_restore  iterations_since_restore  \\\n",
       "trial_id                                                         \n",
       "faaa5_00000                        0                         5   \n",
       "faaa5_00001                        0                         5   \n",
       "faaa5_00002                        0                         5   \n",
       "\n",
       "                                            experiment_tag  \\\n",
       "trial_id                                                     \n",
       "faaa5_00000  0_context_length=100,learning_rate=0.00015071   \n",
       "faaa5_00001    1_context_length=30,learning_rate=0.0029214   \n",
       "faaa5_00002  2_context_length=100,learning_rate=0.00079166   \n",
       "\n",
       "             config.learning_rate  config.context_length  \n",
       "trial_id                                                  \n",
       "faaa5_00000              0.000151                    100  \n",
       "faaa5_00001              0.002921                     30  \n",
       "faaa5_00002              0.000792                    100  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7bb92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f3671e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e724e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65793d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c417a7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3197518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d343433b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 2 is out of bounds for array of dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8d23d270e87c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamin\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/neural_prophet/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamin\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2857\u001b[0m     \"\"\"\n\u001b[1;32m   2858\u001b[0m     return _wrapreduction(a, np.minimum, 'min', axis, None, out,\n\u001b[0;32m-> 2859\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/neural_prophet/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 2 is out of bounds for array of dimension 0"
     ]
    }
   ],
   "source": [
    "np.min(10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73640f30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf628812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a77f8a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.0601654074086814,\n",
       " 'context_length': 10,\n",
       " 'hidden_size': 16,\n",
       " 'attention_head_size': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fad45cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>time_this_iter_s</th>\n",
       "      <th>done</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time_total_s</th>\n",
       "      <th>...</th>\n",
       "      <th>hostname</th>\n",
       "      <th>node_ip</th>\n",
       "      <th>time_since_restore</th>\n",
       "      <th>timesteps_since_restore</th>\n",
       "      <th>iterations_since_restore</th>\n",
       "      <th>experiment_tag</th>\n",
       "      <th>config.learning_rate</th>\n",
       "      <th>config.context_length</th>\n",
       "      <th>config.hidden_size</th>\n",
       "      <th>config.attention_head_size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217f9_00000</th>\n",
       "      <td>0.585543</td>\n",
       "      <td>12.794128</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>374ac13a0d874cd78833bb53a8fd825b</td>\n",
       "      <td>2021-05-21_15-24-11</td>\n",
       "      <td>1621599851</td>\n",
       "      <td>992.677625</td>\n",
       "      <td>...</td>\n",
       "      <td>MacBook-Pro-Polina.local</td>\n",
       "      <td>192.168.0.7</td>\n",
       "      <td>992.677625</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0_attention_head_size=2,context_length=10,hidd...</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217f9_00001</th>\n",
       "      <td>0.335199</td>\n",
       "      <td>11.922237</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>91d1cf0e9d574b45b972a8df7e14a0bc</td>\n",
       "      <td>2021-05-21_15-39-29</td>\n",
       "      <td>1621600769</td>\n",
       "      <td>1911.347692</td>\n",
       "      <td>...</td>\n",
       "      <td>MacBook-Pro-Polina.local</td>\n",
       "      <td>192.168.0.7</td>\n",
       "      <td>1911.347692</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>1_attention_head_size=1,context_length=10,hidd...</td>\n",
       "      <td>0.060165</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217f9_00002</th>\n",
       "      <td>0.643699</td>\n",
       "      <td>22.244758</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>e9709940e5de48da840b8de46ebc4544</td>\n",
       "      <td>2021-05-21_15-22-29</td>\n",
       "      <td>1621599749</td>\n",
       "      <td>890.869505</td>\n",
       "      <td>...</td>\n",
       "      <td>MacBook-Pro-Polina.local</td>\n",
       "      <td>192.168.0.7</td>\n",
       "      <td>890.869505</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2_attention_head_size=2,context_length=10,hidd...</td>\n",
       "      <td>0.031605</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 loss  time_this_iter_s  done timesteps_total episodes_total  \\\n",
       "trial_id                                                                       \n",
       "217f9_00000  0.585543         12.794128  True            None           None   \n",
       "217f9_00001  0.335199         11.922237  True            None           None   \n",
       "217f9_00002  0.643699         22.244758  True            None           None   \n",
       "\n",
       "             training_iteration                     experiment_id  \\\n",
       "trial_id                                                            \n",
       "217f9_00000                  10  374ac13a0d874cd78833bb53a8fd825b   \n",
       "217f9_00001                 100  91d1cf0e9d574b45b972a8df7e14a0bc   \n",
       "217f9_00002                  10  e9709940e5de48da840b8de46ebc4544   \n",
       "\n",
       "                            date   timestamp  time_total_s  ...  \\\n",
       "trial_id                                                    ...   \n",
       "217f9_00000  2021-05-21_15-24-11  1621599851    992.677625  ...   \n",
       "217f9_00001  2021-05-21_15-39-29  1621600769   1911.347692  ...   \n",
       "217f9_00002  2021-05-21_15-22-29  1621599749    890.869505  ...   \n",
       "\n",
       "                             hostname      node_ip time_since_restore  \\\n",
       "trial_id                                                                \n",
       "217f9_00000  MacBook-Pro-Polina.local  192.168.0.7         992.677625   \n",
       "217f9_00001  MacBook-Pro-Polina.local  192.168.0.7        1911.347692   \n",
       "217f9_00002  MacBook-Pro-Polina.local  192.168.0.7         890.869505   \n",
       "\n",
       "             timesteps_since_restore  iterations_since_restore  \\\n",
       "trial_id                                                         \n",
       "217f9_00000                        0                        10   \n",
       "217f9_00001                        0                       100   \n",
       "217f9_00002                        0                        10   \n",
       "\n",
       "                                                experiment_tag  \\\n",
       "trial_id                                                         \n",
       "217f9_00000  0_attention_head_size=2,context_length=10,hidd...   \n",
       "217f9_00001  1_attention_head_size=1,context_length=10,hidd...   \n",
       "217f9_00002  2_attention_head_size=2,context_length=10,hidd...   \n",
       "\n",
       "            config.learning_rate  config.context_length  config.hidden_size  \\\n",
       "trial_id                                                                      \n",
       "217f9_00000             0.000435                     10                  16   \n",
       "217f9_00001             0.060165                     10                  16   \n",
       "217f9_00002             0.031605                     10                  32   \n",
       "\n",
       "             config.attention_head_size  \n",
       "trial_id                                 \n",
       "217f9_00000                           2  \n",
       "217f9_00001                           1  \n",
       "217f9_00002                           2  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c519c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a08d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e84b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
