{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b2e2806",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ourownstory/neural_prophet/blob/master/example_notebooks/autoregression_yosemite_temps.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80959639",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization with Ray Tune\n",
    "\n",
    "We introduce the module for hyperparameter otpimization with Ray Tune.\n",
    "\n",
    "It supports automatic tuning, with predefined by us hyperparameter sets, as well as manual tuning with user-provided configuration of the parameters.\n",
    "\n",
    "Firstly, we will show how it works with NP model in automated mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0756bd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install NeuralProphet from our repository\n",
    "# !pip install git+https://github.com/adasegroup/neural_prophet.git # may take a while\n",
    "# !pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29c90a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3fa0841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neuralprophet import NeuralProphet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fcfe68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  \"update your install command.\", FutureWarning)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from neuralprophet.hyperparameter_tuner import tune_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7af071d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-01 00:00:00</td>\n",
       "      <td>27.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-05-01 00:05:00</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-05-01 00:10:00</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ds     y\n",
       "0  2017-05-01 00:00:00  27.8\n",
       "1  2017-05-01 00:05:00  27.0\n",
       "2  2017-05-01 00:10:00  26.8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    data_location = \"https://raw.githubusercontent.com/adasegroup/neural_prophet/master/\"\n",
    "else:\n",
    "    data_location = \"../\"\n",
    "\n",
    "df = pd.read_csv(data_location + \"example_data/yosemite_temps.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a2da3f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "freq = '5min'\n",
    "best_params, results_df = tune_hyperparameters('NP',\n",
    "                                               df,\n",
    "                                               freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c15953b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'growth': 'linear',\n",
       " 'n_changepoints': 10,\n",
       " 'changepoints_range': 0.5,\n",
       " 'trend_reg': 10.0,\n",
       " 'yearly_seasonality': False,\n",
       " 'weekly_seasonality': True,\n",
       " 'daily_seasonality': True,\n",
       " 'seasonality_mode': 'additive',\n",
       " 'seasonality_reg': 0.0,\n",
       " 'n_lags': 30,\n",
       " 'd_hidden': 8,\n",
       " 'num_hidden_layers': 2,\n",
       " 'ar_sparsity': 0.8,\n",
       " 'learning_rate': 0.024150905458487568,\n",
       " 'loss_func': 'Huber',\n",
       " 'normalize': 'auto'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf5b127",
   "metadata": {},
   "source": [
    "This dictionary can further be used in initialization of NeuralProphet model.\n",
    "\n",
    "This function has also additional parameters:\n",
    "- **num_epochs**: Max possible number of epochs to train each model.\n",
    "- **num_samples**: Number of samples from hyperparameter spaces to check.\n",
    "- **resources_per_trial**: Resources per trial setting for ray.tune.run, {'cpu': 1, 'gpu': 2} for example\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3158e074",
   "metadata": {},
   "source": [
    "This function additionally will output the dataframe with detailed result of each trial if return_results is set to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42a9dd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config.growth</th>\n",
       "      <th>config.n_changepoints</th>\n",
       "      <th>config.changepoints_range</th>\n",
       "      <th>config.trend_reg</th>\n",
       "      <th>config.yearly_seasonality</th>\n",
       "      <th>config.weekly_seasonality</th>\n",
       "      <th>config.daily_seasonality</th>\n",
       "      <th>config.seasonality_mode</th>\n",
       "      <th>config.seasonality_reg</th>\n",
       "      <th>config.n_lags</th>\n",
       "      <th>config.d_hidden</th>\n",
       "      <th>config.num_hidden_layers</th>\n",
       "      <th>config.ar_sparsity</th>\n",
       "      <th>config.learning_rate</th>\n",
       "      <th>config.loss_func</th>\n",
       "      <th>config.normalize</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8f12d_00000</th>\n",
       "      <td>off</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>multiplicative</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>MSE</td>\n",
       "      <td>off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8f12d_00001</th>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>additive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.019590</td>\n",
       "      <td>MSE</td>\n",
       "      <td>soft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8f12d_00002</th>\n",
       "      <td>linear</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>multiplicative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>MSE</td>\n",
       "      <td>standardize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8f12d_00003</th>\n",
       "      <td>linear</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>multiplicative</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>Huber</td>\n",
       "      <td>off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8f12d_00004</th>\n",
       "      <td>linear</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>additive</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>MSE</td>\n",
       "      <td>soft</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            config.growth  config.n_changepoints  config.changepoints_range  \\\n",
       "trial_id                                                                      \n",
       "8f12d_00000           off                     10                        0.9   \n",
       "8f12d_00001        linear                    100                        0.8   \n",
       "8f12d_00002        linear                     10                        0.5   \n",
       "8f12d_00003        linear                     10                        0.5   \n",
       "8f12d_00004        linear                     10                        0.5   \n",
       "\n",
       "             config.trend_reg  config.yearly_seasonality  \\\n",
       "trial_id                                                   \n",
       "8f12d_00000              10.0                      False   \n",
       "8f12d_00001               0.5                       True   \n",
       "8f12d_00002               0.0                       True   \n",
       "8f12d_00003              10.0                       True   \n",
       "8f12d_00004               0.5                       True   \n",
       "\n",
       "             config.weekly_seasonality  config.daily_seasonality  \\\n",
       "trial_id                                                           \n",
       "8f12d_00000                      False                      True   \n",
       "8f12d_00001                      False                      True   \n",
       "8f12d_00002                      False                     False   \n",
       "8f12d_00003                       True                      True   \n",
       "8f12d_00004                       True                      True   \n",
       "\n",
       "            config.seasonality_mode  config.seasonality_reg  config.n_lags  \\\n",
       "trial_id                                                                     \n",
       "8f12d_00000          multiplicative                     1.0            100   \n",
       "8f12d_00001                additive                     0.0             10   \n",
       "8f12d_00002          multiplicative                     0.0             30   \n",
       "8f12d_00003          multiplicative                     0.5             30   \n",
       "8f12d_00004                additive                     0.5             10   \n",
       "\n",
       "             config.d_hidden  config.num_hidden_layers  config.ar_sparsity  \\\n",
       "trial_id                                                                     \n",
       "8f12d_00000               64                         8                 0.8   \n",
       "8f12d_00001              128                        16                 0.8   \n",
       "8f12d_00002              128                         2                 0.1   \n",
       "8f12d_00003                8                         8                 0.8   \n",
       "8f12d_00004               64                         2                 0.8   \n",
       "\n",
       "             config.learning_rate config.loss_func config.normalize  \n",
       "trial_id                                                             \n",
       "8f12d_00000              0.000733              MSE              off  \n",
       "8f12d_00001              0.019590              MSE             soft  \n",
       "8f12d_00002              0.000306              MSE      standardize  \n",
       "8f12d_00003              0.000195            Huber              off  \n",
       "8f12d_00004              0.008229              MSE             soft  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[['config.growth', 'config.n_changepoints', 'config.changepoints_range',\n",
    "       'config.trend_reg', 'config.yearly_seasonality',\n",
    "       'config.weekly_seasonality', 'config.daily_seasonality',\n",
    "       'config.seasonality_mode', 'config.seasonality_reg', 'config.n_lags',\n",
    "       'config.d_hidden', 'config.num_hidden_layers', 'config.ar_sparsity',\n",
    "       'config.learning_rate', 'config.loss_func', 'config.normalize']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136dba8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c754f6a",
   "metadata": {},
   "source": [
    "## Manual mode\n",
    "\n",
    "In case of manual mode, a user must provide a config dictionary with hyperparameter spaces in compatability with Ray Tune api.\n",
    "\n",
    "We provide a minimal example below, for more information on Search Spaces withit this link https://docs.ray.io/en/master/tune/api_docs/search_space.html?highlight=tune.choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1549cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "config = {'n_lags': tune.grid_search([10, 20, 30]),\n",
    "          'learning_rate': tune.loguniform(1e-4, 1e-1),\n",
    "          'num_hidden_layers': tune.choice([2, 8, 16])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b24847",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = '5min'\n",
    "best_params, results_df = tune_hyperparameters('NP', \n",
    "                                               df,\n",
    "                                               freq, \n",
    "                                               mode = 'manual',\n",
    "                                              config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4517177",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1335411",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13716c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95bf1e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m 0 | lstm      | LSTM         | 6.1 M \n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m 1 | linear    | Linear       | 257   \n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m 6.1 M     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m 6.1 M     Total params\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m 24.241    Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m 0 | lstm      | LSTM         | 271 K \n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m 1 | linear    | Linear       | 65    \n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m 271 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m 271 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m 1.086     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28978)\u001b[0m \n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m 0 | lstm      | LSTM         | 7.6 K \n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m 1 | linear    | Linear       | 9     \n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m 7.6 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m 7.6 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m 0.030     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28977)\u001b[0m \n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m 0 | lstm      | LSTM         | 51.7 K\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m 1 | linear    | Linear       | 65    \n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m 51.8 K    Trainable params\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m 51.8 K    Total params\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m 0.207     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28973)\u001b[0m \n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m 0 | lstm      | LSTM         | 17.7 K\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m 1 | linear    | Linear       | 17    \n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m 17.7 K    Trainable params\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m 17.7 K    Total params\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m 0.071     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28976)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m 0 | lstm      | LSTM         | 249 K \n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m 1 | linear    | Linear       | 129   \n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m 249 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m 249 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m 1.000     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28972)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m 0 | lstm      | LSTM         | 30.0 K\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m 1 | linear    | Linear       | 17    \n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m 30.0 K    Trainable params\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m 30.0 K    Total params\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m 0.120     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m 0 | lstm      | LSTM         | 2.1 M \n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m 1 | linear    | Linear       | 129   \n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m 2.1 M     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m 2.1 M     Total params\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m 8.332     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28971)\u001b[0m \n",
      "                                       \n",
      "                                       \n",
      "                                       \n",
      "                                       \n",
      "                                       \n",
      "                                       \n",
      "                                       \n",
      "                                       \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m 0 | lstm      | LSTM         | 1.2 K \n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m 1 | linear    | Linear       | 9     \n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m 1.2 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m 1.2 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m 0.005     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29025)\u001b[0m \n",
      "                                       \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m 0 | lstm      | LSTM         | 2.9 M \n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m 1 | linear    | Linear       | 257   \n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m 2.9 M     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m 2.9 M     Total params\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m 11.642    Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m 0 | lstm      | LSTM         | 275 K \n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m 1 | linear    | Linear       | 65    \n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m 275 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m 275 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m 1.102     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29042)\u001b[0m \n",
      "                                       \n",
      "                                       \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m 0 | lstm      | LSTM         | 57.9 K\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m 1 | linear    | Linear       | 65    \n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m 57.9 K    Trainable params\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m 57.9 K    Total params\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m 0.232     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m \n",
      "                                       \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m 0 | lstm      | LSTM         | 146 K \n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m 1 | linear    | Linear       | 129   \n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m 146 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m 146 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m 0.586     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29089)\u001b[0m \n",
      "                                       \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m 0 | lstm      | LSTM         | 6.1 M \n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m 1 | linear    | Linear       | 257   \n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m 6.1 M     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m 6.1 M     Total params\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m 24.290    Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m 0 | lstm      | LSTM         | 6.2 M \n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m 1 | linear    | Linear       | 257   \n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m 6.2 M     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m 6.2 M     Total params\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m 24.659    Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m \n",
      "                                       \n",
      "                                       \n",
      "2021-05-21 14:43:19,049\tWARNING util.py:162 -- The `start_trial` operation took 1.055 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m 0 | lstm      | LSTM         | 74.8 K\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m 1 | linear    | Linear       | 65    \n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m 74.8 K    Trainable params\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m 74.8 K    Total params\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m 0.299     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m \n",
      "                                       \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   | Name      | Type         | Params\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m 0 | lstm      | LSTM         | 559 K \n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m 1 | linear    | Linear       | 257   \n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m 2 | loss_func | SmoothL1Loss | 0     \n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m -------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m 559 K     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m 559 K     Total params\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m 2.237     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m WARNING - (py.warnings._showwarnmsg) - /Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   warnings.warn(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m \n",
      "                                       \n",
      "2021-05-21 14:48:23,596\tWARNING tune.py:507 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m 2021-05-21 14:48:23,672\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 349, in step\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m     block=True, timeout=RESULT_FETCH_TIMEOUT)\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/queue.py\", line 179, in get\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m     self.not_empty.wait(remaining)\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/threading.py\", line 300, in wait\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m     gotit = waiter.acquire(True, timeout)\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=28974)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m 2021-05-21 14:48:23,660\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 349, in step\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m     block=True, timeout=RESULT_FETCH_TIMEOUT)\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/queue.py\", line 179, in get\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m     self.not_empty.wait(remaining)\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/threading.py\", line 300, in wait\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m     gotit = waiter.acquire(True, timeout)\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=29098)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m 2021-05-21 14:48:23,648\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 349, in step\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m     block=True, timeout=RESULT_FETCH_TIMEOUT)\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/queue.py\", line 179, in get\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m     self.not_empty.wait(remaining)\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/threading.py\", line 300, in wait\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m     gotit = waiter.acquire(True, timeout)\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=29108)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m 2021-05-21 14:48:23,676\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 349, in step\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m     block=True, timeout=RESULT_FETCH_TIMEOUT)\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/queue.py\", line 179, in get\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m     self.not_empty.wait(remaining)\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/threading.py\", line 300, in wait\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m     gotit = waiter.acquire(True, timeout)\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=29147)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m 2021-05-21 14:48:23,692\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 349, in step\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m     block=True, timeout=RESULT_FETCH_TIMEOUT)\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/queue.py\", line 179, in get\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m     self.not_empty.wait(remaining)\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/threading.py\", line 300, in wait\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m     gotit = waiter.acquire(True, timeout)\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=28975)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m 2021-05-21 14:48:23,712\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 349, in step\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m     block=True, timeout=RESULT_FETCH_TIMEOUT)\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/queue.py\", line 179, in get\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m     self.not_empty.wait(remaining)\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/threading.py\", line 300, in wait\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m     gotit = waiter.acquire(True, timeout)\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=29034)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m 2021-05-21 14:48:23,694\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 349, in step\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m     block=True, timeout=RESULT_FETCH_TIMEOUT)\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/queue.py\", line 179, in get\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m     self.not_empty.wait(remaining)\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/threading.py\", line 300, in wait\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m     gotit = waiter.acquire(True, timeout)\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=29083)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m 2021-05-21 14:48:23,715\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 349, in step\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m     block=True, timeout=RESULT_FETCH_TIMEOUT)\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/queue.py\", line 179, in get\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m     self.not_empty.wait(remaining)\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/threading.py\", line 300, in wait\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m     gotit = waiter.acquire(True, timeout)\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=29124)\u001b[0m SystemExit: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 14:48:23,922\tERROR tune.py:545 -- Trials did not complete: [train_LSTM_tune_ddb98_00000, train_LSTM_tune_ddb98_00004, train_LSTM_tune_ddb98_00009, train_LSTM_tune_ddb98_00011, train_LSTM_tune_ddb98_00013, train_LSTM_tune_ddb98_00014, train_LSTM_tune_ddb98_00015, train_LSTM_tune_ddb98_00016, train_LSTM_tune_ddb98_00017, train_LSTM_tune_ddb98_00018, train_LSTM_tune_ddb98_00019, train_LSTM_tune_ddb98_00020, train_LSTM_tune_ddb98_00021, train_LSTM_tune_ddb98_00022, train_LSTM_tune_ddb98_00023, train_LSTM_tune_ddb98_00024, train_LSTM_tune_ddb98_00025, train_LSTM_tune_ddb98_00026, train_LSTM_tune_ddb98_00027, train_LSTM_tune_ddb98_00028, train_LSTM_tune_ddb98_00029, train_LSTM_tune_ddb98_00030, train_LSTM_tune_ddb98_00031, train_LSTM_tune_ddb98_00032, train_LSTM_tune_ddb98_00033, train_LSTM_tune_ddb98_00034, train_LSTM_tune_ddb98_00035, train_LSTM_tune_ddb98_00036, train_LSTM_tune_ddb98_00037, train_LSTM_tune_ddb98_00038, train_LSTM_tune_ddb98_00039]\n",
      "2021-05-21 14:48:23,923\tWARNING tune.py:554 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    }
   ],
   "source": [
    "freq = '5min'\n",
    "best_params, results_df = tune_hyperparameters('LSTM',\n",
    "                                               df,\n",
    "                                               freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d7f3a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.013718466327407178,\n",
       " 'd_hidden': 128,\n",
       " 'n_lags': 100,\n",
       " 'num_hidden_layers': 2,\n",
       " 'lstm_bias': True,\n",
       " 'lstm_bidirectional': False}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ba8d816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>time_this_iter_s</th>\n",
       "      <th>done</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time_total_s</th>\n",
       "      <th>...</th>\n",
       "      <th>time_since_restore</th>\n",
       "      <th>timesteps_since_restore</th>\n",
       "      <th>iterations_since_restore</th>\n",
       "      <th>experiment_tag</th>\n",
       "      <th>config.learning_rate</th>\n",
       "      <th>config.d_hidden</th>\n",
       "      <th>config.n_lags</th>\n",
       "      <th>config.num_hidden_layers</th>\n",
       "      <th>config.lstm_bias</th>\n",
       "      <th>config.lstm_bidirectional</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ddb98_00000</th>\n",
       "      <td>0.047840</td>\n",
       "      <td>184.533827</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>a26094effe6246d5af6b147ae60a5933</td>\n",
       "      <td>2021-05-21_14-47-36</td>\n",
       "      <td>1.621598e+09</td>\n",
       "      <td>1486.938134</td>\n",
       "      <td>...</td>\n",
       "      <td>1486.938134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0_d_hidden=128,learning_rate=0.000337,lstm_bia...</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>128.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ddb98_00001</th>\n",
       "      <td>0.051339</td>\n",
       "      <td>48.592918</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4ebbb108e09e4b2986c3835634ed0295</td>\n",
       "      <td>2021-05-21_14-38-22</td>\n",
       "      <td>1.621597e+09</td>\n",
       "      <td>932.583967</td>\n",
       "      <td>...</td>\n",
       "      <td>932.583967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1_d_hidden=8,learning_rate=0.0097144,lstm_bias...</td>\n",
       "      <td>0.009714</td>\n",
       "      <td>8.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ddb98_00002</th>\n",
       "      <td>0.255517</td>\n",
       "      <td>28.091819</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4be3033d37004d4f98c47a2d0d564c11</td>\n",
       "      <td>2021-05-21_14-27-36</td>\n",
       "      <td>1.621596e+09</td>\n",
       "      <td>285.940451</td>\n",
       "      <td>...</td>\n",
       "      <td>285.940451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2_d_hidden=64,learning_rate=0.00010157,lstm_bi...</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>64.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ddb98_00003</th>\n",
       "      <td>0.000029</td>\n",
       "      <td>11.933487</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>44266641f7124ffda7d62dbdab2b0b49</td>\n",
       "      <td>2021-05-21_14-43-17</td>\n",
       "      <td>1.621597e+09</td>\n",
       "      <td>1227.605633</td>\n",
       "      <td>...</td>\n",
       "      <td>1227.605633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3_d_hidden=128,learning_rate=0.013718,lstm_bia...</td>\n",
       "      <td>0.013718</td>\n",
       "      <td>128.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ddb98_00004</th>\n",
       "      <td>0.047503</td>\n",
       "      <td>91.865763</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>f6f09c2bc638437498bd2d01fd44961d</td>\n",
       "      <td>2021-05-21_14-47-51</td>\n",
       "      <td>1.621598e+09</td>\n",
       "      <td>1501.235601</td>\n",
       "      <td>...</td>\n",
       "      <td>1501.235601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4_d_hidden=8,learning_rate=0.0013775,lstm_bias...</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>8.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 loss  time_this_iter_s   done  timesteps_total  \\\n",
       "trial_id                                                          \n",
       "ddb98_00000  0.047840        184.533827  False              NaN   \n",
       "ddb98_00001  0.051339         48.592918   True              NaN   \n",
       "ddb98_00002  0.255517         28.091819   True              NaN   \n",
       "ddb98_00003  0.000029         11.933487   True              NaN   \n",
       "ddb98_00004  0.047503         91.865763  False              NaN   \n",
       "\n",
       "             episodes_total  training_iteration  \\\n",
       "trial_id                                          \n",
       "ddb98_00000             NaN                 8.0   \n",
       "ddb98_00001             NaN                20.0   \n",
       "ddb98_00002             NaN                10.0   \n",
       "ddb98_00003             NaN               100.0   \n",
       "ddb98_00004             NaN                16.0   \n",
       "\n",
       "                                experiment_id                 date  \\\n",
       "trial_id                                                             \n",
       "ddb98_00000  a26094effe6246d5af6b147ae60a5933  2021-05-21_14-47-36   \n",
       "ddb98_00001  4ebbb108e09e4b2986c3835634ed0295  2021-05-21_14-38-22   \n",
       "ddb98_00002  4be3033d37004d4f98c47a2d0d564c11  2021-05-21_14-27-36   \n",
       "ddb98_00003  44266641f7124ffda7d62dbdab2b0b49  2021-05-21_14-43-17   \n",
       "ddb98_00004  f6f09c2bc638437498bd2d01fd44961d  2021-05-21_14-47-51   \n",
       "\n",
       "                timestamp  time_total_s  ...  time_since_restore  \\\n",
       "trial_id                                 ...                       \n",
       "ddb98_00000  1.621598e+09   1486.938134  ...         1486.938134   \n",
       "ddb98_00001  1.621597e+09    932.583967  ...          932.583967   \n",
       "ddb98_00002  1.621596e+09    285.940451  ...          285.940451   \n",
       "ddb98_00003  1.621597e+09   1227.605633  ...         1227.605633   \n",
       "ddb98_00004  1.621598e+09   1501.235601  ...         1501.235601   \n",
       "\n",
       "            timesteps_since_restore iterations_since_restore  \\\n",
       "trial_id                                                       \n",
       "ddb98_00000                     0.0                      8.0   \n",
       "ddb98_00001                     0.0                     20.0   \n",
       "ddb98_00002                     0.0                     10.0   \n",
       "ddb98_00003                     0.0                    100.0   \n",
       "ddb98_00004                     0.0                     16.0   \n",
       "\n",
       "                                                experiment_tag  \\\n",
       "trial_id                                                         \n",
       "ddb98_00000  0_d_hidden=128,learning_rate=0.000337,lstm_bia...   \n",
       "ddb98_00001  1_d_hidden=8,learning_rate=0.0097144,lstm_bias...   \n",
       "ddb98_00002  2_d_hidden=64,learning_rate=0.00010157,lstm_bi...   \n",
       "ddb98_00003  3_d_hidden=128,learning_rate=0.013718,lstm_bia...   \n",
       "ddb98_00004  4_d_hidden=8,learning_rate=0.0013775,lstm_bias...   \n",
       "\n",
       "             config.learning_rate  config.d_hidden config.n_lags  \\\n",
       "trial_id                                                           \n",
       "ddb98_00000              0.000337            128.0          30.0   \n",
       "ddb98_00001              0.009714              8.0         100.0   \n",
       "ddb98_00002              0.000102             64.0         100.0   \n",
       "ddb98_00003              0.013718            128.0         100.0   \n",
       "ddb98_00004              0.001378              8.0         100.0   \n",
       "\n",
       "             config.num_hidden_layers  config.lstm_bias  \\\n",
       "trial_id                                                  \n",
       "ddb98_00000                      16.0             False   \n",
       "ddb98_00001                       8.0             False   \n",
       "ddb98_00002                       8.0             False   \n",
       "ddb98_00003                       2.0              True   \n",
       "ddb98_00004                      16.0             False   \n",
       "\n",
       "             config.lstm_bidirectional  \n",
       "trial_id                                \n",
       "ddb98_00000                       True  \n",
       "ddb98_00001                       True  \n",
       "ddb98_00002                      False  \n",
       "ddb98_00003                      False  \n",
       "ddb98_00004                       True  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b4cdbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b6bb81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c4a4c13",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 16:12:55,547\tINFO services.py:1269 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n",
      "2021-05-21 16:12:57,430\tWARNING function_runner.py:545 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   | Name            | Type       | Params\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m -----------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m 0 | loss            | MASE       | 0     \n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m 1 | logging_metrics | ModuleList | 0     \n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m 2 | net_blocks      | ModuleList | 7.4 M \n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m -----------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m 7.4 M     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m 7.4 M     Total params\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m 29.405    Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   | Name            | Type       | Params\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m -----------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m 0 | loss            | MASE       | 0     \n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m 1 | logging_metrics | ModuleList | 0     \n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m 2 | net_blocks      | ModuleList | 7.5 M \n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m -----------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m 7.5 M     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m 7.5 M     Total params\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m 29.993    Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   | Name            | Type       | Params\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m -----------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m 0 | loss            | MASE       | 0     \n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m 1 | logging_metrics | ModuleList | 0     \n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m 2 | net_blocks      | ModuleList | 7.4 M \n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m -----------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m 7.4 M     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m 7.4 M     Total params\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m 29.405    Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m 2021-05-21 16:13:04,716\tERROR function_runner.py:254 -- Runner Thread raised error.\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     self._status_reporter.get_checkpoint())\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/utils/trainable.py\", line 339, in _inner\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     inner(config, checkpoint_dir=None)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/utils/trainable.py\", line 330, in inner\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     trainable(config, **fn_kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/fds/neural_prophet/neuralprophet/hyperparameter_tuner.py\", line 99, in train_NBeats_tune\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 499, in fit\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     self.dispatch()\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 546, in dispatch\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     self.accelerator.start_training(self)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 73, in start_training\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     self.training_type_plugin.start_training(trainer)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 114, in start_training\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     self._results = trainer.run_train()\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 637, in run_train\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     self.train_loop.run_training_epoch()\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 492, in run_training_epoch\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 654, in run_training_batch\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 433, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     using_lbfgs=is_lbfgs,\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py\", line 1390, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     optimizer.step(closure=optimizer_closure)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py\", line 214, in step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py\", line 134, in __optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 277, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 282, in run_optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 163, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     optimizer.step(closure=lambda_closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/torch/optim/optimizer.py\", line 89, in wrapper\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_forecasting/optim.py\", line 131, in step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     _ = closure()\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 649, in train_step_and_backward_closure\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     split_batch, batch_idx, opt_idx, optimizer, self.trainer.hiddens\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 742, in training_step_and_backward\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 293, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     training_step_output = self.trainer.accelerator.training_step(args)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 156, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     return self.training_type_plugin.training_step(*args)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 125, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     return self.lightning_module.training_step(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/fds/neural_prophet/neuralprophet/models/NBeats.py\", line 24, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     assert len(log[\"loss\"].size()) >= 1\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m AssertionError\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m Exception in thread Thread-2:\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     self._status_reporter.get_checkpoint())\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/utils/trainable.py\", line 339, in _inner\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     inner(config, checkpoint_dir=None)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/utils/trainable.py\", line 330, in inner\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     trainable(config, **fn_kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/fds/neural_prophet/neuralprophet/hyperparameter_tuner.py\", line 99, in train_NBeats_tune\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 499, in fit\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     self.dispatch()\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 546, in dispatch\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     self.accelerator.start_training(self)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 73, in start_training\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     self.training_type_plugin.start_training(trainer)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 114, in start_training\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     self._results = trainer.run_train()\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 637, in run_train\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     self.train_loop.run_training_epoch()\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 492, in run_training_epoch\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 654, in run_training_batch\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 433, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     using_lbfgs=is_lbfgs,\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py\", line 1390, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     optimizer.step(closure=optimizer_closure)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py\", line 214, in step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py\", line 134, in __optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 277, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 282, in run_optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 163, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     optimizer.step(closure=lambda_closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/torch/optim/optimizer.py\", line 89, in wrapper\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_forecasting/optim.py\", line 131, in step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     _ = closure()\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 649, in train_step_and_backward_closure\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     split_batch, batch_idx, opt_idx, optimizer, self.trainer.hiddens\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 742, in training_step_and_backward\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 293, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     training_step_output = self.trainer.accelerator.training_step(args)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 156, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     return self.training_type_plugin.training_step(*args)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 125, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     return self.lightning_module.training_step(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m   File \"/Users/polina/fds/neural_prophet/neuralprophet/models/NBeats.py\", line 24, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m     assert len(log[\"loss\"].size()) >= 1\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m AssertionError\n",
      "\u001b[2m\u001b[36m(pid=33144)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m 2021-05-21 16:13:04,794\tERROR function_runner.py:254 -- Runner Thread raised error.\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     self._status_reporter.get_checkpoint())\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/utils/trainable.py\", line 339, in _inner\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     inner(config, checkpoint_dir=None)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/utils/trainable.py\", line 330, in inner\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     trainable(config, **fn_kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/fds/neural_prophet/neuralprophet/hyperparameter_tuner.py\", line 99, in train_NBeats_tune\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 499, in fit\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     self.dispatch()\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 546, in dispatch\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     self.accelerator.start_training(self)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 73, in start_training\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     self.training_type_plugin.start_training(trainer)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 114, in start_training\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     self._results = trainer.run_train()\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 637, in run_train\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     self.train_loop.run_training_epoch()\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 492, in run_training_epoch\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 654, in run_training_batch\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 433, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     using_lbfgs=is_lbfgs,\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py\", line 1390, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     optimizer.step(closure=optimizer_closure)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py\", line 214, in step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py\", line 134, in __optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 277, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 282, in run_optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 163, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     optimizer.step(closure=lambda_closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/torch/optim/optimizer.py\", line 89, in wrapper\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_forecasting/optim.py\", line 131, in step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     _ = closure()\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 649, in train_step_and_backward_closure\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     split_batch, batch_idx, opt_idx, optimizer, self.trainer.hiddens\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 742, in training_step_and_backward\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 293, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     training_step_output = self.trainer.accelerator.training_step(args)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 156, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     return self.training_type_plugin.training_step(*args)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 125, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     return self.lightning_module.training_step(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/fds/neural_prophet/neuralprophet/models/NBeats.py\", line 24, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     assert len(log[\"loss\"].size()) >= 1\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m AssertionError\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m Exception in thread Thread-2:\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     self._status_reporter.get_checkpoint())\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/utils/trainable.py\", line 339, in _inner\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     inner(config, checkpoint_dir=None)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/utils/trainable.py\", line 330, in inner\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     trainable(config, **fn_kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/fds/neural_prophet/neuralprophet/hyperparameter_tuner.py\", line 99, in train_NBeats_tune\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 499, in fit\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     self.dispatch()\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 546, in dispatch\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     self.accelerator.start_training(self)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 73, in start_training\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     self.training_type_plugin.start_training(trainer)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 114, in start_training\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     self._results = trainer.run_train()\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 637, in run_train\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     self.train_loop.run_training_epoch()\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 492, in run_training_epoch\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 654, in run_training_batch\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 433, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     using_lbfgs=is_lbfgs,\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py\", line 1390, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     optimizer.step(closure=optimizer_closure)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py\", line 214, in step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py\", line 134, in __optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 277, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 282, in run_optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 163, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     optimizer.step(closure=lambda_closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/torch/optim/optimizer.py\", line 89, in wrapper\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_forecasting/optim.py\", line 131, in step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     _ = closure()\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 649, in train_step_and_backward_closure\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     split_batch, batch_idx, opt_idx, optimizer, self.trainer.hiddens\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 742, in training_step_and_backward\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 293, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     training_step_output = self.trainer.accelerator.training_step(args)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 156, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     return self.training_type_plugin.training_step(*args)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 125, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     return self.lightning_module.training_step(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m   File \"/Users/polina/fds/neural_prophet/neuralprophet/models/NBeats.py\", line 24, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m     assert len(log[\"loss\"].size()) >= 1\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m AssertionError\n",
      "\u001b[2m\u001b[36m(pid=33142)\u001b[0m \n",
      "2021-05-21 16:13:04,842\tERROR trial_runner.py:732 -- Trial train_NBeats_tune_42ab3_00001: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=33144, ip=192.168.0.7)\n",
      "  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "    result = self.train()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "    result = self.step()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n",
      "    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=33144, ip=192.168.0.7)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "    self._entrypoint()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n",
      "    self._status_reporter.get_checkpoint())\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/utils/trainable.py\", line 339, in _inner\n",
      "    inner(config, checkpoint_dir=None)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/utils/trainable.py\", line 330, in inner\n",
      "    trainable(config, **fn_kwargs)\n",
      "  File \"/Users/polina/fds/neural_prophet/neuralprophet/hyperparameter_tuner.py\", line 99, in train_NBeats_tune\n",
      "    trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 499, in fit\n",
      "    self.dispatch()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 546, in dispatch\n",
      "    self.accelerator.start_training(self)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 73, in start_training\n",
      "    self.training_type_plugin.start_training(trainer)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 114, in start_training\n",
      "    self._results = trainer.run_train()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 637, in run_train\n",
      "    self.train_loop.run_training_epoch()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 492, in run_training_epoch\n",
      "    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 654, in run_training_batch\n",
      "    self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 433, in optimizer_step\n",
      "    using_lbfgs=is_lbfgs,\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py\", line 1390, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py\", line 214, in step\n",
      "    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py\", line 134, in __optimizer_step\n",
      "    trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 277, in optimizer_step\n",
      "    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 282, in run_optimizer_step\n",
      "    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 163, in optimizer_step\n",
      "    optimizer.step(closure=lambda_closure, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/torch/optim/optimizer.py\", line 89, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_forecasting/optim.py\", line 131, in step\n",
      "    _ = closure()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 649, in train_step_and_backward_closure\n",
      "    split_batch, batch_idx, opt_idx, optimizer, self.trainer.hiddens\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 742, in training_step_and_backward\n",
      "    result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 293, in training_step\n",
      "    training_step_output = self.trainer.accelerator.training_step(args)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 156, in training_step\n",
      "    return self.training_type_plugin.training_step(*args)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 125, in training_step\n",
      "    return self.lightning_module.training_step(*args, **kwargs)\n",
      "  File \"/Users/polina/fds/neural_prophet/neuralprophet/models/NBeats.py\", line 24, in training_step\n",
      "    assert len(log[\"loss\"].size()) >= 1\n",
      "AssertionError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m 2021-05-21 16:13:04,877\tERROR function_runner.py:254 -- Runner Thread raised error.\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     self._status_reporter.get_checkpoint())\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/utils/trainable.py\", line 339, in _inner\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     inner(config, checkpoint_dir=None)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/utils/trainable.py\", line 330, in inner\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     trainable(config, **fn_kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/fds/neural_prophet/neuralprophet/hyperparameter_tuner.py\", line 99, in train_NBeats_tune\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 499, in fit\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     self.dispatch()\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 546, in dispatch\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     self.accelerator.start_training(self)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 73, in start_training\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     self.training_type_plugin.start_training(trainer)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 114, in start_training\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     self._results = trainer.run_train()\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 637, in run_train\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     self.train_loop.run_training_epoch()\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 492, in run_training_epoch\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 654, in run_training_batch\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 433, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     using_lbfgs=is_lbfgs,\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py\", line 1390, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     optimizer.step(closure=optimizer_closure)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py\", line 214, in step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py\", line 134, in __optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 277, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 282, in run_optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 163, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     optimizer.step(closure=lambda_closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/torch/optim/optimizer.py\", line 89, in wrapper\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_forecasting/optim.py\", line 131, in step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     _ = closure()\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 649, in train_step_and_backward_closure\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     split_batch, batch_idx, opt_idx, optimizer, self.trainer.hiddens\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 742, in training_step_and_backward\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 293, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     training_step_output = self.trainer.accelerator.training_step(args)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 156, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     return self.training_type_plugin.training_step(*args)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 125, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     return self.lightning_module.training_step(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/fds/neural_prophet/neuralprophet/models/NBeats.py\", line 24, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     assert len(log[\"loss\"].size()) >= 1\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m AssertionError\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m Exception in thread Thread-2:\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 267, in run\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     self._status_reporter.get_checkpoint())\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/utils/trainable.py\", line 339, in _inner\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     inner(config, checkpoint_dir=None)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/utils/trainable.py\", line 330, in inner\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     trainable(config, **fn_kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/fds/neural_prophet/neuralprophet/hyperparameter_tuner.py\", line 99, in train_NBeats_tune\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 499, in fit\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     self.dispatch()\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 546, in dispatch\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     self.accelerator.start_training(self)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 73, in start_training\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     self.training_type_plugin.start_training(trainer)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 114, in start_training\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     self._results = trainer.run_train()\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 637, in run_train\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     self.train_loop.run_training_epoch()\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 492, in run_training_epoch\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 654, in run_training_batch\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 433, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     using_lbfgs=is_lbfgs,\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py\", line 1390, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     optimizer.step(closure=optimizer_closure)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py\", line 214, in step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py\", line 134, in __optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 277, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 282, in run_optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 163, in optimizer_step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     optimizer.step(closure=lambda_closure, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/torch/optim/optimizer.py\", line 89, in wrapper\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_forecasting/optim.py\", line 131, in step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     _ = closure()\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 649, in train_step_and_backward_closure\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     split_batch, batch_idx, opt_idx, optimizer, self.trainer.hiddens\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 742, in training_step_and_backward\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 293, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     training_step_output = self.trainer.accelerator.training_step(args)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 156, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     return self.training_type_plugin.training_step(*args)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 125, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     return self.lightning_module.training_step(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m   File \"/Users/polina/fds/neural_prophet/neuralprophet/models/NBeats.py\", line 24, in training_step\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m     assert len(log[\"loss\"].size()) >= 1\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m AssertionError\n",
      "\u001b[2m\u001b[36m(pid=33141)\u001b[0m \n",
      "2021-05-21 16:13:04,908\tERROR trial_runner.py:732 -- Trial train_NBeats_tune_42ab3_00002: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=33142, ip=192.168.0.7)\n",
      "  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "    result = self.train()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "    result = self.step()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n",
      "    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=33142, ip=192.168.0.7)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "    self._entrypoint()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n",
      "    self._status_reporter.get_checkpoint())\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/utils/trainable.py\", line 339, in _inner\n",
      "    inner(config, checkpoint_dir=None)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/utils/trainable.py\", line 330, in inner\n",
      "    trainable(config, **fn_kwargs)\n",
      "  File \"/Users/polina/fds/neural_prophet/neuralprophet/hyperparameter_tuner.py\", line 99, in train_NBeats_tune\n",
      "    trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 499, in fit\n",
      "    self.dispatch()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 546, in dispatch\n",
      "    self.accelerator.start_training(self)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 73, in start_training\n",
      "    self.training_type_plugin.start_training(trainer)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 114, in start_training\n",
      "    self._results = trainer.run_train()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 637, in run_train\n",
      "    self.train_loop.run_training_epoch()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 492, in run_training_epoch\n",
      "    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 654, in run_training_batch\n",
      "    self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 433, in optimizer_step\n",
      "    using_lbfgs=is_lbfgs,\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py\", line 1390, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py\", line 214, in step\n",
      "    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py\", line 134, in __optimizer_step\n",
      "    trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 277, in optimizer_step\n",
      "    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 282, in run_optimizer_step\n",
      "    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 163, in optimizer_step\n",
      "    optimizer.step(closure=lambda_closure, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/torch/optim/optimizer.py\", line 89, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_forecasting/optim.py\", line 131, in step\n",
      "    _ = closure()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 649, in train_step_and_backward_closure\n",
      "    split_batch, batch_idx, opt_idx, optimizer, self.trainer.hiddens\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 742, in training_step_and_backward\n",
      "    result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 293, in training_step\n",
      "    training_step_output = self.trainer.accelerator.training_step(args)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 156, in training_step\n",
      "    return self.training_type_plugin.training_step(*args)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 125, in training_step\n",
      "    return self.lightning_module.training_step(*args, **kwargs)\n",
      "  File \"/Users/polina/fds/neural_prophet/neuralprophet/models/NBeats.py\", line 24, in training_step\n",
      "    assert len(log[\"loss\"].size()) >= 1\n",
      "AssertionError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 16:13:05,054\tERROR trial_runner.py:732 -- Trial train_NBeats_tune_42ab3_00000: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/worker.py\", line 1481, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=33141, ip=192.168.0.7)\n",
      "  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 173, in train_buffered\n",
      "    result = self.train()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/trainable.py\", line 232, in train\n",
      "    result = self.step()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 366, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 513, in _report_thread_runner_error\n",
      "    (\"Trial raised an exception. Traceback:\\n{}\".format(err_tb_str)\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=33141, ip=192.168.0.7)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "    self._entrypoint()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 316, in entrypoint\n",
      "    self._status_reporter.get_checkpoint())\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/function_runner.py\", line 580, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/utils/trainable.py\", line 339, in _inner\n",
      "    inner(config, checkpoint_dir=None)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/utils/trainable.py\", line 330, in inner\n",
      "    trainable(config, **fn_kwargs)\n",
      "  File \"/Users/polina/fds/neural_prophet/neuralprophet/hyperparameter_tuner.py\", line 99, in train_NBeats_tune\n",
      "    trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 499, in fit\n",
      "    self.dispatch()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 546, in dispatch\n",
      "    self.accelerator.start_training(self)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 73, in start_training\n",
      "    self.training_type_plugin.start_training(trainer)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 114, in start_training\n",
      "    self._results = trainer.run_train()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 637, in run_train\n",
      "    self.train_loop.run_training_epoch()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 492, in run_training_epoch\n",
      "    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 654, in run_training_batch\n",
      "    self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 433, in optimizer_step\n",
      "    using_lbfgs=is_lbfgs,\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py\", line 1390, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py\", line 214, in step\n",
      "    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py\", line 134, in __optimizer_step\n",
      "    trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 277, in optimizer_step\n",
      "    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 282, in run_optimizer_step\n",
      "    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 163, in optimizer_step\n",
      "    optimizer.step(closure=lambda_closure, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/torch/optim/optimizer.py\", line 89, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_forecasting/optim.py\", line 131, in step\n",
      "    _ = closure()\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 649, in train_step_and_backward_closure\n",
      "    split_batch, batch_idx, opt_idx, optimizer, self.trainer.hiddens\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 742, in training_step_and_backward\n",
      "    result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 293, in training_step\n",
      "    training_step_output = self.trainer.accelerator.training_step(args)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 156, in training_step\n",
      "    return self.training_type_plugin.training_step(*args)\n",
      "  File \"/Users/polina/.conda/envs/neural_prophet/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 125, in training_step\n",
      "    return self.lightning_module.training_step(*args, **kwargs)\n",
      "  File \"/Users/polina/fds/neural_prophet/neuralprophet/models/NBeats.py\", line 24, in training_step\n",
      "    assert len(log[\"loss\"].size()) >= 1\n",
      "AssertionError\n",
      "2021-05-21 16:13:05,179\tERROR tune.py:545 -- Trials did not complete: [train_NBeats_tune_42ab3_00000, train_NBeats_tune_42ab3_00001, train_NBeats_tune_42ab3_00002]\n",
      "2021-05-21 16:13:05,198\tWARNING experiment_analysis.py:580 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'trial_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/neural_prophet/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'trial_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bdc74c330998>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                \u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                               num_samples = 3)\n\u001b[0m",
      "\u001b[0;32m~/fds/neural_prophet/neuralprophet/hyperparameter_tuner.py\u001b[0m in \u001b[0;36mtune_hyperparameters\u001b[0;34m(model_name, df, freq, num_epochs, mode, config, num_samples, resources_per_trial, return_results)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/neural_prophet/lib/python3.7/site-packages/ray/tune/analysis/experiment_analysis.py\u001b[0m in \u001b[0;36mresults_df\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             ],\n\u001b[0;32m--> 508\u001b[0;31m             index=\"trial_id\")\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     def get_best_trial(self,\n",
      "\u001b[0;32m~/.conda/envs/neural_prophet/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mfrom_records\u001b[0;34m(cls, data, index, exclude, columns, coerce_float, nrows)\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1870\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__iter__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1871\u001b[0;31m                 \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1872\u001b[0m                 \u001b[0mexclude\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/neural_prophet/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'trial_id'"
     ]
    }
   ],
   "source": [
    "freq = '5min'\n",
    "best_params, results_df = tune_hyperparameters('NBeats',\n",
    "                                               df,\n",
    "                                               freq, \n",
    "                                              num_samples = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc1fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ba43ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc573993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373853ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a78b6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bd989d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e2f615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cabab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e84b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
